---
output:
  word_document: default
  pdf_document:
    df_print: paged
    keep_tex: yes
    latex_engine: xelatex
  html_document:
    df_print: paged
lang: ru-russian
header-includes:
- \newcommand{\bcenter}{\begin{center}}
- \newcommand{\ecenter}{\end{center}}
- \usepackage[russian]{babel}
fontenc: T2A
babel-lang: russian
---

<center>
# **Компьютерная работа №1 <br/> Многомерные статистические методы <br/> 2020-21 гг. — 2 курс**
</center>

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

## **1. Постановка задачи**
На первом этапе необходимо импортировать все необходимые библиотеки и данные, с которыми будет впоследствии вестись работа. 
``` {r, comment = NA}
library('rio')
library('DescTools')
library('ggpubr')
library('ggplot2')
library('nortest')
library('fBasics')
library('moments')
library('car')
library('dplyr')
library('readxl')
library('tibble')
library('EnvStats')
library('outliers')
library('Hmisc')
library('corrplot')
library('ppcor')
library('DiscriMiner')
library('pracma')
library('FactoMineR')
library('factoextra')
#library('devtools')
library('psych')
library('htmlTable')
library('REdaS')
library('grid')
library('plotly')
library('robustHD')
library('lmtest')
library('sjPlot')
library('normtest')
library('GGally')
library('leaps')
```

## **1.1 Описание и обоснование системы показателей, обоснование репрезентативности выборки
Для анализа был взят массив данных по продолжительности жизни (life expectancy) с сайта kaggle. В данном массиве от всемирной организации здравоохранения (WHO), включающем различные показатели, которые предположительно влияют на продолжительность жизни, изначально 2938 наблюдений и 22 переменных, 20 из которых числовые. В рамках работы мы взяли 9 переменных c условно непрерывными данными и таким образом получилось 183 взаимно независимых наблюдения, напомню, что по условию требовалось не менее 8 переменных и не менее 80 наблюдений.
Что касается репрезентативности выборки, то данные были получены по 193 странам, в числе которых как развитые страны, как США, Великобритания, Франция т.д., так и развивающиеся как Ангола, Зимбабве, Уругвай и далее по списку; так что взятые данные крайне неплохо отражают генеральную совокупность, так как все категории стран представлены в близких к реалиям пропорциях, напомню, что ООН обычно публикует данные по 195 странам, в нашем же случае взяты 193 страны.
Разумеется, что из всего множества переменных в качестве зависимой была выбрана life expectancy, остальные были взяты в качестве объясняющих, как и требуется в задании, мы постарались взять такие переменные, чтобы предположительно (до исследования) влияние объясняющих переменных на зависимую было примерно равнозначно. По условию задания мы не могли взять временные ряды, поэтому мы зафиксировали для анализа последний год, в котором бы не было в пропусков в данных, таковым оказался 2014 год. Также при отборе переменных мы пытались отобрать такие, которые бы были похожи на нормальный или логнормальный закон распределения, что помогло бы уверенно провести регрессионный анализ. Особое внимание мы также уделили пропускам в данных и нулям, объясню подробнее: пропуски в данных мешали нам должным образом проводить анализ, так как нам бы попросту пришлось бы выкинуть ряд переменных и взять на их место другие (было бы странно выкидывать ВВП и иные очевидно необходимые показатели просто из-за пропусков в массиве), так что мы нашли данные по пропускам на иных сайтах и заполнили их. 
Говоря подробнее про отобранные переменные, мы отобрали: 1. Life expectancy (предположительно зависимая) 2. Adult.mortality т.к. распределение предположительно подчиняется логнормальному закону, и эта переменная предположительно неплохо объясняет продолжительность жизни 3. Alcohol предположительно подчиняется логнормальному закону и предположительно неплохо объясняет продолжительность жизни, плюс по переменной был всего 1 пропуск в данных  4. Total_exp предположительно подчиняется нормальному закону и предположительно неплохо объясняет продолжительность жизни, плюс по переменной было всего 2 пропуск в данных  5. GDP предположительно подчиняется логнормальному закону и предположительно неплохо объясняет продолжительность жизни, хоть по ней и было много пропусков, пришлось их заполнить, так как переменная предположительно неплохо объясняет зависимую переменную  6. Thinness_5_9 предположительно подчиняется логнормальному закону и предположительно неплохо объясняет продолжительность жизни 7. Diphteria предположительно подчиняется логнормальному закону и предположительно неплохо объясняет продолжительность жизни 8. Thinness_10_19 также предположительно подчиняется логнормальному закону и предположительно неплохо объясняет продолжительность жизни 9. Schooling предположительно подчиняется нормальному закону и предположительно неплохо объясняет продолжительность жизни.

Дальше мы проделаем и еще раскроем некоторые действия с данными и  в частности с переменными, о которых писалось выше:
```{r, comment = NA}
raw_data <- read.csv('life_expectancy_data.csv', header = TRUE, sep = ',')
data <- raw_data[which(raw_data$Year == 2014),]
rownames(data) <- 1:nrow(data) # Присвоим индексы наблюдениям в соответствии с их естественной очередностью, а не тем порядком, что был в исходной таблице.
data$Adult.Mortality <- as.numeric(data$Adult.Mortality) # приведём условно непрерывные данные, содержащие целые числа, к типу Numeric
data$infant.deaths <- as.numeric(data$infant.deaths)
data$Hepatitis.B  <- as.numeric(data$Hepatitis.B )
data$Measles  <- as.numeric(data$Measles)
data$under.five.deaths <- as.numeric(data$under.five.deaths)
data$Polio <- as.numeric(data$Polio)
data$Diphtheria <- as.numeric(data$Diphtheria)
data <- data[, !(names(data) == "Year")] # Теперь, когда рассматривается только 2014 год, эта переменная более не информативна.
str(data)
```

Посмотрим гистограммы по неочищенным данным. Чтобы не перегружать работу, ниже будет представлена лишь часть гистограмм:

```{r, comment = NA}
boxplot(scale(data[,3:ncol(data)]))

vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
plot1<-ggplot(data, aes(x=Life.expectancy)) + geom_histogram(color="black", fill='#00cccc')
plot2<-ggplot(data, aes(x=HIV.AIDS)) + geom_histogram(color="black", fill='#00cccc')
plot3<-ggplot(data, aes(x=infant.deaths)) + geom_histogram(color="black", fill='#00cccc')
plot4<-ggplot(data, aes(x=Alcohol)) + geom_histogram(color='black', fill='#00cccc')
plot5<-ggplot(data, aes(x=BMI)) + geom_histogram(color='black', fill='#00cccc')
plot6<-ggplot(data, aes(x=Diphtheria)) + geom_histogram(color='black', fill='#00cccc')

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 3)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
print(plot3, vp = vplayout(1, 3))
print(plot4, vp = vplayout(2, 1))
print(plot5, vp = vplayout(2, 2))
print(plot6, vp = vplayout(2, 3))
```

Видно, что в данных имеются выбросы, некоторые данные обладают большим числом нулевых наблюдений, некоторые -- атипичными (например,  `BMI` и `Diphteria`), далёкими от нормального либо логнормального закона распределениями. Для более качественного анализа, тем не менее, требуется, чтобы данные были близки к нормальному (или логнормальному) закону, поскольку большинство применяемых методов анализа данных подразумевают их нормальность. Это необходимо учитывать при выборе объясняющих переменных.

Теперь необходимо оставить лишь 9 переменных -- 8 объясняющих и 1 зависимую, -- которые нам потребуются для дальнейшего анализа. Также оставим одну дамми-переменную, которая позволит ориентироваться в данных:

```{r, comment = NA}
data <- data[, c("Country", "Life.expectancy", "Adult.Mortality",
                    "Alcohol", "Total.expenditure", "Diphtheria",
                    "GDP", "thinness.5.9.years",
                    "thinness..1.19.years", "Schooling")]

# Также упростим названия переменных
names(data) <- c("Country", "Life_exp", "Adult_mort",
                    "Alcohol", "Total_exp", "Dipht",
                    "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")
```
Наконец, был получен итоговый датасет, показывающий данные по различным странам по 9 информативным показателям за 2014 год. Обработка данных, тем не менее, на этом ещё не окончена: необходимо провести диагностику пропусков.

Дело в том, что в таблице с данными присутствует большое число пропусков, поэтому было решено заполнить эти пропуски данными, найденными на сторонних ресурсах. Поскольку в качестве объясняющих переменных были выбраны `Life.expectancy`,`Adult.Mortality`,`Alcohol`,`Total.expenditure`,`Diphtheria`, `GDP`, `Thinness.5.9.years`, `Thinness..1.19.years`, `Schooling`, то для начала необходимо в целом изучить, какова ситуация с пропусками наблюдений по этим признакам:

```{r, comment = NA}
d_names <- names(data[,2:ncol(data)])
for (i in d_names) {
  print(i, quote = FALSE)
  print(which(is.na((data[i]))))
}
```

Как видим, есть пропуски по переменным `Alcohol`, `Total_exp`, `Thinness_10_19`, `Thinness_5_9`, а также особенно много пропусков по `Schooling` и `GDP`. Благо их довольно легко заполнить, найдя информацию на сторонних ресурсах.

Данных по потреблению алкоголя в Южном Судане нет почти нигде, но, учитывая, что эта страна совсем недавно обрела независимость от Судана и по прочим показателям не особенно отличается от этой страны, а также то, что Южный Судан ещё не сильно христианизирован (в исламских странах, как известно, по религиозным соображениям потребление алкоголя, как правило, значительно ниже), то мы дополним это значение тем же, что значится для Судана. В пользу этого также говорит и то, что потребление алкоголя в Центральной Африке довольно низкое (по сравнению, например, с югом континента):
```{r, comment = NA}
data$Alcohol[151] <- 0.01 
```

Далее дополним `Total_exp` в соответствии с данными на сайте http://data.un.org/Data.aspx?d=WHO&f=MEASURE_CODE%3AWHS7_113:

```{r, comment = NA}
data$Total_exp[45] <- 6.1 # КНДР 
data$Total_exp[149] <- 4.3 # Сомали
```


Данные по ВВП найти совсем несложно, хотя пропусков придётся заполнить немало. Однако в процессе заполнения пропусков было выяснено, что исходные данные содержат большие неточности: например, ВВП на душу населения в Болгарии намного выше, чем в Германии или в Канаде. При всём уважении к Болгарии, такое вряд ли возможно:

```{r}
print(c("ВВП Болгарии:", data$GDP[which(data$Country == "Bulgaria")]), quote=FALSE)
print(c("ВВП Германии:", data$GDP[which(data$Country == "Germany")]), quote=FALSE)
print(c("ВВП Канады:", data$GDP[which(data$Country == "Canada")]), quote=FALSE)
```
Скорее всего, имело место уменьшение некоторых значений на целый порядок, следовательно, работать с такими данными бесполезно -- они ложные. Поэтому было решено найти новые данные по ВВП по всем странам за 2014 год на сайте https://data.worldbank.org/indicator/NY.GDP.PCAP.CD?view=map&year=2014, затем выбрать те 183 страны, которые и так есть в исходном датасете, и просто заменить нужные колонки:

```{r}
gdp_da <- read.csv("gdp_data.csv", sep = ",", header = FALSE)
gdp <- gdp_da[,1:ncol(gdp_da)]
data$GDP <- gdp$V6
```

Пропуски по `Thinness_5_9` были заполнены по данным сайта https://www.who.int/data/maternal-newborn-child-adolescent-ageing/indicator-explorer-new/mca для Судана и Южного Судана (оценочные значения ввиду отсутствия данных):

```{r}
data$Thinness_5_9[151] <- 7.3
data$Thinness_5_9[154] <- 7.6
```

Там же для этих же стран найдены пропуски по `Thinness_10_19`

```{r}
data$Thinness_10_19[151] <- 7.8
data$Thinness_10_19[154] <- 7.9
```


И, наконец, с помошью того же сайта, что указан выше, заполним пропуски по `Schooling`:

```{r}
data$Schooling[28] <- 5.4
data$Schooling[44] <- 12.5
data$Schooling[45] <- 7.8
data$Schooling[46] <- 6.7
data$Schooling[131] <- 12.5
data$Schooling[132] <- 10.0
data$Schooling[149] <- 4.7
data$Schooling[173] <- 11.8
data$Schooling[174] <- 5.8
data$Schooling[175] <- 12.9
```


Напоследок ещё раз выведем парочку гистограмм и ящик с усами по обновлённым переменным, которые теперь не содержат пропусков и имеют более чёткий и структурированный вид. Заметно, что обновлённые гистограммы уже по большей части напоминают гистограммы данных, гипотетически подчиняющихся нормальному и логнормальному закону распределения (хотя и заметна асимметрия):
```{r, comment = NA}
boxplot(scale(data[,3:ncol(data)]))

vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
plot1<-ggplot(data, aes(x=Life_exp)) + geom_histogram(color="black", fill='#00cccc')
plot2<-ggplot(data, aes(x=Adult_mort)) + geom_histogram(color="black", fill='#00cccc')
plot3<-ggplot(data, aes(x=Alcohol)) + geom_histogram(color="black", fill='#00cccc')
plot4<-ggplot(data, aes(x=Total_exp)) + geom_histogram(color='black', fill='#00cccc')
plot5<-ggplot(data, aes(x=GDP)) + geom_histogram(color='black', fill='#00cccc')
plot6<-ggplot(data, aes(x=Thinness_10_19)) + geom_histogram(color='black', fill='#00cccc')

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 3)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
print(plot3, vp = vplayout(1, 3))
print(plot4, vp = vplayout(2, 1))
print(plot5, vp = vplayout(2, 2))
print(plot6, vp = vplayout(2, 3))
```

## **1.2 Выдвижение рабочих гипотез исследования
Нашим предположением по работе (гипотезой) является то, что все выбранные нами переменные с высокой долей вероятности объясняют зависимую переменную life expectancy.  
В нашей работе до начала исследований мы выдвинули ряд гипотез. Во-первых, мы выдвинули предположение о зависимой переменной, что она подчиняется нормальному закону распределения. Во-вторых, мы предполагаем, что существует тесная связь между зависимой переменной продолжительностью жизни и объясняющими переменными. В-третьих, мы предполагаем, что, наверное, зависимая переменная будет сильнее всего меняться при изменении GDP И Schooling и total expenditure т.к. это три основных показателя развитости страны, а очевидно, что чем более развитая страна, тем выше по идее должна быть продолжительность жизни. В-четвертых, разные переменные будут в разномнаправлении коррелировать с зависимой переменной.

## **2. Оценка основных характеристик СВ**

В качестве зависимой переменной была выбрана продолжительность жизни. Далее будут рассмотрены статистические характеристики этой величины. Наиболее простой способ их изучить –- воспользоваться функцией `Desc()` из пакета `DescTools`, которая показывает почти все характеристики случайной величины, а также гистограмму распределения, ядерную оценку плотности, ящик с усами и огиву:
```{r, comment=NA}
y <- data$Life_exp
Desc(y)
```

Визуальный анализ показывает, что данные очень слабо напоминают нормальное распределение. Ядерная оценка плотности, как видно, очень далека от идеального Гауссовского колокольчика. Тем не менее, известно, что параметр `aes` при вызове `Desc()` может искажать вид нетипичных данных, поэтому все графики будут построены вручную для более качественного анализа.

Пока же перейдём к рассмотрению характеристик случайной величины `y`.

##### **2.1 Характеристики положения СВ** 

При вызове `Desc(y)` были получены значения некоторых выборочных характеристик центра положения, в частности, известно, что $\bar{y}=`r mean(y)`$, $Me_y^*=`r median(y)`$. Необходимо также найти моду. Поскольку мода в случае непрерывных данных строится по интервальному ряду, будет проведено независимое вычисление моды вручную и сопоставление с тем результатом, что был получен функцией в R:

```{r, comment = NA}
# Вычисление моды функцией R:
Mode(y)
```

```{r, comment = NA}
# Вычисление моды вручную
R <- max(y) - min(y) # Размах выборки
h <- R / (1 + log2(183)) # Ширина интервала
a <- min(y) - (h/2)
upper_bound <- seq(from = a + h, to =  a + 10*h, by = h)
# Проверка, что количество интервалов оптимально - должно быть выведено 'TRUE':
(upper_bound[9] < max(y)) & (upper_bound[10] >= max(y))
```
```{r, comment = NA}
fr1 <- 0
fr2 <- 0
fr3 <- 0
fr4 <- 0
fr5 <- 0
fr6 <- 0
fr7 <- 0
fr8 <- 0
fr9 <- 0
fr10 <- 0

for (i in y) {
  if (i < upper_bound[1]) {
    fr1 <- fr1 + 1
  } else if ((i > upper_bound[1]) & (i < upper_bound[2])) {
    fr2 <- fr2 + 1
  } else if ((i > upper_bound[2]) & (i < upper_bound[3])) {
    fr3 <- fr3 + 1
  } else if ((i > upper_bound[3]) & (i < upper_bound[4])) {
    fr4 <- fr4 + 1
  } else if ((i > upper_bound[4]) & (i < upper_bound[5])) {
    fr5 <- fr5 + 1
  } else if ((i > upper_bound[5]) & (i < upper_bound[6])) {
    fr6 <- fr6 + 1
  } else if ((i > upper_bound[6]) & (i < upper_bound[7])) {
    fr7 <- fr7 + 1
  } else if ((i > upper_bound[7]) & (i < upper_bound[8])) {
    fr8 <- fr8 + 1
  } else if ((i > upper_bound[8]) & (i < upper_bound[9])) {
    fr9 <- fr9 + 1
  } else if ((i > upper_bound[9]) & (i < upper_bound[10])) {
    fr10 <- fr10 + 1
  }
}

frequencies <- c(fr1, fr2, fr3, fr4, fr5, fr6, fr7, fr8, fr9, fr10)
Mode_y <- upper_bound[6] + (h * ((frequencies[7] - frequencies[6]) / (2 * frequencies[7] - frequencies[6] - frequencies[8])))

Mode_y
```

Мода, посчитанная вручную, как видно, отличается от той, что была дана функцией `Mode()`.

Также, поскольку мимоходом были получены интервалы по формуле Стёрждесса, построим также гистограмму и ядерную оценку плотности. Левосторонняя асимметрия теперь ещё заметнее, но, по крайней мере, ядерная оценка плотности более-менее сглаженная:
```{r, comment = NA}
hist(y, breaks = seq(from = a, to =  a + 10*h, by = h),
     col = '#00ccc2',
     main = 'Гистограмма распределения и ядерная оценка плотности',
     xlab = 'Средняя продолжительность жизни',
     ylab = 'Частость', freq = FALSE,
     ylim = c(0, 0.06))

kde <- density(y)
lines(kde)
```


В целом, видно, что, во-первых, характеристики центра отличаются друг от друга, но не сильно. Мода и медиана: $Mo_y^*=`r Mode_y`$, $Me_y^*=`r median(y)`$ -- отличаются примерно на 3 единицы измерения, если ориентироваться на значение, полученное вручную, и 2 единицы -- если смотреть значение, рассчитанное автоматически. Выборочное матожидание $\bar{y}=`r mean(y)`$ (относительно общего диапазона) отличается в меньшую сторону. Это, во-первых, говорит о том, что, поскольку $\bar{y} < Me_y$, выборка обладает левосторонней асимметрией, во-вторых, поскольку для нормального закона свойственно то, что характеристики центра -- мода, медиана и матожидание -- примерно равны друг другу, то имеющееся распределение данных, будучи асимметричным, не вписывается в эту парадигму. Впрочем, более детальную картину можно получить при анализе других рядов характеристик, например, коэффициента асимметрии.

##### **2.2 Характеристики разброса СВ** 
Все необходимые, кроме некоторых, характеристики уже получены функцией `Desc()`. Дополним их:

```{r, comment = NA}
print('Уточнение значения выборочной дисперсии:', quote=FALSE)
var(y)
print('Пределы вырьирования значения выборки:', quote=FALSE)
range(y)
print('Размах вариации:', quote=FALSE)
max(y) - min(y)
print('Коэффициент вариации:', quote=FALSE)
cv(y)
print('Квартильное отклонение:', quote=FALSE)
IQR(y)/2
print('Две трети выборочной дисперсии:', quote=FALSE)
sd(y)*(2/3)
```

Как видно, минимальным значением в выборке является 48.1, максимальным -- 89.

По вызову `Desc()` в более верхних строках кода известно, что интерквартильный размах (разница между третьим и первым квартилем, характеризующая интервал значений признака, содержащий центральные 50% наблюдений выборки) $IQR=11.25$. Квартильное отклонение, устойчивый к выбросам аналог размаха, показывает значение $Q=5.625$, причём значение разброса $R = 40.9$, что намного (в 8 раз) больше. То, что квартильное отклонение $Q=5.625$, а $\dfrac{2}{3} \sigma = `r sd(y)*(2/3)`$, показывает, что распределение можно считать умеренно асимметричным, поскольку эти значения примерно одинаковы.

Выборочная дисперсия и среднее квадратическое отклонение характеризуют степень разброса значений случайной величины вокруг её среднего значения. В терминах тех данных, что анализируются настоящим расчётом, эти характеристики показывают степень разброса значений продолжительности жизни для исследуемых 183 стран. Выборочная (исправленная) дисперсия $s^2=`r var(y)`$, выборочное СКО $s=8.56$ -- довольно небольшое относительно выборки, в принципе, можно сказать -- и визуальный анализ это подтверждает, -- что значения сконцентрированы примерно около выборочного среднего. Неустойчивый к выбросам коэффициент вариации, показывающий меру рассеяния исседуемого признака, $V_s=`r cv(y)*100` \% <33\%$, следовательно, выборку можно охарактеризовать как однородную. Относительный показатель квартильной вариации имеет следующее значение:

```{r, comment = NA}
(IQR(y)/(2 * median(y)))*100
```

Это значение является аналогом коэффициента вариации, который, поскольку содержит в формуле выборочное среднее, гораздо менее устойчив к выбросам. И хотя $V_s$ и так показывал довольно низкий процент вариации, уточнение с помошью относительного показателя интерквартильной вариации дало значение $K_Q = 7.64\%$, что окончательно позволяет сделать вывод о том, что выборка однородная.

Более того, исследованные характеристики разброса позволяют сделать вывод, что либо в выборке совсем не содержится выбросов, либо их мало и они незначительно отличаются от общепринятых метрик, например, от $3IQR$. Подтвердить эти гипотезы можно будет при диагностике выбросов.

Также рассмотрим в этой части коэффициенты эксцесса и асимметрии, полученные функцией `Desc(y)`. Коэффициент асимметрии $Ac^* = -0.37$, что показывает, что, во-первых, поскольку $sign(Ac^*) = −1$, имеет место левосторонняя асимметрия, во-вторых, несмотря на то, что $ |Ac^*|< 0.5$, коэффициент асимметрии довольно близок по своему значению к пороговому, следовательно, следует говорить о достаточной асимметрии гистограммы (и именно это показывало построение графических интерпретаций имеющихся данных).

Коэффициент эксцесса $Ek^* = -0.38$, что показывает следующее: во-первых, так как $Ek^*<0$ и $|Ek^*|<0.5$ (хоть и близок к этому значению), график распределения имеет более плоскую вершину относительно графика плотности нормального распределения.


##### **2.3 Ранговые характеристики СВ** 

Уже посчитано, что разброс $R = 40.9$. Получим более понятные данные:

```{r, comment = NA}
summary(y)
```
Как видно, $y_{min}=48.1$, $Me=y_{0.5}=73.6$, $y_{0.75}=76.85$, при этом максимальное значение в выборке $y_{max} = 89$, что незначительно (на $15\%$) больше $75\%$ значений выборки. Значит, опять-таки, скорее всего, в выборке нет существенных выбросов.

Можно также посчитать, во сколько раз продолжительность жизни в 10% стран с самым высоким уровнем этого показателя больше продолжительности жизни в $10\%$ наименее благополучных в этом плане стран:

```{r, comment = NA}
kf <- quantile(y, probs = seq(0, 1, 0.1))[10]/quantile(y, probs = seq(0, 1, 0.1))[2] 
names(kf) <- 'Во сколько раз продолжительность жизни в 10% стран с самым высоким уровнем этого показателя больше продолжительности жизни в 10% наименее благополучных в этом плане стран:'
kf
```

Несущественное отличие, впрочем, если речь идёт о продолжительности жизни, тут вряд ли стоило ожидать существенного разброса: всё-таки человечий век ограничен.

Итак, суммируя все выкладки, можно сделать следующее краткое заключение: выборка -- и это связано со спецификой обрабатываемых данных, а именно -- возраста человека, который ограничен по значению -- однородна, имеет мало (или совсем не имеет) выбросов, значения внутри выборки не сильно варьируются. Распределение имеет левостороннюю асимметрию, что означает, что средние значения продолжительности жизни (интервал примерно в 11 лет) сконцентрированы -- об этом же свидетельствует значение моды и медианы -- около примерно 70+ лет, и это хороший, довольно большой возраст. Значит, в целом человеческая цивилизация успешно добивается увеличения продолжительности жизни, и в дальнейшем будет проанализировано, какие именно факторы влияют на достижение такого результата. 

## **3. Диагностика выбросов**
Как было проанализировано ранее, результирующая переменная, скорее всего, либо совсем не имеет выбросов (условно, по метрике 3IQR), либо имеет их очень мало. Далее необходимо проверить это аналитически, для чего будут проведены различные процедуры диагностики выбросов, более или менее чувствительные и "строгие".

Приступим к удалению выбросов. Сначала пропишем функции, возвращающие выбросы, определённые по правилу 3 и 1.5 IQR, по входным данным:
```{r, comment=NA}
IQR3 <- function(y) {
  out_of_3IQR <- boxplot.stats(y, coef = 3)$out
  return(out_of_3IQR)
}

IQR1.5 <- function(y) {
  out_of_1_5IQR <- boxplot.stats(y, coef = 1.5)$out
  return(out_of_1_5IQR)
}
```
Теперь применим их:

```{r, comment = NA}
IQR3(y)
IQR1.5(y)
```
Как видно, в соответствии с правилом 3 IQR, согласно которому наблюдение квалифицируется как выброс в том случае, если оно не входит в интервал $(Q_1-3IQR;Q_3+3IQR)$, то есть превосходит значение суммы третьей квартили и добавленному к ней тройному интерквартильному размаху (или, напротив, меньше значения разности $Q_1-3IQR$), в исследуемой выборке выбросов нет совсем. Более "чувствительное" правило 1.5 IQR, по которому интервал вхождения значений гораздо уже, идентифицировало 1 аномальное значение: 48.1.

Можно в целом прикинуть, какой интервал дают эти правила:

```{r, comment = NA}
quantile(y)
IQR(y)

# Правило 3 IQR
65.60-3*IQR(y)
76.85+3*IQR(y)

# Правило 1.5 IQR
65.60-1.5*IQR(y)
76.85+1.5*IQR(y)
```

Видно, что в первом случае интервал $(31.85;110.6)$ гораздо шире, чем во втором: $(48.725;93.725)$.

Теперь воспользуемся правилом 3 сигм, наиболее информативным для нормально распределённых данных (в нашем случае о нормальности говорить рано: недостаточно фактов), которое гласит: вероятность того, что любая случайная величина отклонится от своего среднего значения менее чем на $3 \sigma$, превышает $\dfrac{8}{9}$, то есть практически все значения случайной величины заключены в интервале $(\mu^*-3\sigma;\mu^*+3\sigma)$, где $\mu^*$ -- выборочное матожидание:

```{r, comment = NA}
mean(y)-3*SD(y)
mean(y)+3*SD(y)
sort(y)[1:3]
sort(y)[179:182]
```

Не мудрствуя лукаво и просто отсортировав выборку, чтобы посмотреть несколько первых и последних значений, можно убедиться, что с вероятностью $\dfrac{8}{9}$ в выборке нет выбросов. Это, конечно, при условии большого допущения о нормальном законе распределения случайной величины, впрочем, полученный результат эквивалентен тому, что был получен для правила 3 IQR.

Для анализа 8 объясняющих переменных, думается, вполне можно обойтись лишь правилом 3 и 1.5 IQR, поскольку именно он будет в дальнешем задействоваться на других этапах работы:

```{r, comment=NA}
d_names <- names(data)[2:10]
for (i in d_names) {
  print(i, quote = FALSE)
  print(IQR3(as.matrix(data[i])), quote=FALSE)
}
```

Наибольшее число выбросов зафиксировано по переменной `Dipht`. По другим переменным имеется 1-2 наблюдение, не входящее в интервал. По правилу 1.5 IQR, разумеется, выбросов должно быть больше:

```{r, comment=NA}
d_names <- names(data)[2:10]
for (i in d_names) {
  print(i, quote = FALSE)
  print(IQR1.5(as.matrix(data[i])), quote=FALSE)
}
```

Что и требовалось показать: добавилось выбросов по переменным `GDP`, `Total_exp` и другим.

Правило $3\sigma$ к этим переменным применять не будем, поскольку это довольно опасно: распределения данных, как было показано, имеют по многим переменным вид, близкий к логнормальному, следовательно, возможно, правило, которое применяется в основном для нормально распределённых данных, будет неуместно.

Визулизиуем выбросы по каждой переменной. Ящик с усами очень хорошо показывает это:

```{r, comment = NA}
plot1<-boxplot(scale(data[,2:6]),  col = '#00cccc')
plot2<-boxplot(scale(data[,7:10]),  col = '#00cccc')
```

Было решено отмасштабировать данные, потому что имеются переменные с очень большими относительно других переменных значениями. Видно, что ящичковая диаграмма строит выбросы по правилу 1.5 IQR, и выбросами -- как и было ранее показано -- обладает существенное число переменных. Однако выборка и так не особенно большая, чтобы удалять данные чрезмерно тщательно, поэтому было решено остановиться именно на 3IQR. Также есть основания полагать, что после логарифмирования части данных удастся стабилизировать дисперсию, и выбросов, возможно, будет меньше. Убедимся в этом, выбрав для начала данные для логарифмирования, что можно осуществить достаточно надёжно, лишь посмотрев на выборочные распределения:

```{r, comment = NA}

vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
plot1<-ggplot(data, aes(x=Life_exp)) + geom_histogram(color="black", fill='#00cccc')
plot2<-ggplot(data, aes(x=Adult_mort)) + geom_histogram(color="black", fill='#00cccc')
plot3<-ggplot(data, aes(x=Alcohol)) + geom_histogram(color="black", fill='#00cccc')
plot4<-ggplot(data, aes(x=Total_exp)) + geom_histogram(color='black', fill='#00cccc')
plot5<-ggplot(data, aes(x=GDP)) + geom_histogram(color='black', fill='#00cccc')
plot6<-ggplot(data, aes(x=Thinness_5_9)) + geom_histogram(color='black', fill='#00cccc')
plot7<-ggplot(data, aes(x=Dipht)) + geom_histogram(color='black', fill='#00cccc')
plot8<-ggplot(data, aes(x=Thinness_10_19)) + geom_histogram(color='black', fill='#00cccc')
plot9<-ggplot(data, aes(x=Schooling)) + geom_histogram(color='black', fill='#00cccc')

grid.newpage()
pushViewport(viewport(layout = grid.layout(3, 3)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
print(plot3, vp = vplayout(1, 3))
print(plot4, vp = vplayout(2, 1))
print(plot5, vp = vplayout(2, 2))
print(plot6, vp = vplayout(2, 3))
print(plot7, vp = vplayout(3, 1))
print(plot8, vp = vplayout(3, 2))
print(plot9, vp = vplayout(3, 3))
```

Судя по гистограммам, есть смысл логарифмировать данные по всем переменным, кроме `Schooling` и `Total_exp`, которые и так имеют квазинормальный вид, и `Dipht`, распределение которой и вовсе атипично. Также в ходе экспериментов было решено не логарифмировать переменные `Adult_mort` и `Alcohol`, поскольку логарифмирование (и даже логарифмирование с линейными преобразованиями, например, домножением на 1000 или 100 для увеличения значения и устранения необходимости работать с отрицательными значениями после логарифмирования) мало помогает в стабилизации дисперсии и выбросов в этих случаях, а распределение после всех преобразований было всё ещё мало похожим на нормальное (p-value меньше всякого разумного уровня значимости), к тому же генерируется чрезвычайно много выбросов.

```{r, comment = NA}
data_ex <- data.frame(data)
data_ex$Adult_mort <- log(data_ex$Adult_mort)
data_ex$Alcohol <- log(data_ex$Alcohol * 1000)
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
plot1<-ggplot(data_ex, aes(x=Alcohol)) + geom_histogram(color="black", fill='#00cccc')
plot2<-ggplot(data_ex, aes(x=Adult_mort)) + geom_histogram(color="black", fill='#00cccc')


grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
```

Так что преобразование проведём только над некоторыми переменными:
```{r, comment = NA}
data1 <- data.frame(data)

#data$Adult_mort <- log(data$Adult_mort) -- было решено не логарифмировать
#data1$Alcohol <- log(data$Alcohol) -- было решено не логарифмировать

data1$GDP <- log(data$GDP)
data1$Thinness_5_9 <- log(data1$Thinness_5_9)
data1$Thinness_10_19 <- log(data1$Thinness_10_19)
```


```{r, comment = NA}

vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
plot1<-ggplot(data1, aes(x=Life_exp)) + geom_histogram(color="black", fill='#00cccc')
plot2<-ggplot(data1, aes(x=Alcohol)) + geom_histogram(color="black", fill='#00cccc')
plot3<-ggplot(data1, aes(x=Total_exp)) + geom_histogram(color='black', fill='#00cccc')
plot4<-ggplot(data1, aes(x=GDP)) + geom_histogram(color='black', fill='#00cccc')
plot5<-ggplot(data1, aes(x=Thinness_10_19)) + geom_histogram(color='black', fill='#00cccc')
plot6<-ggplot(data1, aes(x=Dipht)) + geom_histogram(color='black', fill='#00cccc')
plot7<-ggplot(data1, aes(x=Thinness_5_9)) + geom_histogram(color='black', fill='#00cccc')
plot8<-ggplot(data1, aes(x=Schooling)) + geom_histogram(color='black', fill='#00cccc')

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
print(plot3, vp = vplayout(2, 1))
print(plot4, vp = vplayout(2, 2))

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
print(plot5, vp = vplayout(1, 1))
print(plot6, vp = vplayout(1, 2))
print(plot7, vp = vplayout(2, 1))
print(plot8, vp = vplayout(2, 2))
```

По гистограммам видно, что более адекватно  стала выглядеть выглядеть переменная `GDP`; у переменных `Thinness_5_9` и `Thinness_10_19` есть некоторый "взрыв" значений в правом хвосте распределения.

Конечно, логарифмирование ударило по дальнейшей интерпретационной силе показателей, зато удалось стабилизировать переменные, что также немаловажно.

Проинспектируем выбросы после преобразований:

```{r, comment = NA}
plot1<-boxplot(scale(data1[,2:6]),  col = '#00cccc')
plot2<-boxplot(scale(data1[,7:10]),  col = '#00cccc')
```

Видно, что выбросов стало меньше. Правда, осталось значительное число аномальных наблюдений у тех переменных, которые никак не обрабатывались. Выведем значения выбросов:

```{r, comment=NA}
d_names <- names(data1)[2:10]
for (i in d_names) {
  print(c(i, ":", IQR3(as.matrix(data1[i]))), quote=FALSE)
}
```
Выбросов по правилу 3 IQR, как и раньше, гораздо меньше, чем по правилу 1.5 IQR, но после логарифмирования выбросы и вовсе пропали (кроме переменной `Dipht`).

Для диагностики выбросов можно также воспользоваться критериями Граббса и Рознера о наличии аномальных наблюдений. Критерий Граббса выполняет итеративную проверку $H_0$ о том, что одно максимальное или минимальное наблюдение в выборке является типичным наблюдением. В переменной `Life_exp` как раз было выявлено одно экстремальное (по правилу 1.5 IQR) наблюдение. Критерий же Рознера позволяет проверить $k>1$ наблюдение на аномальность. Нулевая гипотеза формулируется в слеюущем виде: $H_0$: {$k$ наибольших наблюдений взяты из той же генеральной совокупности, что и $(n−k)$ первых наблюдений}.

Стоит, однако, учитывать, что оба теста разработаны в предположении нормального распределения генеральной совокупности, из которой взята однородная выборка, следовательно, использовать эти тесты нужно осторожно. Обратимся к уже выведенным гистограммам по всем переменным и оценим примерные распределения данных.

Как видно, ряд данных (и, главное, результирующая переменная) имеют распределение, внешне близкое к нормальному, но часть переменных далеки по своему виду от Гауссовской кривой (например, всё та же `Dispht`). Имея это в виду, проведём тест:
```{r, comment = NA}
grubbs.test(y, type = 10, opposite = FALSE, two.sided = FALSE)
```

Как видно, p-value = 0.5244, соответственно, на довольно высоком уровне значимости можно утверждать, что гипотеза о типичности наблюдения в левом хвосте не отвергается. К слову, если выставить `type = 11`, что будет равносильно проверке о том, что оба наблюдения в обоих хвостах являются типичными, то p-value в этом случае достигает 1. То есть можно заключить, что -- при условии нормального распределения переменной `data$Live_exp$ -- выбросов по этой переменной нет.

Теперь проведём тест Рознера. Сразу выведем тот подмассив результатов, который нам нужен. Параметр $k$ можно было бы выставить равным 1, потому что многие предыдущие методы выявляли лишь это наблюдение в качества выброса, но в итоге было решено взять стандартное значение. И, что и требовалось показать, очередной тест указывает на то, что в выборке нет выбросов -- при условии, что выборка взята из нормальной ГС.    

```{r, comment = NA}
rosnerTest(y, k = 3, alpha = 0.05)$all.stats
```


И, чтобы не забивать пространство работы, объясняющие переменные не будут подвергаться такому тестированию -- во-первых, потому что распределения этих переменных в большинстве случаев явно не нормальные, во-вторых, потому что уже была сделана диагностика по правилу 3 IQR, которая, как видно по `data$Life_exp$, принесла те же результаты, что и эти тесты.

В целом, можно сказать, что данные довольно удачные в плане выбросов: по правилу 3 IQR их нет вообще (за исключением одной переменной), по правилу 1.5 IQR имеются некоторые аномальные значения. Результирующая переменная в силу своей специфики (она обозначает средний возраст, грубо говоря) вполне ожидаемо не имела выбросов (это было показано большинством проведённых тестов).

## **4. Проверка соответствия эмпирического распределения нормальному закону**

В предыдущих частях работы уже обсуждались численные характеристики коэффициентов асимметрии и эксцесса для результирующей переменной и был сделан вывод, что они свидетельствуют о несущественной левосторонней асимметрии и некоторой плосковершинности относительно кривой Гаусса, а примерного равенства выборочных моды, медианы и среднего, свойственного нормальному закону распределения, у исследуемой зависимой переменной нет. Так что есть основания сомневаться в нормальности распределения этой переменной.

Далее на этом этапе проанализировать графические визуализации переменной. Начнём с гистограммы плотности с нанесённой поверх теоретической кривой Гаусса:

```{r, comment = NA}
hist_2 = hist(y, freq = TRUE, col = '#de49a2', 
            breaks = 'sturges', xlab = 'Нижняя граница интервала',
            ylab = 'Частота', 
            ylim = c(0, 50), 
            main = 'Гистограмма эмпирических частот \n по формуле Стерджеса')
x_vals <- seq(from = min(y) - 10,to = max(y) + 10,length = 1000)

hh = unlist(hist_2['breaks'])[2] - unlist(hist_2['breaks'])[1]
y_vals <- dnorm(x_vals, mean = mean(y), sd = sd(y))*hh*183

hist_5 = hist(y, freq = TRUE, col = '#00ccc2', breaks = 'sturges', xlab = 'Нижняя граница интервала', ylab = 'Частота', ylim = c(0, 50), main = 'Гистограмма частот и \n теоретическая кривая Гаусса') 
lines(x_vals, y_vals, col = '#de49a2')
```

Видно всё то, что описывалось ранее: асимметрия и плосковершинность. Также заметно, что результирующая переменная неплохо приближается теоретической кривой Гаусса, хотя и не идеально.

Построим также квантильный график для логарифированной переменной new_y, выбрав в качестве теоретического распределения нормальное.

```{r, comment = NA}
qqnorm(scale(y), main = 'Нормальный график Q-Q',
       xlab = 'Теоретические квантили', ylab = 'Выборочные квантили')
lines(scale(y), scale(y), type = 'l')
```
Как видно, данные группируются вдоль кривой не очень хорошо, имеется много отклонений. Местами точки эмпирического распределения близки к прямой под углом 45 градусов на графике, что говорило бы в пользу нормальности распределения, однако есть области, где скачки и расхождения серьёзны.

Построим также диаграмму “стебель-листья”, которая тоже позволяет визуально оценить распределение:

```{r, comment = NA}
stem(y, scale = 1)
```

... и график dot plot, который также позволяет оценить плотность распределения:
```{r, comment = NA}
ggplot(data1, aes(x = Life_exp)) + geom_dotplot() + ylim(0, 1)
```

В обоих случаях видно, что данные, в принципе, похожи на кривую Гаусса.

Итого, графические методы показали, что распределение данных с натяжкой можно считать похожими на нормальное, хотя результат разнится от метода к методу. Перейдём к более надёжному методу проверки нормальности: тестам на нормальность.

Начать стоит с наиболее мощного критерия провекри на нормальность: критерия Шапиро-Уилка

```{r, comment = NA}
shapiroTest(y, title = NULL, description = NULL)
```
Значение p-value =  0.00213 меньше многих разумных уровней значимости, самым оптимистичным выводом было бы не отвергать гипотезу о нормальности на уровне значимости $\alpha = 0.001$ согласно тесту Шапиро-Уилка. Впрочем, нужно иметь в виду, что тест Шапиро-Уилка рекомендован для малых выборок (из-за своей чувствительности), а при больших объемах выборки имеет ограничения в применении, для чего была предложена его модификация – критерий Шапиро-Франчиа (Shapiro-Francia SF test). и хотя 183 наблюдения -- это по-прежнему маленький объём выборки, и вряд ли он годится для серьёзных аппроксимационных алгоритмов, которые могут исказить картину, всё-таки 183>100 (для такого объема обычно рекомендуется критерий Шапиро-Уилка), поэтому можно также воспользоваться модификацией критерия Шапиро-Уилка – критерием Шапиро-Франчиа (Shapiro-Francia SF test):

```{r, comment = NA}
sf.test(y)
```

p-value, как видно, стал больше, хоть и не намного. Теперь гипотезу о нормальности можно не отвергать на весьма разумном уровне значимости $\alpha=0.005$, что, конечно, всё ещё мало.

Конечно, можно проверить и не такие сильные тесты: например, с помощью критерия Пирсона или критерия Колмогорова, но, поскольку эти тесты очень маломощные, будем использовать их модификации: например, модификацию теста Колмогорова-Смирнова -- тест Лиллиефорса:

```{r, comment = NA}
lillie.test(y)
pearson.test(y)
```

Как видно, p-value получилось очень маленьким для обоих тестов -- меньше, чем в случае более надёжных тестов, следовательно, нет особого смысла и значимости принимать эти результаты всерьёз.
Также из интереса посмотрим распределения объясняющих переменных:

```{r, comment=NA}
d_names <- names(data1)[2:10]
for (i in d_names) {
  print(i)
  print(sf.test(as.matrix(data1[i])))
}
```
Как видно, чрезвычайно высоким относительно других p-value обладает переменная `Schooling`. Также относительно неплохие результаты у тех переменных, что были прологарифмированы. Очевидно сильное отклонение от нормального закона имеют `Alcohol`, `Dipht` и `Adult_mort` -- то, что мы и предполагали ранее.

Итак, имеющуюся результирующую переменную можно с большими оговорками считать похожей на нормальную. При пессимистичном анализе, разумеется, по многим причинам (как графическим, так и аналитическим) гипотезу о нормальности нужно отвернуть. Тем не менее, p-value получился не самым маленьким из возможных.


## **5. Корреляционный анализ**
Сначала посмотрим, в каких переменных содержатся выбросы согласно правилу 3IQR:
```{r, comment = NA}
d_names <- names(data1)[2:10]
for (i in d_names) {
  print(i, quote = FALSE)
  print(IQR3(as.matrix(data1[i])), quote=FALSE)}
```
Видим, что выбросы необходимо удалить только по переменной "Dipht":
```{r, comment = NA}
data_new <- data1[-which(data1$Dipht %in% IQR3(data1$Dipht)),]
str(data_new)
```


Получили данные после удаления выбросов. Теперь приступим непосредственно к корреляционному анализу.

##### **5.1**

Итак, построим облака корреляции для исследования взаимосвязи между переменными, отображающие положительную зависимость, обратную зависимость, отсутствие зависимости. Сначала рассмотрим случай до удаления выбросов. Также изобразим облако корреляции и по переменной "Dipht", чтобы отобразить изменение на диаграмме рассеивания в случае до удаления выбросов и после (так как выбросы были удалены только по переменной "Dipht", остальные переменные остались без изменения и остальные диаграммы будут одинаковы как до, так и после удаления выбросов):
```{r, comment = NA}
ggscatter(data1, x = "Adult_mort", y = "Life_exp", xlab = "Уровень смертности среди взрослых", ylab = "Продолжительность жизни")
ggscatter(data1, x = "GDP", y = "Life_exp", xlab = "ВВП", ylab = "Продолжительность жизни")
ggscatter(data1, x = "Total_exp", y = "Life_exp", xlab = "Доля государственных расходов на здравоохранение\n в общем объеме государственных расходов, в процентах", ylab = "Продолжительность жизни")
ggscatter(data1, x = "Dipht", y = "Life_exp", xlab = "Вакцинация против столбняка, дифтерии и коклюша (ДПЗ) \n среди годовалых детей, в процентах", ylab = "Продолжительность жизни")
```

На первой диаграмме можем увидеть обратную зависимость, на второй - положительную, а на третьей - отсутствие зависимости между переменными (хотя при менее строгом рассмотрении можно было бы проследить некоторую, очень слабую, положительную взаимосвязь). Последняя диаграмма, характеризующая переменную `Dipht`, отображает некоторую положительную связь.

Теперь рассмотрим случай после удаления выбросов:
```{r, comment = NA}
ggscatter(data_new, x = "Adult_mort", y = "Life_exp", xlab = "Уровень смертности среди взрослых", ylab = "Продолжительность жизни")
ggscatter(data_new, x = "GDP", y = "Life_exp", xlab = "ВВП", ylab = "Продолжительность жизни")
ggscatter(data_new, x = "Total_exp", y = "Life_exp", xlab = "Доля государственных расходов на здравоохранение\n в общем объеме государственных расходов, в процентах", ylab = "Продолжительность жизни")
ggscatter(data_new, x = "Dipht", y = "Life_exp", xlab = "Вакцинация против столбняка дифтерии и коклюша (ДПЗ) среди годовалых детей, в процентах", ylab = "Продолжительность жизни")
```

Первые три диаграммы идентичны тем, что были построены в случае до удаления выбросов, и они отображают обратную, прямую и отсутствующую зависимость соответственно. Видим, что последняя диаграмма имеет немного другой вид по сравнению со случаем до удаления выбросов из-за ставшего куда менее заметным кластера точек, находящегося ближе к началу координат -- из-за этого угол наклона облака точек стал более пологим, а само облако -- более рассеянным.

##### **5.2**
Далее построим и проинтерпретируем матрицу парных коэффициентов корреляции. Рассмотрим сначала случай до удаления аномальных наблюдений.
 
Корреляционная матрица для количественных переменных:
```{r, comment = NA}
cor_m1 <- cor(data1[,-1], method = 'pearson')
cor_m1
```
Наиболее сильная положительная связь наблюдается между переменными `Thinness_5_9` и `Thinness_10_19` (0,96),  `Life_exp` и `Schooling` ($\hat{\rho}= 0,79$), также `GDP` И `Schooling` ($\hat{\rho}= 0,8$), `Life_exp` и `GDP` ($\hat{\rho}= 0,77$). Наиболее сильная обратная связь наблюдается между переменными `Life_exp` и `Adult_mort` ($\hat{\rho}= -0,76$), `Life_exp` и `Thinness_5_9` ($\hat{\rho}= -0,59$), `GDP` и `Adult_mort` ($\hat{\rho}= -0,61$). Самая слабая связь между переменными `Dipht` и `Thinness_10_19` ($\hat{\rho}= -0,05$). В целом, видим, что присутствует довольно сильная -- в большинстве случаев -- связь между зависимой переменной `Life_exp` и остальными объясняющими переменными, за исключением `Total_exp` и `Dipht`, где коэффициент корреляции по модулю меньше 0,4 (умеренная связь).

Корреляционная матрица для количественных переменных с p-value:
```{r, comment = NA}
cor_m2 <- rcorr(as.matrix(data1[,-1]), type = 'pearson')
cor_m2
```
Итак, получив значения p-value, можем сделать вывод о статистической значимости парных коэффициентов корреляции.Все коэффиенты, для которых p-value больше 0,05, являются незначимыми на уровне значимости 0,95. Как и стоило ожидать, очень высокий p-value имеет коэффициент корреляции между `Dipht` и `Thinness_10_19`, между которыми наблюдается отсутствие связи.

Далее подтвердим полученные выводы более наглядно с помощью еще двух способов отображения матрицы парных коэффициентов корреляции.

Корреляционная матрица:
```{r, comment = NA}
corrplot(as.matrix(cor_m1), type = 'full', method = 'circle', tl.col = 'black', tl.srt = 45)
```

Этот график показывает направление связи между двумя переменными, для которой выше была дана численная интерпретация (красным показана отрицательная зависимость, синим - положительная), а также силу связи между ними (чем более крупный и насыщенный по цвету круг, тем более сильная связь между переменными). 

На следующем графике можно наглядно увидеть направление связи (по наклону овала) между двумя переменными (красным показана отрицательная зависимость, синим - положительная), а также силу связи между ними (чем более насыщенный по цвету овал, чем он более вытянут, тем более сильная связь между переменными): 

```{r, comment = NA}
corrplot.mixed(cor(data1[,-1]), lower.col = 'black', upper = 'ellipse')
```
В левой части видим значения парных коэффициентов корреляции, в правой - сами эллипсы, которые визуализируют эти значения.

Теперь построим и проинтерпретируем матрицу парных коэффициентов корреляции ПОСЛЕ удаления аномальных наблюдений.

Корреляционная матрица для количественных переменных:
```{r, comment = NA}
cor_m11 <- cor(data_new[,-1], method = 'pearson')
cor_m11
```
Наиболее сильная положительная связь наблюдается между переменными `Thinness_5_9` и `Thinness_10_19` ($\hat{\rho}= 0,95$), `Life_exp` и `Schooling` ($\hat{\rho}= 0,78$), также присутствует сильная связь между `GDP` И `Schooling` ($\hat{\rho}= 0,81$), `Life_exp` и `GDP` ($\hat{\rho}= 0,78$). Наиболее сильная обратная связь наблюдается между переменными `Life_exp` и `Adult_mort` ($\hat{\rho}= -0,76$), `Life_exp` и `Thinness_5_9` ($\hat{\rho}= -0,58$), `GDP` и `Adult_mort` ($\hat{\rho}= -0,59$). Самая слабая связь, как и прежде, наблюдается между переменными `Dipht` и `Thinness_10_19` ($\hat{\rho}= -0,09$). В целом, видим, что между зависимой переменной `Life_exp` и остальными объясняющими переменными довольно сильная связь, за исключением `Total_exp`, где коэффициент $\hat{\rho}<0,3$ (очень слабая связь), что весьма странно, учитывая, что `Total_exp` показывает,грубо говоря, траты государства на здравоохранение. Можем отметить, что в результате удаления выбросов сильно возросла связь между `Life_exp` и `Dipht` (была 0,38, стала 0,56). 

В целом, связь между переменной `Dipht` и остальными переменными стала еще более сильной после удаления выбросов (выбросы, напомним, были найдены только по переменной `Dipht`).

Корреляционная матрица для количественных переменных с p-value:
```{r, comment = NA}
cor_m22 <- rcorr(as.matrix(data_new[,-1]), type = 'pearson')
cor_m22
```
Получив значения p-value, можем сделать вывод о статистической значимости парных коэффициентов корреляции. Видно, что все коэффиенты, для которых p-value больше 0,05, являются незначимыми на уровне значимости 0,95: это особенно справедливо, как и раньше, для переменных `Dipht` и `Thinness_10_19`

И снова, как и в случае до удаления выбросов, получим более наглядную форму представления матрицы парных коэффициентов корреляции.

Корреляционная матрица:
```{r, comment = NA}
corrplot(as.matrix(cor_m11), type = 'full', method = 'circle', tl.col = 'black', tl.srt = 45)
```
```{r, comment = NA}
corrplot.mixed(cor(data_new[,-1]), lower.col = 'black', upper = 'ellipse', number.cex = 1, tl.cex = 0.75)
```


Таким образом, в большинстве случаев положительная и обратная связь между переменными стала незначительно слабее после удаления выбросов (в паре случаев связь, наоборот, незначительно возросла по модулю). Однако и положительная, и обратная связь между переменной Dipht и остальными переменными стала сильнее.


Проверим значимость коэффициентов корреляции. Комбинируем оценку корреляционной матрицы с результатами тестов на значимость:
```{r, comment = NA}
res1 <- cor.mtest(data_new[,-1], conf.level = 0.95)
corrplot(cor(data_new[,-1]), p.mat = res1$p, sig.level = 0.05)
```
Видим статистически незначимые коэффициенты корреляции на 5% уровне значимости (показаны крестами): следовательно, связи между `Thinness_10_19` и `Dopht`, а также между `Total_exp` и `Adult_mort` являются статистически незначимыми. Коэффициенты парной корреляции между зависимой переменной Life_exp и остальными объясняющими переменными являются значимыми. 


Итак, связь между зависимой переменной `Life_exp` и остальными объясняющими является в большинстве своем средней, так как значения коэффициентов корреляции больше `0,5`, но меньше `0,8`. Однако до удаления выбросов связь между `Life_exp` и `Total_exp`, а также `Life_exp` и `Dipht` является умеренной (коэффициенты больше 0,3, но меньше 0,5). После удаления выбросов связь между `Life_exp` и `Total_exp` стала слабой (коэффициенты больше 0,1, но меньше 0,3), а связь между `Life_exp` и `Dipht` стала средней. Кроме того, как говорилось выше, в большинстве случаев положительная и обратная связь между переменными стала немного слабее после удаления выбросов. Однако и положительная, и обратная связь между переменной `Dipht` и остальными переменными стала сильнее после удаления выбросов. Можно также отметить, что связь между переменной `Total_exp` и всеми остальными либо слабая, либо умеренная. Сильную связь, очень близкую к линейной функциональной, видим только между переменными `Thinness_5_9` и `Thinness_10_19`, чуть слабее, но всё ещё очень сильную связь -- между `GDP` и `Schooling`.


##### **5.3**

Для описания доверительных интервалов коэффициентов корреляции необходимо вновь вывести результат выполнения функции `cor.mtest(data_new[,-1], conf.level = 0.95)`, который содержит границы интервалов для коэффициентов корреляции, построенные на 95%-ном уровне значимости:
```{r, comment = NA}
res1
```
lowCI показывает нижнюю границу доверительного интервала для коэффициента корреляции, uppCI - верхнюю (в порядке, соответствующем матрице парных коэффициентов корреляции). Описывать все интервалы было бы слишком долгим занятием, потому что по результатам прошлой работы получилось всего 2 незначимых коэффициента, поэтому выведем только их:

- между `Total_exp` и `Adult_mort`: $\hat{\rho} \in (-0.27;-0.0228)$
- между `Dipht` и `Thinness_10_19`: $\hat{\rho} \in (-0.24;-0.06)$

Все остальные коэффициенты корреляции являются значимыми. Приведём лишь некоторые, касающиеся результирующей переменной `Life_exp`:

- между `Life_exp` и `Adult_mort`: $\hat{\rho} \in (-0.81;-0.68)$
- между `Life_exp` и `Alcohol`: $\hat{\rho} \in (0.39;0.61)$
- между `Life_exp` и `Total_exp`: $\hat{\rho} \in ( 0.1255;0.4)$
- между `Life_exp` и `Dipht`: $\hat{\rho} \in (0.44;0.65)$
- между `Life_exp` и `GDP`: $\hat{\rho} \in (0.71;0.83)$
- между `Life_exp` и `Thinness_5_9`: $\hat{\rho} \in (-0.67;-0.47)$
- между `Life_exp` и `Thinness_10_19`: $\hat{\rho} \in (-0.64;-0.43)$
- между `Life_exp` и `Schooling`: $\hat{\rho} \in (0.71;0.83)$

##### **5.4**
На предыдущем этапе было необходимо проанализировать парные коэффициенты корреляции, которые характеризуют тесноту линейной зависимости между двумя переменными на фоне действия всех остальных показателей. Теперь же нужно проанализировать частные коэффициенты корреляции, показывающие тесноту линейной зависимости между двумя переменными при исключении влияния всех остальных показателей, входящих в модель.

Построим и проинтерпретируем матрицу частных коэффициентов корреляции. Проверим значимость коэффициентов корреляции:

```{r, comment = NA}
cor_p <- pcor(data_new[,-1])
cor_p$estimate
cor_p$p.value
```
С помощью полученной матрицы можем увидеть силу и направление связи между двумя переменными при фиксированном влиянии остальных переменных. Наиболее сильная положительная связь наблюдается между переменными `Thinness_10_19` и `Thinness_5_9` (значение коэффициента 0,92), а также `Schooling` и `GDP` (значение коэффициента 0,46 - умеренная связь). Наиболее сильная обратная зависимость наблюдается между переменными Life_exp и Adult_mort (коэффициент -0,57 - средняя связь). Наиболее слабая связь видна между `Life_exp` и `Thinness_10_19` (коэффициент -0,0025).

Заметно, что в очень многих случаях значения частных коэффициентов корреляции стали меньше, чем парные -- это значит, что прочие переменные, которые фиксировались при вычислении частного коэффициента корреляции, **усиливают** связь между переменными. Например, это особенно хорошо иллюстрируется в следующих двух случаях: между `GDP` и `Schooling` и между `Thinness_5_9` и `Thinness_10_19`. Раньше парный коэффициент корреляции между `GDP` и `Schooling` составлял аж 0.81, но частный коэффициент корреляции между `GDP` и `Schooling` меньше чуть ли не в 2 раза (0.46) -- значит, на связь между этими переменными оказывают влияние и прочие факторы. А вот в случае `Thinness_5_9` и `Thinness_5_9`, которые, в общем-то, явно **коллинеарны**, фиксация остальных переменных не особо повлияла на связь -- она всё такая же близкая к линейной функциональной -- и этот результат вполне логичен.


cor_p$p.value дает значения p-value, благодаря которым можно узнать, являются ли частные коэффициенты корреляции значимыми. Если p-value больше 0.05, то коэфициент является незначимым на уровне значимости 0,95. Так, незначимыми являются частные коэффициенты корреляции, характеризующие тесноту связи (при фиксированном влиянии остальных переменных) между переменными `Life_exp` и `Alcohol`, `Life_exp` и `Total_exp`, `Life_exp` и `Thinness_5_9`, `Life_exp` и `Thinness_10_19`. `Total_exp` и `Adult_mort`, `Dipht` и `Adult_mort`, `GDP` и `Adult_mort`, `Thinness_5_9` и `Adult_mort`, `Thinness_10_19` и `Adult_mort`, `Dipht` и `Alcohol`, `Thinness_5_9` и `Alcohol`, `Thinness_10_19` и `Alcohol`, `Schooling` и` Alcohol`, `Dipht` и `Total_exp`, `GDP` и `Total_exp`, `Thinness_5_9` и `Total_exp`, `Thinness_10_19` и `Total_exp`, `Schooling` и `Total_exp`, `GDP` и `Dipht`, `Thinness_5_9` и `Dipht`, `Thinness_5_9` и `GDP`, `Thinness_10_19` и `GDP`, `Schooling` и `Thinness_5_9`. И это многое говорит о выборке в целом: значит, связь между переменными гораздо сильнее на фоне остальных переменных, с учётом их влияния.

Займёмся построением доверительных интервалов для частных коэффициентов корреляции. Напишем функцию, которая последовательно делает преобразование Фишера, считает интервал на уровне надежности $\gamma = 0.95$ и делает обратное преобразование Фишера. Также было учтено то, что частный коэффициент корреляции может иметь отрицательное значение: 

```{r, comment = NA}
conf_interval <- function(r,n) {
  z = (1/2)*log((1+r)/(1-r))
  t = 1.959963985
  delta = t * sqrt(1/(n-3-2))
  z_l = z - delta
  z_u = z + delta
  r_l = (exp(2*z_l)-1)/(exp(2*z_l)+1)
  r_u = (exp(2*z_u)-1)/(exp(2*z_u)+1)
  rho_l = min(abs(r_l), abs(r_u)) * sign(r_l)
  rho_u = max(abs(r_l), abs(r_u)) * sign(r_u)
  return(c(rho_l, rho_u))
}
```

Далее последовательно по каждой переменной будем с помощью `conf_interval()` выводить доверительные интервалы для частных коэффициентов корреляции, причём всего нужно вывести $C^2_9-9=27$ коэффициентов без учета частных коэффициентов корреляции переменной с самой собой и повторов. Также по результату вывода функции можн определить значимость коэффициента: если интервал содержит 0, то коэффициент незначим:

```{r, comment = NA}
corr <- as.data.frame(cor_p$estimate)
list1 <- corr$Life_exp[2:nrow(corr)]
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list1) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```
```{r, comment = NA}
list2 <- corr$Adult_mort[3:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list2) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```

```{r, comment = NA}
list3 <- corr$Alcohol[4:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list3) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```
```{r, comment = NA}
list4 <- corr$Total_exp[5:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list4) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```
```{r, comment = NA}
list5 <- corr$Total_exp[6:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list5) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```
```{r, comment = NA}
list6 <- corr$Dipht[7:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list6) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```
```{r, comment = NA}
list7 <- corr$Dipht[8:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list7) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```

```{r, comment = NA}
list8 <- corr$Dipht[9:nrow(corr)] # отсечём первые два -- это повторение и 1.
print("Доверительные интервалы для частных коэффициентов корреляции", quote = 0)
for (i in list8) {
  print(i)
  print(conf_interval(i, nrow(data_new)))
}
```

##### **5.5**

Сравнив парные и частные коэффициенты корреляции по зависимой переменной `Life_exp`, видим, что парные коэффициенты корреляции больше частных, что означает, что тесноту связи двух переменных усиливают остальные переменные.То есть связь между зависимой переменной `Life_exp` и каждой объясняющей переменной усиливают остальные объясняющие переменные. Наблюдаются случаи, когда остальные переменные оказывают влияние не только на силу связи между двумя переменными, но и на ее направление. Например, парный коэффициент корреляции для переменных `Adult_mort` и `Alcohol` равен -0,26, а частный коэффициент корреляции составляет 0,18. В целом, в большинстве случаев парные коэффициенты корреляции превышают значения частных и влияние остальных переменных усиливает связь между двумя переменными.


##### **5.6**

Рассчитаем множественный коэффициент корреляции:
```{r, comment = NA}
model <- lm(Life_exp ~ Adult_mort + Alcohol + Total_exp + Dipht + GDP + Thinness_5_9 + Thinness_10_19 + Schooling, data = data_new)
sqrt(summary(model)$r.squared)
```
Проверка его значимости:
$k = 9$ (определили по количеству переменных, не считая первой, отображающей страну); $n = 170$ (по количеству наблюдений); $r_{1/2,...,k} = 0,9008921$, \[F_{набл} = \dfrac{\dfrac 1{k-1}r^2_{1/2,...,k}}{\dfrac 1{n-k}(1-r^2_{1/2,...,k})} = 86,6993224347,\]
\[F_{крит}(0,05; 8; 161) = 1,996324,\]
\[F_{набл} > F_{крит},\] значит, коэффициент является значимым.
Выводы: полученный множественный коэффициент корреляции показывает тесноту линейной связи между переменной `Life_exp` и массивом остальных переменных. Эта связь в данном случае является сильной. Как и должно быть, множественный коэффициент корреляции превышает любой парный и частный коэффициент корреляции, характеризующий статистическую связь результирующего показателя (`Life_exp`).

## **6. Регрессионный анализ. Линейная регрессионная модель**

Теперь перейдем к первому этапу регрессионного анализа -- к построению линейных регрессионных моделей. В качестве зависимой переменной будет рассматриваться средняя продолжительность жизни - `Life_expectancy`. Работать будем с массивом `data_new`, в котором уже была ранее, на этапе корреляционного анализа, произведена обработка выбросов.

Для начала важно упомянуть, что сравнение частных коэффициентов корреляции позволяет в регрессионном анализе ранжировать факторы по тесноте их связи с результирующим показателем (у). Так, например, на этапе проведения корреляционного анализа были получены следующие значения частного коэффициента корреляции между `names(data_new)[2]` и `names(data_new)[3]`, `names(data_new)[4]`, `names(data_new)[5]`, `names(data_new)[6]`, `names(data_new)[7]`, `names(data_new)[8]`, `names(data_new)[9]`, `names(data_new)[10]`: -0.564792428, 0.1100511138, 0.094119555, 0.16716068, 0.25372406, -0.0980522791, -0.002507440, 0.269340887. Видно, что самые большие (по модулю) коэффициенты приходятся на `names(data_new)[3]`, `names(data_new)[6]` и `names(data_new)[10]`, и, как позже будет видно, регрессия именно по этим переменным будет наиболее значима (это можно оценить даже на глаз, посмотрев на графики истинной зависимости и уравнения регрессии, построенные по каждой модели).

#### **6.1. Построение двумерных моделей линейной регрессии**

Начнем с построения двумерной модели линейной регрессии. Аккуратно проинтерпретируем двумерные модели значимых переменных. Такими являются: `Adult_mort`, `Dipht`, `GDP`, `Schooling`. Такой вывод был сделан по результатам построения многомерной линейной регрессионной модели, которую можно увидеть в следующем пункте. Графики остатков будем смотреть только для проверки гетероскедастичности остатков (график самих остатков и график квантиль-квантиль), выводить их не будем, чтобы не перегружать и без того объёмный файл. Для это просто пропишем `include = FALSE`:
```{r, comment = NA}
lm1 <- lm(Life_exp ~ Adult_mort, data_new)
lm2 <- lm(Life_exp ~ Dipht, data_new)
lm3 <- lm(Life_exp ~ GDP, data_new)
lm4 <- lm(Life_exp ~ Schooling, data_new)
```

Для начала рассмотрим `lm1`:
```{r, comment = NA}
summary(lm1)
```

Модель регрессии выглядит следующим образом: $M(y|x)=\hat{y}=80.71-0.06x$, где x -- это `Adult_mort`. Коэффициент детерминации первой модели равен 0,5723, следовательно, смертность взрослых (`Adult_mort`) объясняет $57.23\%$ (что не очень много) вариации продолжительности жизни (результирующей переменной `Life_exp`). p-value < 2.2e-16, что гораздо меньше всякого разумного уровня значимости, следовательно, нулевая гипотеза $H_0: \beta_1=0$ (т.е. гипотеза о том, что независимая переменная не объясняет зависимую), отвергается. Свободный член $\hat{\beta}_0=80.71$ и определяет точку пересечения линии регрессии с осью ординат, показывая среднее значение `y` в точке $x=0$, где x -- объясняющая переменная. Коэффициент $\hat{\beta}_0=-0.06$ показывает, что при увеличении `Adult_mort` на единицу своего измерения переменная `Life_exp` уменьшится **в среднем** на -0.06, и это вполне логично: чем меньше людей в расчёте на 1000 человек умирает, тем дольше такие люди могут жить, хотя стоит отметить, что это лишь корреляционная зависимость. Также построим график: как видно, модель не учитывает особенности истинной зависимости, и остатки также, хоть и неоднородны, но отклоняются от нормального распределения:

```{r}
plot_model(lm1, type = 'slope')
```

```{r, include=FALSE}
res1 <- lm1$residuals
plot(seq(1, nrow(data_new), 1), res1, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res1), col = 'red', lwd = 2)

hist(res1, breaks = sqrt(length(res1)), xlab = 'остаток', ylab = 'частота',
main = 'Гистограмма распределения частот остатков')

qqnorm(res1, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
main = 'Нормальный график квантиль-квантиль')
qqline(res1, col = 'red', lwd = 2)
```


Рассмотрим `lm2`:
```{r, comment = NA}
summary(lm2)
```
Модель регрессии выглядит следующим образом: $M(y|x)=\hat{y}=37.35+0.39x$, где x -- это `Dipht`. Коэффициент детерминации второй модели равен 0.2596, следовательно, `Dipht` объясняет всего $25.96\%$ (что относительно мало) вариации продолжительности жизни (результирующей переменной `Life_exp`). p-value < 3.601e-15, что гораздо меньше всякого разумного уровня значимости, следовательно, нулевая гипотеза $H_0: \beta_1=0$ (т.е. гипотеза о том, что независимая переменная не объясняет зависимую), отвергается. Свободный член $\hat{\beta}_0=37.35$ и определяет точку пересечения линии регрессии с осью ординат, показывая среднее значение `y` в точке $x=0$, где x -- объясняющая переменная. Коэффициент $\hat{\beta}_0=0.39$ показывает, что при увеличении `Dipht` на единицу своего измерения переменная `Life_exp` увеличится **в среднем** на 0.39. И это логично: действительно, чем лучше дело в стране обстоит с иммунизацией от дифтерии, столбняка и коклюша, тем выше продолжительность жизни, хотя нужно понимать, что речь идёт не о **причинно-следственной** связи, а о корреляции. Также построим график: как видно, модель учитывает направление истинной зависимости, но всё равно имеются особенности, которые не учитываются моделью, а остатки гетероскедастичны, очень похожи на нормально распределённые:

```{r, comment = NA}
plot_model(lm2, type = 'slope')
```
```{r, include=FALSE}
res1 <- lm2$residuals
plot(seq(1, nrow(data_new), 1), res1, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res1), col = 'red', lwd = 2)

hist(res1, breaks = sqrt(length(res1)), xlab = 'остаток', ylab = 'частота',
main = 'Гистограмма распределения частот остатков')

qqnorm(res1, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
main = 'Нормальный график квантиль-квантиль')
qqline(res1, col = 'red', lwd = 2)
```

Рассмотрим `lm3`:
```{r, comment = NA}
summary(lm3)
```
Модель регрессии выглядит следующим образом: $M(y|x)=\hat{y}=23.55+5.26x$, где x -- это `GDP`. Коэффициент детерминации третьей модели равен 0.6067, следовательно, ВВП (`GDP`) объясняет всего $25.96\%$ (что относительно мало) вариации продолжительности жизни (результирующей переменной `Life_exp`). Есть основания полагать, что, как и в случае с корреляционным анализом, эта переменная вносит куда больший вклад при учёте влияния остальных, нежели изолированно -- это можно проверить при построении множественной регрессии. p-value < 2.2e-16, что гораздо меньше всякого разумного уровня значимости, следовательно, нулевая гипотеза $H_0: \beta_1=0$ (т.е. гипотеза о том, что независимая переменная не объясняет зависимую), отвергается. Свободный член $\hat{\beta}_0=68.51$ и определяет точку пересечения линии регрессии с осью ординат, показывая среднее значение `y` в точке $x=0$, где x -- объясняющая переменная. Коэффициент $\hat{\beta}_0=0.39$ показывает, что при увеличении `GDP` на единицу своего измерения переменная `Life_exp` увеличится **в среднем** на 5.26 -- большое улучшение. И это логично: действительно, чем выше ВВП страны, тем выше продолжительность жизни -- недаром ВВП является компонентом ИЧР. Также построим график: как видно, модель учитывает направление истинной зависимости и в целом хорошо её приближает, хотя есть отклонение в хвостах. Остатки гетероскедастичны, очень похожи на нормально распределённые, хотя заметна асимметрия:


```{r}
plot_model(lm3, type = 'slope')
```

```{r, include=FALSE}
res1 <- lm3$residuals
plot(seq(1, nrow(data_new), 1), res1, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res1), col = 'red', lwd = 2)

hist(res1, breaks = sqrt(length(res1)), xlab = 'остаток', ylab = 'частота',
main = 'Гистограмма распределения частот остатков')

qqnorm(res1, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
main = 'Нормальный график квантиль-квантиль')
qqline(res1, col = 'red', lwd = 2)
```

Наконец, рассмотрим `lm4`:
```{r, comment = NA}
summary(lm4)
```

Модель регрессии выглядит следующим образом: $M(y|x)=\hat{y}=43.71-2.2x$, где x -- это `Schooling`. Коэффициент детерминации четвертой модели равен 0.6074, следовательно, ВВП (`Schooling`) объясняет $60.74\%$ вариации продолжительности жизни (результирующей переменной `Life_exp`), что является большим процентом, как бы странно это ни было (на фоне остальных переменных кажется, что те чисто логически должны вложить больший вклад, однако на деле всё не так. Наверное, дело в том, что речь идёт о линейной зависимости). p-value < 2.2e-16, что гораздо меньше всякого разумного уровня значимости, следовательно, нулевая гипотеза $H_0: \beta_1=0$ (т.е. гипотеза о том, что независимая переменная не объясняет зависимую), отвергается. Свободный член $\hat{\beta}_0=43.71$ и определяет точку пересечения линии регрессии с осью ординат, показывая среднее значение `y` в точке $x=0$, где x -- объясняющая переменная. Коэффициент $\hat{\beta}_0=2.2$ показывает, что при увеличении `Schooling` на единицу своего измерения переменная `Life_exp` увеличится **в среднем** на 2.2. И это логично: действительно, чем больше лет тратит гражданин на обучение в школе в стране, тем выше продолжительность жизни -- хотя здесь наверняка имеет место не **причинно-следственная** связь, а корреляция, потому что явно дольше учиться могут позволить себе те дети, которые не вынуждены выживать. Также построим график: как видно, модель неплохо приближает истинную зависимость, остатки гетероскедастичны, очень похожи на нормально распределённые, хотя заметна асимметрия:


```{r, comment = NA}
plot_model(lm4, type = 'slope')
```

```{r, include=FALSE}
res1 <- lm4$residuals
plot(seq(1, nrow(data_new), 1), res1, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res1), col = 'red', lwd = 2)

hist(res1, breaks = sqrt(length(res1)), xlab = 'остаток', ylab = 'частота',
main = 'Гистограмма распределения частот остатков')

qqnorm(res1, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
main = 'Нормальный график квантиль-квантиль')
qqline(res1, col = 'red', lwd = 2)
```

Итак, наилучшие модели среди моделей двумерной линейной регрессии по всем рассмотренным параметрам -- это модели зависимости `Life_exp` и `GDP`, `Life_exp` и `Schooling`, а также `Life_exp` и `Adult_mort`. Именно в этих моделях на долю объясняющих переменных приходится максимальное из рассмотренных значение объяснённой доли вариации, а графики уравнения регрессии неплохо приближают истинные зависимости.


#### **6.2. Построение множественной линейной регрессии**

Теперь, когда рассмотрены все двумерные модели линейной регрессии, можно перейти к анализу линейной модели множественной регрессии, которая, по нашим гипотезам, исходя из анализа частных коэффициентов корреляции, должна показывать хороший результат. Проверим это:

```{r, comment = NA}
lm5 <- lm(Life_exp ~ Adult_mort + Dipht + GDP + Schooling, data_new)
```

Модель регрессии выглядит следующим образом: $M(y|x)=\hat{y}=46.03-0.03x_1+0.046x_4+1.53x_5+0.97x_8$, где x_i -- это одно из значений в упорядоченном множестве {`names(data_new)[3]`, `names(data_new)[4]`, `names(data_new)[5]`, `names(data_new)[6]`, `names(data_new)[7]`, `names(data_new)[8]`, `names(data_new)[9]`}. Коэффициент детерминации модели равен 0.791, следовательно, все переменные объясняют $79.1\%$ вариации продолжительности жизни (результирующей переменной `Life_exp`). p-value < 2.2e-16, что гораздо меньше всякого разумного уровня значимости, следовательно, нулевая гипотеза об одновременном равенстве нулю всех
коэффициентов при x $H_0: \beta_i=0$ (т.е. гипотеза о том, что независимые переменные не объясняют зависимую), отвергается. Свободный член $\hat{\beta}_0=46.03$ и определяет точку пересечения линии регрессии с осью ординат, показывая среднее значение `y` в точке $x=0$, где x -- объясняющая переменная. Коэффициенты, как видно, несколько поменялись: видно сильное влияение `GDP` ($\beta = 1.53$) (которое теперь наивысшее, что и ожидалось), `Schooling` ($\beta = 0.97$). Интересно, что влияние `Adult_mort` больше не такое сильное. Также построим график наблюдаемых и модельных значений зависимой переменной. Было проведено тестирование на 4 значимых переменных, но в качестве демонстрации была оставлена переменная `Adult_mort`, наблюдения которой имеют необычную форму 2 кластеров. Как видно, модель очень неплохо приближает истинную зависимость: замечательно то, что модель "увидела", что данные по форме представляют собой два кластера точек, и в правой части первого квадранта модель также демонстрирует очень хорошую обощающую способность (и это подтверждает второй график, на котором синей линией показана сама регрессионная модель, а чёрными точками -- истинные значения). По остальным переменным -- это осталось за кадром -- переменная проявила себя столь же хорошо. Остатки модели гетероскедастичны, очень похожи на нормально распределённые, хотя заметна асимметрия; p-value = 0.002, что является довольно низким значением, но можно подобрать **разумный** уровень значимости, на котором гипотеза о нормальном распределении остатков не отвергается:

```{r, comment = NA}
pred <- predict(lm5)
df_true <- data.frame(x = data_new$Adult_mort, y = data_new$Life_exp)
df_true$продолжительность_жизни <- 'наблюдаемая'
df_pred <- data.frame(x = data_new$Adult_mort, y = pred)
df_pred$продолжительность_жизни <- 'модельная'
df <- rbind.data.frame(df_true, df_pred)
ggplot(df, aes(x = x, y = y)) + geom_point(aes(color = продолжительность_жизни)) + labs(x = 'Adult_mort', y = 'продолжительность жизни')
```

```{r, comment = NA}
pred <- predict(lm5)
plot(seq(1, nrow(data_new), 1), data_new$Life_exp, pch = 16, xlab = 'номер наблюдения', ylab = 'предсказываемая продолжительность жизни')
lines(seq(1, nrow(data_new), 1), pred, col = 'blue', lwd = 2)

```

```{r, comment = NA}
res1 <- lm5$residuals
plot(seq(1, nrow(data_new), 1), res1, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res1), col = 'red', lwd = 2)

hist(res1, breaks = sqrt(length(res1)), xlab = 'остаток', ylab = 'частота',
main = 'Гистограмма распределения частот остатков')

qqnorm(res1, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
main = 'Нормальный график квантиль-квантиль')
qqline(res1, col = 'red', lwd = 2)

sf.test(res1)
```

#### **6.3. Выбор лучшей модели**

Определим лучшую линейную регрессионную модель с помощью информационного критерия Акаике:

```{r}
AIC(lm1)
AIC(lm2)
AIC(lm3)
AIC(lm4)
AIC(lm5)
```

Согласно данному критерию, лучшей является модель множественной линейной регрессии (lm5), потому что значение AIC этой модели получилось наименьшим. Это подтверждает сделанные ранее выводы. В дальнейшем мы будем сравнивать эту модель с лучшей моделью нелинейной регрессии.

#### **6.4. Корректная запись уравнения регрессии. Выводы**

Модель регрессии выглядит следующим образом: $M(y|x)=\hat{y}=49.24-0.03x_1+0.12x_2+0.13x_3+0.07x_4+1.4x_5-1.14x_6-0.03x_7+0.65x_8$, где x_i -- это одно из значений в упорядоченном множестве {`names(data_new)[3]`, `names(data_new)[4]`, `names(data_new)[5]`, `names(data_new)[6]`, `names(data_new)[7]`, `names(data_new)[8]`, `names(data_new)[9]`}

Найдем для данной модели коэффициенты эластичности и проинтерпретируем их. 

```{r}
elas1 <- as.numeric(lm5$coefficients["Adult_mort"]*mean(data_new$Adult_mort)/mean(data_new$Life_exp))
elas2 <- as.numeric(lm5$coefficients["Dipht"]*mean(data_new$Dipht)/mean(data_new$Life_exp))
elas3 <- as.numeric(lm5$coefficients["GDP"]*mean(data_new$GDP)/mean(data_new$Life_exp))
elas4 <- as.numeric(lm5$coefficients["Schooling"]*mean(data_new$Schooling)/mean(data_new$Life_exp))
elas1
elas2
elas3
elas4
```

Коэффициент эластичности `Adult_mort` равен -0.06634738, это говорит о том, что зависимая переменная `Life_exp` изменится на данную величину при изменении `Adult_mort` на 1%. Аналогично для остальных рассматриваемых переменных. Можно заметить, что наибольшей эластичностью обладает переменная `GDP`, а наименьшей - `Dipht`.


По итогам линейного регрессионного анализа было выявлено, что оптимальной моделью является многомерная линейная регрессионная модель с 4 объясняющими переменными (предикторами): `Adult_mort`, `Dipht`, `GDP`, `Schooling`. Были также проанализированы коэффициенты эластичности.

## **7. Регрессионный анализ. Нелинейная (степенная) регрессионная модель**

Было решено умножить значения переменной `Alcohol` на 1000 (перевести количество литров в год в миллилитры в год), чтобы после логарифмирования у нас не было отрицательных значений.

```{r}
data_new$Alcohol <- data_new$Alcohol * 1000
```

Для проведения регрессионного анализа потребуется удалить выбросы из наших данных, чтобы они не искажали результат.

```{r, comment=NA}
# удаляем наблюдения, для которых присутствуют отрицательные значения после логарифмирования:
data2 <- data_new[-(which(data_new$Thinness_5_9 <= 0)),] 
data2 <- data2[-(which(data2$Thinness_10_19 <= 0)),]
```

После всех подготовлений у нас осталось 145 наблюдений. Этого более чем достаточно для анализа. 

Глядя на линейную модель множественной регрессии (предыдущий пункт), можно понять, что наименьшее влияние на зависимую переменную оказывают переменные `Total_exp`, `Thinness_5_9` и `Thinness_10_19`. Именно их и стоит взять за основу нелинейных регрессионных моделей. Для начала построим графики зависисомости переменной `Life_exp` (Y) от этих трех переменных (X), чтобы попробовать подобрать оптимальную нелинейную модель.

```{r}
mlm <- lm(Life_exp ~ Adult_mort + Alcohol + Total_exp + Dipht + GDP + Thinness_5_9 + Thinness_10_19 + Schooling, data_new)
summary(mlm)

plot(x = data2$Total_exp, y = data2$Life_exp)
plot(x = data2$Thinness_5_9, y = data2$Life_exp)
plot(x = data2$Thinness_10_19, y = data2$Life_exp)
```

Видно, что связь продолжительности жизни с `Thinness_5_9` и `Thinness_10_19` отдаленно напоминает линейную. В совокупности с тем фактом, что эти переменные оказывают малое влияние на зависимую, можно предположить, что вклад этих переменных уже учтен другими. Что касается зависимости продолжительности жизни от `Total_exp`, то она кажется совсем хаотичной и непохожей ни на одну из известных функций.

Соответственно, в нашей ситуации мы не можем улучшить регрессионную модель засчет нелинейных зависимостей. Но, поскольку нелинейные регрессионные модели все равно требуется построить, сейчас они будут построены.

#### **7.1. Полиномиальная модель**

Попробуем рассмотреть квадратическую зависисмость продолжительности жизни от переменной `Total_exp`:

```{r}
data2$Total_exp_2 <- data2$Total_exp^2
nlm1 <- lm(Life_exp ~ Adult_mort + Alcohol + Total_exp_2 + Total_exp + Dipht + GDP + Thinness_5_9 + Thinness_10_19 + Schooling, data2)
summary(nlm1)
```
Коэффициент детерминации равен 0.793, это означает, что 79,3% вариации продолжительности жизни может быть объяснено всеми остальными переменными в этой модели. p-value < 2.2e-16, что меньше любого разумного уровня значимости, значит гипотеза о том, что независимые переменные не объясняют зависимую, отвергается. Стандартная ошибка остатков 3.619, что прилично отличается от нуля, значит данная регрессионная модель не имеет сильной прогнозирующей способности.

#### **7.2. Эскпоненциальная модель**

Эта модель строится с помощью логарифмирования зависимой переменной и дальнейшего построения линейной модели.

```{r}
data2$l_Life_exp <- log(data2$Life_exp)
nlm2 <- lm(l_Life_exp ~ Adult_mort + Alcohol + Total_exp + Dipht + GDP + Thinness_5_9 + Thinness_10_19 + Schooling, data2)
summary(nlm2)
```
Коэффициент детерминации равен 0.7816, это означает, что 78,16% вариации логарифма продолжительности жизни может быть объяснено всеми остальными переменными в этой модели. p-value < 2.2e-16, что меньше любого разумного уровня значимости, значит гипотеза о том, что независимые переменные не объясняют зависимую, отвергается. Стандартная ошибка остатков 0.0551, что достаточно мало отличается от нуля, значит данная регрессионная модель имеет наибольшую прогнозирующую способность из всех построенных моделей.

#### **7.3. Степенная модель**

Степенная модель - противоположность экспоненциальной. Для нее мы логарифмируем все переменные, кроме зависисмой. Поэтому в такой модели нельзя использовать факторные переменные, но у нас их нет.

```{r}
data2$l_Adult_mort <- log(data2$Adult_mort)
data2$l_Alcohol <- log(data2$Alcohol)
data2$l_Total_exp <- log(data2$Total_exp)
data2$l_Dipht <- log(data2$Dipht)
data2$l_GDP <- log(data2$GDP)
data2$l_Thinness_5_9 <- log(data2$Thinness_5_9)
data2$l_Thinness_10_19 <- log(data2$Thinness_10_19)
data2$l_Schooling <- log(data2$Schooling)
nlm3 <- lm(Life_exp ~ l_Adult_mort + l_Alcohol + l_Total_exp + l_Dipht + l_GDP + l_Thinness_5_9 + l_Thinness_10_19 + l_Schooling, data2)
summary(nlm3)

```

Коэффициент детерминации равен 0.6936, это означает, что 69,36% вариации продолжительности жизни может быть объяснено всеми остальными логарифмированными переменными в этой модели. p-value < 2.2e-16, что меньше любого разумного уровня значимости, значит гипотеза о том, что независимые переменные не объясняют зависимую, отвергается. Стандартная ошибка остатков 4.387, что прилично отличается от нуля, значит данная регрессионная модель не имеет сильной прогнозирующей способности.

#### **7.4. Сравнение моделей**

По коэффициентам детерминации $(R^2)$ и RSE (стандартной ошибке остатков) наилучшей является вторая модель - экспоненциальная. Она имеет наибольший коэффициент детерминации (0.7816) при наименьшей ошибке (0.0551). сравним так же модели с помощью информационного критерия Акаике - AIC.

```{r}
AIC(nlm1)
AIC(nlm2)
AIC(nlm3)
```
Однозначно, экспоненциальная модель является наилучшей, так как для нее AIC наименьший.

Проверим гипотезу о нормальном распределении случайной ошибки для каждой модели. Начнем с полиномиальной:

```{r}
res1 <- nlm1$residuals
plot(seq(1, nrow(data2), 1), res1, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res1), col = 'red', lwd = 2)

hist(res1, breaks = sqrt(length(res1)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков')

qqnorm(res1, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res1, col = 'red', lwd = 2)
```

По гистограмме распределение не очень похоже на нормальное, ровно как и по другим графикам. Посмотрим на тесты Пирсона и Шапиро-Уилка:

```{r}
PearsonTest(res1)
shapiro.test(res1)
```
При уровне значимости ниже 0.03308 гипотеза о нормальности распределения не отвергается, так что можно считать распределение стандартной ошибки нормальным для полиномиальной модели.

Проверим теперь экпоненциальную модель

```{r}
res2 <- nlm2$residuals
plot(seq(1, nrow(data2), 1), res2, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res2), col = 'red', lwd = 2)

hist(res2, breaks = sqrt(length(res2)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков')

qqnorm(res2, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res2, col = 'red', lwd = 2)

PearsonTest(res2)
shapiro.test(res2)
```

По  тесту Шапиро-Уилка гипотеза о нормальном распределении остатков не отвергается для экспоненциальной модели, по тесту Пирсона отвергается, но тест Шапиро-Уилка гораздо более точный, так что мы делаем вывод, что гипотеза не отвергается. Получается, по этому параметру данная модель тоже хороша, как и по предыдущим параметрам.

Проверим степенную модель:

```{r}
res3 <- nlm3$residuals
plot(seq(1, nrow(data2), 1), res3, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res3), col = 'red', lwd = 2)

hist(res3, breaks = sqrt(length(res3)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков')

qqnorm(res3, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res3, col = 'red', lwd = 2)

PearsonTest(res3)
shapiro.test(res3)
```

Для данной модели гипотеза скорее не отвергается, чем отвергается, потому что не очень стабильный тест Пирсона ее отверг, тогда как более точный тест Шапиро-Уилка - нет.

Из всего вышенаписанного делаем вывод, что наилучшая модель - экпоненциальная (модель nlm2).

#### **7.5. График наблюдаемых и модельных значений зависимой переменной**

Построим график модельных и фактических значений (предсказанных и истинных) зависимой переменной для наилучшей модели:

```{r}
pred2 <- predict(nlm2) #у с крышечкой
plot(seq(1, nrow(data2), 1), data2$l_Life_exp, pch = 16,
     xlab = 'номер наблюдения', ylab = 'продолжительность жизни')
lines(seq(1, nrow(data2), 1), pred2, col = 'blue', lwd = 2)
```
На данном графике синей линией показаны модельные значения зависимой переменной, а черные точки означают фактические значения данной переменной. Как мы видим, эти элементы графика в целом совпадают, что говорит об относительном качестве модели.


Построим еще один график, подтверждающий качество нашей модели. Во всех моделях достаточно большой вклад вносит переменная `Adult_mort`, поэтому следующий график построим для зависимости продолжительности жизни и взрослой смертности.

```{r}
df_true2 <- data.frame(x = data2$Adult_mort, y = data2$l_Life_exp)
df_true2$продолжительность_жизни <- 'фактическая'

df_pred2 <- data.frame(x = data2$Adult_mort, y = pred2)
df_pred2$продолжительность_жизни <- 'модельная'

df2 <- rbind.data.frame(df_true2, df_pred2)

ggplot(df2, aes(x = x, y = y)) +
  geom_point(aes(color = продолжительность_жизни)) +
  labs(x = 'смертность взрослых', y = 'продолжительность жизни')
```

Можно заметить, что фактическое распределение поделено на некоторые 2 кластера, и модельные значения даже повторяют это разделение на кластеры. Это безусловно хороший показатель, значит наша модель достаточно точная.

#### **7.6. Корректная запись уравнения регрессии. Выводы**

Рассматриваемая модель может быть описана следующим уравнением: \[\hat{y} = \sum_{i = 1}^{8} b_{i} \cdot e^{x_i} + \varepsilon = -0.0005096 \cdot e^{x_1} + 0.0000008554 \cdot e^{x_2} + 0.001554 \cdot e^{x_3} + 0.0008225 \cdot e^{x_4} + 0.01259 \cdot e^{x_5} - 0.00682 \cdot e^{x_6} + 0.005637 \cdot e^{x_7} + 0.01079 \cdot e^{x_8} + \varepsilon\]

Найдем для данной модели коэффициенты эластичности и проинтерпретируем их. 

```{r}
elas1 <- as.numeric(nlm2$coefficients["Adult_mort"]*mean(data_new$Adult_mort)/mean(data_new$Life_exp))
elas2 <- as.numeric(nlm2$coefficients["Alcohol"]*mean(data_new$Alcohol)/mean(data_new$Life_exp))
elas3 <- as.numeric(nlm2$coefficients["Total_exp"]*mean(data_new$Total_exp)/mean(data_new$Life_exp))
elas4 <- as.numeric(nlm2$coefficients["Dipht"]*mean(data_new$Dipht)/mean(data_new$Life_exp))
elas5 <- as.numeric(nlm2$coefficients["GDP"]*mean(data_new$GDP)/mean(data_new$Life_exp))
elas6 <- as.numeric(nlm2$coefficients["Thinness_5_9"]*mean(data_new$Thinness_5_9)/mean(data_new$Life_exp))
elas7 <- as.numeric(nlm2$coefficients["Thinness_10_19"]*mean(data_new$Thinness_10_19)/mean(data_new$Life_exp))
elas8 <- as.numeric(nlm2$coefficients["Schooling"]*mean(data_new$Schooling)/mean(data_new$Life_exp))
elas1
elas2
elas3
elas4
elas5
elas6
elas7
elas8
```

Коэффициент эластичности `Adult_mort` равен -0.001031462, это говорит о том, что зависимая переменная `Life_exp` изменится на данную величину при изменении `Adult_mort` на 1%. Аналогично для остальных рассматриваемых переменных. Можно заметить, что наибольшей эластичностью обладает переменная `Schooling`, а наименьшей - `Alcohol`.

#### **8. Регрессионный анализ. Итог**

Сравним линейные и нелинейные модели и выберем наилучшую из них. Лучшей линейной моделью оказалась модель множественной линейной регрессии (lm5), а лучшей нелинейной моделью - экспоненциальная модель (nlm2). Сравним их с помощью информационного критерия Акаике: 

```{r}
AIC(lm5)
AIC(nlm2)
```

Видно, что наименьший коэффициент вышел у нелинейной экспоненциальной модели. Значит данная модель является наилучшей.

### Сопоставление результатов с выдвигаемыми гипотезами
В целом можно сказать, что поти все гипотезы подтвердились.После проевдеения эмпирического исседования мы показали выше, что дейтвительно, все переменные объясняют зависимую переменную. Зависимая переменна не подчиняется нормальному закону при высоких уровнях, однако некоторые тесты показали 0,005, что недостаточно, чтобы говорить о нормальном распределении, так что данную гипотеза оказалась ошибочной. Что касается тесной связи между зависимой переменной и остальными, то эта гипотеза подтвердилась, подтвердилось и то, что была связь прямая между, к примеру, SCHOOLING И GDP и обратная, к примеру, alchohol, то есть гипотеза о разнонаправленности связи между зависимой переменной и некоторыми объясняющими подтвердилась. А вот что касается гипотезы о силе изменения зависимой переменной от изменения иных, мы немного прогадали, так как переменная total expenditure показала себя не так уверенно, как shooling и gdp, так что данная гипотеза частично отвергается, но лишь частично. Вот такие выводы мы получили, анализируя гипотезы до исследования и те результаты, что мы получили.