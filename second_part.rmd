---
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: paged
    keep_tex: yes
    latex_engine: xelatex
  word_document: default
lang: ru-russian
header-includes:
- \newcommand{\bcenter}{\begin{center}}
- \newcommand{\ecenter}{\end{center}}
- \usepackage[russian]{babel}
fontenc: T2A
babel-lang: russian
---

<center>
# **Компьютерная работа №2 <br/> Многомерные статистические методы <br/> 2020-21 гг. — 2 курс**
#### **выполнили: Гильфанова Камиля (БСТ195), Ерёменко Борис (БСТ196), Заславская Анна (БСТ195), Никонова Ксения (БСТ195), Санталова Мария (БСТ195)**
</center>
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

## **1. Импорт данных**
На первом этапе необходимо импортировать все необходимые библиотеки и данные, с которыми будет впоследствии вестись работа. 
``` {r, comment = NA}
library('rio')
library('DescTools')
library('ggpubr')
library('ggplot2')
library('nortest')
library('fBasics')
library('moments')
library('car')
library('dplyr')
library('readxl')
library('tibble')
library('EnvStats')
library('outliers')
library('Hmisc')
library('corrplot')
library('ppcor')
library('DiscriMiner')
library('pracma')
library('FactoMineR')
library('factoextra')
library('psych')
library('htmlTable')
library('REdaS')
library('grid')
library('plotly')
library('sjPlot')
```

Импортируемая таблица данных содержит 2938 наблюдений по 22 переменным. Эти наблюдения характеризуют показатели различных стран за промежуток с 2000 по 2015 год. В качестве рассматриваемого года был выбран 2014, поскольку по ряду объясняющих переменных отсутствуют данные за 2015 год.

```{r, comment = NA}
raw_data <- read.csv('life_expectancy_data.csv', header = TRUE, sep = ',')
data <- raw_data[which(raw_data$Year == 2014),]
rownames(data) <- 1:nrow(data) # Присвоим индексы наблюдениям в соответствии с их естественной очередностью, а не тем порядком, что был в исходной таблице.
data$Adult.Mortality <- as.numeric(data$Adult.Mortality) # приведём условно непрерывные данные, содержащие целые числа, к типу Numeric
data$infant.deaths <- as.numeric(data$infant.deaths)
data$Hepatitis.B  <- as.numeric(data$Hepatitis.B )
data$Measles  <- as.numeric(data$Measles)
data$under.five.deaths <- as.numeric(data$under.five.deaths)
data$Polio <- as.numeric(data$Polio)
data$Diphtheria <- as.numeric(data$Diphtheria)
data <- data[, !(names(data) == "Year")] # Теперь, когда рассматривается только 2014 год, эта переменная более не информативна.
str(data)
```

По итогам предварительного анализа имеющихся переменных, проведённого в первой компьютерной работе, было решено взять 9 переменных — 8 объясняющих и 1 зависимую, — которые нам потребуются для дальнейшего анализа. Также была оставлена одна дамми-переменная, которая позволит ориентироваться в данных:

```{r, warning=FALSE, comment=NA}
data <- data[, c("Country", "Life.expectancy", "Adult.Mortality",
                    "Alcohol", "Total.expenditure", "Diphtheria",
                    "GDP", "thinness.5.9.years",
                    "thinness..1.19.years", "Schooling")]

# Также упростим названия переменных
names(data) <- c("Country", "Life_exp", "Adult_mort",
                    "Alcohol", "Total_exp", "Dipht",
                    "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")
```

Наконец, был получен итоговый набор данных, показывающий данные по различным странам по 9 информативным показателям за 2014 год. Обработка данных, тем не менее, на этом ещё не окончена: необходимо провести обработку пропусков, которая будет проведена по той же схеме, что и при выполнении первой компьютерной работы (без сопутствующих пояснений):

```{r, warning=FALSE, comment=NA}
# Пропуски по `Alcohol`
data$Alcohol[151] <- 0.01 

# Пропуски по `Total_exp`
data$Total_exp[45] <- 6.1 # КНДР 
data$Total_exp[149] <- 4.3 # Сомали

# Пропуски по `GDP`
gdp_da <- read.csv("gdp_data.csv", sep = ",", header = FALSE)
gdp <- gdp_da[,1:ncol(gdp_da)]
data$GDP <- gdp$V6

# Пропуски по `Thinness`
data$Thinness_5_9[151] <- 7.3
data$Thinness_5_9[154] <- 7.6

data$Thinness_10_19[151] <- 7.8
data$Thinness_10_19[154] <- 7.9

# Пропуски по `Schooling`
data$Schooling[28] <- 5.4
data$Schooling[44] <- 12.5
data$Schooling[45] <- 7.8
data$Schooling[46] <- 6.7
data$Schooling[131] <- 12.5
data$Schooling[132] <- 10.0
data$Schooling[149] <- 4.7
data$Schooling[173] <- 11.8
data$Schooling[174] <- 5.8
data$Schooling[175] <- 12.9
```

Гистограммы распределения и ящики с усами для обновлённых переменных, которые теперь не содержат пропусков и имеют более чёткий и структурированный вид, выглядят следующим образом:

```{r, warning=FALSE, comment=NA}
# boxplot(scale(data[,3:ncol(data)]))

vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)

plot1 <- ggplot(data, aes(x=Life_exp)) + geom_histogram(color="black", fill='#00cccc')
plot2 <- ggplot(data, aes(x=Adult_mort)) + geom_histogram(color="black", fill='#00cccc')
plot3 <- ggplot(data, aes(x=Alcohol)) + geom_histogram(color="black", fill='#00cccc')
plot4 <- ggplot(data, aes(x=Total_exp)) + geom_histogram(color='black', fill='#00cccc')
plot5 <- ggplot(data, aes(x=Dipht)) + geom_histogram(color='black', fill='#00cccc')

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 3)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
print(plot3, vp = vplayout(1, 3))
print(plot4, vp = vplayout(2, 1))
print(plot5, vp = vplayout(2, 2))

plot13 <- ggplot(data, aes(x=GDP)) + geom_histogram(color='black', fill='#00cccc')
plot14 <- ggplot(data, aes(x=Thinness_5_9)) + geom_histogram(color='black', fill='#00cccc')
plot15 <- ggplot(data, aes(x=Thinness_10_19)) + geom_histogram(color='black', fill='#00cccc')
plot16 <- ggplot(data, aes(x=Schooling)) + geom_histogram(color='black', fill='#00cccc')


grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
print(plot13, vp = vplayout(1, 1))
print(plot14, vp = vplayout(1, 2))
print(plot15, vp = vplayout(2, 1))
print(plot16, vp = vplayout(2, 2))
```

Заметно, что гистограммы по большей части напоминают гистограммы данных, гипотетически подчиняющихся нормальному и логнормальному закону распределения (хотя и заметна асимметрия).

Поскольку многие методы кластеризации (в частности, метод k-means и метод ближнего соседа) довольно чувствительны к выбросам, проанализируем наличие аномальных данных по этим переменным:

```{r, warning=FALSE, comment=NA}
IQR3 <- function(y) {
  out_of_3IQR <- boxplot.stats(y, coef = 3)$out
  return(out_of_3IQR)
}
```

```{r, warning=FALSE, comment=NA}
for (i in names(data)[2:9]) {
  print(IQR3(as.matrix(data[i])))
  print(i)
}
```

Там, где это уместно, логарифмируем данные, стабилизировав таким образом дисперсию и уменьшив выбросы. В частности, после логарифмирования переменных `Thinness_5_9`,  `Thinness_10_19` и `GDP` выбросы по этим переменным пропадают:

```{r, warning=FALSE, comment=NA}
data$GDP <- log(data$GDP)
data$Thinness_5_9 <- log(data$Thinness_5_9)
data$Thinness_10_19 <- log(data$Thinness_10_19)
```

Как видно, остаётся лишь переменная `Dipht`, которая не стабилизируется логарифмированием (наоборот, логарифмирование делает разброс и количество выбросов ещё больше). Её было решено оставить в том виде, в каком она имеется: есть основания полагать, что это поможет более информативно выделить кластеры.

## **2. Выделение главных компонент**


Для дальнейшей работы преобразуем датасет, убрав дамми-переменную и зависимую из числа факторов при построении главных компонент:
```{r, warning=FALSE, comment=NA}
data1 <- data[,3:ncol(data)]
str(data1)
```


Перед непосредственно применением метода главных компонент проверим его применимость для используемого датасета.


Снижение размерности является возможным, если признаки, описывающие совокупность, достаточно коррелированы между собой.
```{r, warning=FALSE, comment=NA}
corrplot(cor(data1))
```


Предполагаем, что признаки с высокой корреляцией скорее всего объединятся в одну компоненту. Например, Thinness_5_9 и Thinness_10_19. Видим, что многие признаки достаточно коррелированы.


Второй метод проверки применимости МГК - расчет значения критерия Кайзера-Майера-Олкина (KMO), которое можно рассчитать по формуле:
\[
KMO = \frac{\sum_{i \neq j} \rho_{ij}^2 }{∑_{i \neq j} ρ_{ij}^2 + \sum_{i \neq j} \phi_{ij}^2}
\]
где $\rho_{ij}$ - это парный, а $\phi_{ij}$ - частный коэффициенты корреляции между признаками $i$, $j$. Если значения выборочных частных коэффициентов близки к нулю, а также если значение критерия KMO близко к единице, то выборка адекватна для применения МГК. также функция KMOS выводит меру адекватности выборки (MSA) для каждого признака. Если MSA близка к единице, то признак "хорошо свернется" в компоненты с остальными признаками.

```{r, warning=FALSE, comment=NA}
KMOS(data1)
```


Видим, что значение критерия КМО достаточно близко к 1, что означает применимость МГК к данной выборке. Также значения MSA для всех признаков близки к 1, следовательно, признаки могут хорошо свернуться в компоненты с прочими.


Третьим способом проверки применимости методов снижения размерности является тест сферичности Бартлетта. С помощью этого теста проверяется нулевая гипотеза о том, что теоретическая корреляционная матрица многомерного распределения вектора случайных величин, представляет собой единичную матрицу. В случае опровержения гипотезы предпосылки использования МГК выполняются.
```{r, warning=FALSE, comment=NA}
bart_spher(data1)
```


Гипотеза отвергается, следовательно, нет препятствий для применения МГК.


Проверив тремя способами применимость МГК, приступим непосредственно к выделению главных компонент.


### **2.1. Вывод о числе главных компонент, которые необходимо оставить для дальнейшего анализа**


Используем критерий Кайзера:
```{r, warning=FALSE, comment=NA}
pc <- PCA(data1, graph = FALSE)
pc$eig
```


В соответствии с данным критерием выбираем те главные компоненты, которым соответствует собственное значение > 1 ( отображены в первом столбце).
Таким образом, выделяем две главные компоненты.


Определим число главных компонент по долям суммарной вариации:


Количество главных компонент выбирается по доле суммарной вариации исходных признаков, которую необходимо сохранить (~70%). Выше уже получены необходимые показатели (в третьем столбце). По данному критерию выделяем три главные компоненты, так как доля суммарной вариации составляет 76%.


Наконец, используем критерий каменистой осыпи. Построим "график каменистой осыпи":
```{r, warning=FALSE, comment=NA}
fviz_eig(pc, addlabels = TRUE)
```


Выберем такое число компонент, после которого происходит резкое падение доли сохраненной дисперсии. Резкое падение происходит после первой компоненты, значит, в соответствии с данным критерием, выделяем одну главную компоненту.


Оптимальным решением будет выделить три главные компоненты, так как именно таким образом главные компоненты будут наиболее точны и информативны, как будет видно по матрице факторных нагрузок в дальнейшем.


### **2.2. Описание суммарного вклада первых главных компонент**
```{r, warning=FALSE, comment=NA}
summary(princomp(data1, cor = TRUE))
```


По второй строке (Proportion of variance) определили, что на первую главную компоненту приходится практически половина сохраненной дисперсии. На вторую - значительно меньше (15,5%), что вполне естественно, и на третью - 11,6%. Доля суммарной вариации для трех главных компонент составляет 76,3% (3-я строка, Cumulative Proportion). Кроме того, по первой строке (Standard deviation) можем увидеть величину стандартного отклонения для выделенных ранее трех главных компонент, которые составляют соответственно 1,98;1,11;0,96.


Вклады переменных в процентах в главные компоненты:
```{r, warning=FALSE, comment=NA}
fviz_contrib(pc, choice = "var", axes = 1, top = 10)
fviz_contrib(pc, choice = "var", axes = 2, top = 10)
fviz_contrib(pc, choice = "var", axes = 3, top = 10)
```


Наибольший вклад в первую главную компоненту вносит переменная Schooling, а также переменные GDP, Thinness_5_9 и Thinness_10_19. Во вторую главную компоненту наибольший вклад вносят переменные Dipht, Thinness_5_9 и Thinness_10_19, а наименьший - Alcohol. В третью главную компоненту основной вклад (более 50%) вносит переменная Total_exp. Эти сведения помогают проинтерпретировать главные компоненты.


Коэффициенты парной корреляции между переменными и главными компонентами, отсортированные по значимости:
```{r, warning=FALSE, comment=NA}
res.desc <- dimdesc(pc, axes = 1:3, proba = 0.05)
res.desc$Dim.1
res.desc$Dim.2
res.desc$Dim.3
```


Первая главная компонента имеет самую сильную прямую связь с переменной Schooling, а обратную - с Thinness_5_9. Вторая главная компонента наиболее коррелирована с переменной Dipht, а третья - с Total_exp.


### **2.3. Построение графика накопленного вклада главных компонент в суммарную дисперсию исходного признакового пространства**


Для построения графика сначала выведем стандартные отклонения ( переменная b). Дисперсия является квадратом стандартного отклонения. Посчитаем дисперсии (переменная c). Посчитаем доли дисперсий как отношение дисперсии к суммарной дисперсии (переменная d). Затем выведем кумулятивные величины с помощью функции cumsum (переменная е). Наконец, полученные кумулятивные величины отобразим на графике с помощью функции plot и получим искомый график.  
```{r, warning=FALSE, comment=NA}
a <- princomp(data1, cor = TRUE)
b <- a$sdev
c <- b^2
d <- c / sum(c)
e <- cumsum(d)
plot(e, type = "b", ylim = 0:1)
```


### **2.4. Интерпретация главных компонент на основе анализа матрицы факторных нагрузок**


Матрица факторных нагрузок:
```{r, warning=FALSE, comment=NA}
pca <- principal(cor(data1), nfactors = 3, rotate = "none", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 3, nrow = ncol(data1))

htmlTable(round(pca_loadings,2), caption = "Матрица факторных нагрузок", header = c("PC1", "PC2", "PC3"), rnames = colnames(data1))
```


По данной матрице определяем корреляцию каждой переменной с каждой главной компонентой. При таком виде матрицы факторных нагрузок сложно проанализировать главные компоненты и дать им название, поэтому примением варимакс-вращение для ясной видимости факторов, отмеченных высокими нагрузками для одних переменных и низкими - для других:
```{r, warning=FALSE, comment=NA}
pca <- principal(cor(data1), nfactors = 3, rotate = "varimax", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 3, nrow = ncol(data1))

htmlTable(round(pca_loadings,2), caption = "Матрица факторных нагрузок (с вращением)", header = c("PC1", "PC2", 'PC3'), rnames = colnames(data1))
```


Как видим, варимакс-вращение помогло получить более понятную и интерпретируемую матрицу нагрузок. Первая главная компонента сильно коррелирует с переменными GDP (ВВП), schooling (Количество лет обучения в школе), Adult_mort (уровень смертности среди взрослого населения) (с этой переменной у главной компоненты обратная связь) и Dipht (количество привитых от столбняка детей в возрасте до одного года в процентах). Это означает, что первая главная компонента может быть названа "Уровень жизни населения". Вторая главная компонента значительно коррелирует с переменными Thinness_5_9 (показатель истощения среди детей в возрасте от 5 до 9 лет в процентах, Thinness_10_19 (показатель истощения среди детей и подростков в возрасте от 10 до 19 лет в процентах, Alcohol (потребление алкоголя на душу населения) и Schooling (количество лет обучения в школе). С последними двумя признаками связь обратная. Вторую главную компоненту назовем "Уровень бедности населения". Третья главная компонента по большей части коррелирует с признаками Total_exp (доля расходов государства на здравоохранение от общих расходов) и Dipht (количество привитых от столбняка детей в возрасте до одного года в процентах). Эта главная компонента получит название "Уровень развития систем здравоохранения".


Если построить матрицу факторных нагрузок с вари-макс вращением для двух главных компонент, то заметна неточность интерпретации и размытость определения главных компонент, так как первая главная компонента представляет собой некий позитивный показатель, а вторая - отрицательный. Поэтому принятое решение о выделении трех главных компонент имеет смысл и дает больше пространства для анализирования.
```{r, warning=FALSE, comment=NA}
pca <- principal(cor(data1), nfactors = 2, rotate = "varimax", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 2, nrow = ncol(data1))

htmlTable(round(pca_loadings,2), caption = "Матрица факторных нагрузок (с вращением)", header = c("PC1", "PC2"), rnames = colnames(data1))
```


## **3. Построение уравнения регрессии с использованием выделенных ГК**

### **3.1. Построение линейного уравнения регрессии на ГК**

Для начала учтем тот факт, что при построении линейного уравнения регрессии используются все переменные, присутствующие в data set. По этой причине будет работать с *data*, а не с *data1*. Теперь мы можем перейти к непосредственному построению уравнения.
```{r, warning=FALSE, comment=NA}
pca_results <- PCA(data1, graph = FALSE, ncp = 3)
pc <- as.data.frame(pca_results$ind$coord[, 1:3])
pc1 <- pc$Dim.1
pc2 <- pc$Dim.2
pc3 <- pc$Dim.3
lm1 <- lm(data$Life_exp ~ pc1 + pc2 + pc3)
summary(lm1)
```

Итак, как можно видеть, что значение p-value < 2.2e-16, что очень мало, следовательно, можно утвреждать, что независимые перменные объясняют зависимую. Коэффициент детерминации = 0,7931, связь между переменными очень тесная. 


### **3.2. Сопоставление свойств ранее полученных уравнений регрессии с уравнением регрессии на ГК**

Для начала вспомним, какие модели из прошлой работы оказались лучишими. Касательно линейных уравнений регрессии лучшими были модели зависимости *Life_exp* и *GDP*, также *Life_exp* и *Schooling* и модель множественной линейной регрессии, в которой показывалась зависимость *life_exp* и *Adult_mort*, *Dipht*, *GDP*, *Schooling*. По итогу анализа наиулчшей из всех была признана модель множественной линейной регрессии.
```{r, warning=FALSE, comment=NA}
data_new <- data[-which(data$Dipht %in% IQR3(data$Dipht)),]
lm2 <- lm(Life_exp ~ GDP, data_new)
lm3 <- lm(Life_exp ~ Schooling, data_new)
lm4 <- lm(Life_exp ~ Adult_mort + Dipht + GDP + Schooling, data_new)
summary(lm2)
summary(lm3)
summary(lm4)
```

Все коэффициенты моделей уже интерпретировались в прошлой работе, не будем останавливаться на этом. Теперь вспомним, какая из нелинейных моеделй регрессии была лучшей. Среди всех моделей такой оказалась экспоненциальная. 

```{r, warning=FALSE, comment=NA}
data_new <- data[-which(data$Dipht %in% IQR3(data$Dipht)),]
data_new$Alcohol <- data_new$Alcohol * 1000
# удаляем наблюдения, для которых присутствуют отрицательные значения после логарифмирования:
data2 <- data_new[-(which(data_new$Thinness_5_9 <= 0)),] 
data2 <- data2[-(which(data2$Thinness_10_19 <= 0)),]
data2$l_Life_exp <- log(data2$Life_exp)
nlm1 <- lm(l_Life_exp ~ Adult_mort + Alcohol + Total_exp + Dipht + GDP + Thinness_5_9 + Thinness_10_19 + Schooling, data2)
summary(nlm1)
```

Все претенденты на звание лучшей модели регрессии выявлены. Обратимся к коэффициенту *AIC*, чтобы сравнить их и все же определить лучшую.
```{r, warning=FALSE, comment=NA}
AIC(lm1)
AIC(lm2)
AIC(lm3)
AIC(lm4)
AIC(nlm1)
```

Видно, что наименьшее значение данного коэффициента принадлежит модели *nlm1* - экспоненциальной модели нелинейной регрессии. Таким образом, лучшим уравением становится именно оно. Запись этого уравнения выглядит так: \[\hat{y} = \sum_{i = 1}^{8} b_{i} \cdot e^{x_i} + \varepsilon = -0.0005096 \cdot e^{x_1} + 0.0000008554 \cdot e^{x_2} + 0.001554 \cdot e^{x_3} + 0.0008225 \cdot e^{x_4} + 0.01259 \cdot e^{x_5} - 0.00682 \cdot e^{x_6} + 0.005637 \cdot e^{x_7} + 0.01079 \cdot e^{x_8} + \varepsilon\].

## **4. Кластерный анализ**

### **4.1. Построение и анализ дендрограмм**

Рассмотрим несколько вариантов разбиения объектов на кластеры. Но для начала необходимо рассмотреть коррелированность признаков: если мы собираемся воспользоваться евклидовым расстоянием, то предпочтительна слабая коррелированность. Для этого построим матрицу корреляций, чтобы примерно оценить, насколько качественным будет дальнейшая кластеризация:
```{r, warning=FALSE, comment=NA}
cluster_data <- scale(data[,3:10])
corrplot.mixed(cor(cluster_data), lower.col = 'black', upper = 'ellipse')
```

Как видно, присутствует достаточно число независимых переменных, которые довольно сильно коррелируют. Также размер выборки (>100) говорит нам о том, что к дальнейшему кластерному анализу следует относиться осторожно, поскольку некоторые предпосылки, необходимые для качественного построения кластеров нижеследующими методами, не выполняются в полной мере.

Теперь инициализируем все объекты с построением кластеров по тому или иному методу.

1. Метод ближнего соседа:
```{r, warning=FALSE, comment=NA}
cluster_data <- scale(data[, 3:10])
hclust_nn <- hcut(cluster_data, cex = 0.5, hc_metric = 'euclidian', hc_method = 'single')
```

2. Метод дальнего соседа:
```{r, warning=FALSE, comment=NA}
hclust_fn <- hcut(cluster_data, cex = 1, hc_metric = 'euclidian', hc_method = 'complete')
```

3. Метод центра тяжести:
```{r, warning=FALSE, comment=NA}
hclust_c <- hcut(cluster_data, cex = 1, hc_metric = 'euclidian', hc_method = 'centroid')
```

4. Метод средней связи:
```{r, warning=FALSE, comment=NA}
hclust_av <- hcut(cluster_data, cex = 1, hc_metric = 'euclidian', hc_method = 'average')
```

5. Метод Уорда:
```{r, warning=FALSE, comment=NA}
hclust_w <- hcut(cluster_data, cex = 1 , hc_metric = 'euclidian', hc_method = 'ward.D2')
```

Изобразим результаты иерархической кластеризации через с помощью дендрограммы:

1. Метод ближнего сосдеа:
```{r, warning=FALSE, comment=NA}
fviz_dend(hclust_nn, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип ближнего соседа)', ylab = 'Расстояние')
```

Как можно заметить, на графике отсутствует четкая иерархия кластеров, отсутствуют четкие кластеры. Можно сказать, что такая иерархия является слабой - данная дендрограмма не помогает определить оптимальное количество кластеров. Наглядно можно прикинуть, что оптимальное количество кластеров (судя по такой метрике, как расстояние между местами объединения данных в кластеры) -- 3, но второй кластер, судя по дендрограмме, будет содержать единственный объект, поэтому R автоматически выбрал 2 кластера (на дендрограмме это показано розовым и бирюзовым цветами).

2. Метод дальнего соседа:
```{r, warning=FALSE, comment=NA}
fviz_dend(hclust_fn, cex = 1, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип дальнего соседа)', ylab = 'Расстояние')
```

Дендрограмма, построенная по принципу дальнего соседа, кажется более информативной, деление на кластеры становится как минимум четче: видно, что, судя по расстоянию между местами объединения данных в кластеры, оптимальным числом кластеров будет 2.

3. Метод центра тяжести:
```{r, warning=FALSE, comment=NA}
fviz_dend(hclust_c, cex = 1, main = 'Дендрограмма (принцип центра тяжести)', ylab = 'Расстояние')
```

В случае построения дендрограммы по принципу центра тяжести наблюдается наиболее непонятное иерархическое распределение из всех возможных. Более того, код выдавал бы ошибку, поскольку количество цветов, обычно используемое интерпретатором R, недостаточно для выделения всех кластеров. И по графику это видно: ничего нельзя понять, этот метод является наименее оптимальным и информативным. 

4. Метод средней связи:
```{r, warning=FALSE, comment=NA}
fviz_dend(hclust_av, cex = 1, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип средней связи)', ylab = 'Расстояние')
```

Дендрограмма по принципу средней связи демонстрирует неравномерное и не очень четкое деление на кластеры, однако можно увидеть преобладание наблюдений в одном кластере и явный их недостаток в другом, но судя по расстоянию между объединениями в кластеры, оптимальное число кластеров = 2.

5. Метод Уорда:
```{r, warning=FALSE, comment=NA}
fviz_dend(hclust_w, cex = 1, color_labels_by_k = TRUE, main = 'Дендрограмма (принцип Уорда)', ylab = 'Расстояние')
```

Метод Уорда -- наилучший из представленных выше метод. Дендрограмма по принципу Уорда показывает достаточно сильную иерархию кластеров, деление можно назвать равномерным. 

В итоге, наиболее информативными дендрограммами стали те, которые были построены согласно методу дальнего соседа и методу Уорда. Остальные же не показывали четкой иерархии кластеров и, следовательно, были менее информативными. Однако самым главным наблюдением, которое можно вынести из построения дендрограмм, является то, что оптимальным количеством кластеров в данном случае является 2.

## **5. Использование метода к-средних для классификации объектов**

### **5.1. Деление на кластеры методом к-средних. Построение графика средних значений показателей в кластерах**

Проведем деление на кластеры с помощью алгоритма к-средних. Для начала стандартизуем данные и возьмем только необходимые переменные:

```{r, warning=FALSE, comment=NA}
data2 <- scale(data[,3:10])
```

Рассмортим деление на 2 кластера, потому что метод локтя и метод силуэтов определили это количество как оптимальное.

```{r, warning=FALSE, comment=NA}
set.seed(1)
kmeans2 <- kmeans(data2, centers = 2)
kmeans2
```

Мы получили 2 кластера по 77 и 106 наблюдений соответственно. Но можно заметить, что значение between_SS / total_SS достаточно низкое - 34.1 %). В хороших моделях оно должно стремиться к 100 %. Построим график средних значений для этих двух кластеров:

```{r, warning=FALSE, comment=NA}
plot(1:ncol(data2), kmeans2$centers[1,], type = 'l', col = 'red', lwd = 2, ylim = c(-3, 3),
     ylab = 'Среднее значение признака', xlab = 'Классифицирующий признак', xaxt = 'n')

lines(1:ncol(data2), kmeans2$centers[2,], type = 'l', col = 'green', lwd = 2)


title('График средних')

axis(1, at = 1:ncol(data2), labels = colnames(data2), las = 2)

legend(1, -1.3, c('Кластер 1', 'Кластер 2'),
       lwd = c(2, 2, 2, 2), col = c('red', 'green'))
```

Заметно, что кластеры различаются по всем переменным. Второй кластер, однако, получился достаточно большой. Поэтому мы решили, было бы логично рассмотреть возможность деления на большее количество кластеров. Тем более это, вероятнее всего, увеличит статистику between_SS / total_SS. Поделим на 3 кластера:

```{r, warning=FALSE, comment=NA}
set.seed(2)
kmeans3 <- kmeans(data2, centers = 3)
kmeans3
```

Мы получили кластеры по 79, 43 и 61 наблюдений, что кажется лучше, чем в прошлом случае. Статистика between_SS / total_SS заметно увеличилась до 45.5 %. Сравним график средних с предыдущим случаем:

```{r, warning=FALSE, comment=NA}
plot(1:ncol(data2), kmeans3$centers[1,], type = 'l', col = 'red', lwd = 2, ylim = c(-3, 3),
     ylab = 'Среднее значение признака', xlab = 'Классифицирующий признак', xaxt = 'n')

lines(1:ncol(data2), kmeans3$centers[2,], type = 'l', col = 'green', lwd = 2)
lines(1:ncol(data2), kmeans3$centers[3,], type = 'l', col = 'blue', lwd = 2)

title('График средних')

axis(1, at = 1:ncol(data2), labels = colnames(data2), las = 2)

legend(1, -1.3, c('Кластер 1', 'Кластер 2', 'Кластер 3' ),
       lwd = c(2, 2, 2, 2), col = c('red', 'green', 'blue'))
```

По графику ситуация не сильно отличается от случая с двумя кластерами - так же заметные различия по всем переменным. Учитывая все вышеупомянутые факторы, мы считаем более грамотным в случае метода к-средних рассматривать деление на 3 кластера, а не на 2.


### **5.2. Проверка гипотезы о равенстве средних в кластерах**

На всякий случай проверим гипотезу о равенстве средних значений в кластерах. Для этого сначала проверим гипотезу о равенстве дисперсий по каждому признаку в каждой паре кластеров (1 и 2, 2 и 3, 1 и 3):

```{r, warning=FALSE, comment=NA}
Cluster <- kmeans3$cluster
data_clusters <- data.frame(data2, Cluster)
cluster1 <- data_clusters[which(data_clusters$Cluster == 1),]
cluster2 <- data_clusters[which(data_clusters$Cluster == 2),]
cluster3 <- data_clusters[which(data_clusters$Cluster == 3),]
for (i in (1:8)) {
  var.test(cluster1[,i], cluster2[,i], alternative = 'greater')
  var.test(cluster1[,i], cluster3[,i], alternative = 'greater')
  var.test(cluster2[,i], cluster3[,i], alternative = 'greater')
  #print(var.test(cluster1[,i], cluster2[,i], alternative = 'greater'))
  #print(var.test(cluster1[,i], cluster3[,i], alternative = 'greater'))
  #print(var.test(cluster2[,i], cluster3[,i], alternative = 'greater'))
}
```

Гипотеза о равенстве дисперсий отверглась лишь в 3 случаях из 24, так что можно считать дисперсии равными. Значит мы можем использовать t-тест Уэлча для проверки гипотезы о равенстве средних.

```{r, warning=FALSE, comment=NA}
for (i in (1:8)) {
  t.test(cluster1[,i], cluster2[,i], var.equal = T, paired = FALSE)
  t.test(cluster1[,i], cluster3[,i], var.equal = T, paired = FALSE)
  t.test(cluster3[,i], cluster2[,i], var.equal = T, paired = FALSE)
  #print(t.test(cluster1[,i], cluster2[,i], var.equal = T, paired = FALSE))
  #print(t.test(cluster1[,i], cluster3[,i], var.equal = T, paired = FALSE))
  #print(t.test(cluster3[,i], cluster2[,i], var.equal = T, paired = FALSE))
}
```

Гипотеза о равенстве средних значений отверглась 21 раз из 24, значит можно смело заявить, что гипотеза отверглась и средние значения кластеров не равны. Кластеры между собой отличаются - как и должно, собственно, быть.

### **5.3. Интерпретация полученных результатов**

На представленном графике красным цветом обозначен первый кластер, зеленым - второй, синим - третий.

Видно, что во втором кластере (43 наблюдения) наименьшие средние показатели у переменной `Adult_mort`(смертности взрослых), `Thinness_5_9` и `Thinness_10_19` (худоба среди детей разных возрастов), наибольшие по `Alcohol` (потребление алкоголя), `Total_exp` (траты государства на здравоохранение), `GDP` (ВВП страны) и `Schooling` (среднее количество лет образования по стране), и средние (относительно двух других кластеров) по переменной `Dipht` - иммунитет к дифтерии среди детей возрастом 1 год.

В третьем кластере (61 наблюдение) ровно наоборот - наибольшие средние показатели у переменной `Adult_mort`, `Thinness_5_9` и `Thinness_10_19`, наименьшие - по всем остальным переменным.

Средние первого кластера (79 наблюдений) находятся посередине между средними второго и третьего кластера по всем переменным, кроме `Dipht` - иммунитет к дифтерии среди детей возрастом 1 год.

Таким образом, было бы логично назвать второй кластер `Хорошо развитые страны`, потому что уровень медицины и образования, ВВП там наибольший, уровень заболеваний наименьший. Тогда третий кластер назовем `Неразвитые страны`. По тому же принципу первый кластер будет называться `Страны со средним уровнем развития`.


### **5.4. Описание кластеров с помощью графических средств**

Посмотрим на наши кластеры в пространстве двух первых главных компонент:

```{r, warning=FALSE, comment=NA}
fviz_cluster(object = kmeans3, data = data2,
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры стран в пространстве первых двух главных компонент')
```

Первое, что можно сказать по этому графику, - 2 главные компоненты объясняют около 65% дисперсии. По второй компоненте разница в кластерах неочевидна, но вот по первой четко видно разделение между кластерами (хоть они немного и пересекаются).

Теперь посмотрим на кластеры в более наглядно понятном пространстве, например, пространстве двух переменных. Для начала возьмем переменные `Adult_mort` и `GDP`:

```{r, warning=FALSE, comment=NA}
hclust_km <- hcut(data2, k = 3, hc_metric = 'euclidian', hc_method = 'complete')
fviz_cluster(object = hclust_km, data = data2, choose.vars = c('Adult_mort', 'GDP'),
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры в пространстве смертности взрослых и ВВП')
```

В пространстве этих переменных кластеры, конечно, сильно пересекаются, но можно отметить, что для красного кластера (Неразвитые страны) характерны наблюдения с наибольшей смертностью взрослых и наименьшим ВВП, а для зеленого кластера (Хорошо развитые страны), наоборот, - наименьшая смертность и наибольший ВВП. Соответственно, синий кластер (Страны со средним уровнем развития) находится посередине. 

```{r, warning=FALSE, comment=NA}
fviz_cluster(object = hclust_km, data = data2, choose.vars = c('GDP', 'Total_exp'),
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры в пространстве ВВП и затрат на здравоохранение')
```

По этому графику видно, что страны с наибольшими ВВП и затратами на здравоохранение отнесены к одному кластеру (зеленый) - Хорошо развитые страны. Страны с наименьшим ВВП и низкими затратами на здравоохранение относятся к красному кластеру - Неразвитые страны. Синий же кластер содержит страны со средним уровнем развития, которым характерны средние показатели ВВП и затрат на здравоохранение. Однако можно заметить, что средние (более крупные точки соответствующей формы) по переменной `Total_exp` достаточно близки (близость между средними кластеров `Неразвитые страны` и `Страны со средним уровнем развития` видна и по графику средних).

### **5.5. Выводы**

Проводя кластеризацию методом к-средних, мы решили взять 3 кластера, а не 2 (оптимальное количество), чтобы уменьшить различия в размерах кластеров, а также увеличить статистику between_SS / total_SS. Ни логика, ни уникальность кластеров от этого не пострадали, так что данное решение было вполне обдуманным. По графику средних кластеры выглядели уникально, средние не совпали ни по одной переменной. Более того, гипотеза о равенстве средних значений отверглась, что подтверждает грамотность проведенной кластеризации. По получившимся средним значениям переменных было решено назвать получившиеся кластеры стран так: `Хорошо развитые страны`, `Неразвитые страны`, `Страны со средним уровнем развития`. Разделение стран именно на такие группы достаточно логично, поэтому мы считаем, что метод к-средних справился со своей задачей.

## **6. Построение регрессионных моделей в кластерах**

Следующим пунктом работы является проведение регрессионного анализа в кластерах, полученных методом k-means, и сопоставление качества регрессионных моделей с теми моделями, что были построены для совокупности в целом. Есть все основания полагать (об этом говорит тот факт, что данные по странам даже на уровне гипотезы компактности должны группироваться по разным кластерам в зависимости от уровня благополучия, и это же показал результат проведения кластеризации), что регрессионные модели, построенные в кластерах, будут более адекватны и качественны.

Хотелось бы также сделать небольшой дисклеймер: разумеется, наша команда зафиксировала случайность (что является ключевым для метода k-means) с помощью соответствующей функции `set.seed()`, однако на случай различных форс-мажорных обстоятельств хотелось бы предупредить, что числа, указанные в пояснениях, могут незначительно отличаться от тех, которые может выдать интерпретатор в ходе компиляции.

Для начала необходимо сформировать новые массивы данных из исходного, нестандартизированного датасета. Для этого получим из объекта `kmeans3`, содержащего всю информацию о кластерах по результатам проведённого кластерного анализа, столбец кластеров  и конкатенируем этот столбец с основным массивом. Далее надо разделить основной массив на три подмножества в соответствии с номером кластера и убрать ненужный теперь столбец кластеров: 
```{r, warning=FALSE, comment=NA}
clusts <- kmeans3$cluster
data_clustered <- as.data.frame(cbind(data[,2:10], clusts))

data_cl_1 <- data_clustered[which(data_clustered$clusts == 1), ][1:9]
data_cl_2 <- data_clustered[which(data_clustered$clusts == 2), ][1:9]
data_cl_3 <- data_clustered[which(data_clustered$clusts == 3), ][1:9]
```

Итого на выходе получены массивы `data_cl_1`, `data_cl_2` и `data_cl_3`, содержащие данные какого-то конкретного кластера (1, 2 или 3)

### **6.1. Построение уравнений регрессии для кластеров и для общей совокупности**

Далее в каждом кластере построим уравнение линейной регрессии. Сначала регрессию будем строить на весь набор объясняющих переменных, затем, судя по уровню значимости, отберём те переменные, которые наиболее значимы статистически и объясняют вариацию результирующей переменной.

#### **Регрессия в кластере 1: страны со средним уровнем развития (развивающиеся страны)**
При построении регресси необходимо учитывать то, что ранее, при первичной обработке данных, уже были прологарифмированы переменные `GDP`, `Thinness_5_9`, `Thinness_10_19`. Поэтому при дальнейшем построении уравнения регрессии были проведены некоторые эксперименты с модификацией всех прочих переменных: в частности, поскольку наибольший вклад внесла логарифмированная переменная `GDP`, было решено построить степенную регрессионную модель. Для этого были прологарифмированы все нелогарифмированные регрессоры и, опять-таки, оставлены лишь те из них, что показали статистическую значимость.
```{r, warning=FALSE, comment=NA}
dd <- as.data.frame(cbind(log(data_cl_1$Life_exp), log(data_cl_1$Adult_mort), log(data_cl_1$Alcohol), log(data_cl_1$Total_exp), log(data_cl_1$Dipht), data_cl_1$GDP, data_cl_1$Thinness_5_9, data_cl_1$Thinness_10_19, log(data_cl_1$Schooling)))
names(dd) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_1_base <- lm(Life_exp ~ ., data_cl_1)
lm_1_1 <- lm(Life_exp ~ Adult_mort + Total_exp + GDP, data_cl_1) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1 <- lm(Life_exp ~ ., dd)
nonlm_2 <- lm(Life_exp ~ Total_exp + GDP + Schooling, dd) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen <- as.data.frame(data_cl_1)
exponen$Life_exp <- log(exponen$Life_exp)
nonlm_exp_t <- lm(Life_exp ~ ., exponen)
nonlm_exp <- lm(Life_exp ~ Adult_mort + Total_exp + GDP, exponen) # регрессия на статистически значимые независимые переменные
```

```{r, warning=FALSE, comment=NA}
BIC(lm_1_base)
BIC(lm_1_1)

BIC(nonlm_1)
BIC(nonlm_2)

BIC(nonlm_exp_t)
BIC(nonlm_exp)
```

Поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -264.3) -- это модель `nonlm_exp` вида $$y = \exp(\beta_0+\sum^k_{i=1} \beta_ix_i + \varepsilon),$$ то более подробное описание качества модели будет сделано лишь для этой модели. Это экспоненциальная модель, построенная на регрессоры `Adult_mort`, `Total_exp` и `GDP`. Конечно, эта модель объясняет лишь $45\%$ вариации результирующей переменной (для сравнения: регрессия, построенная на все объясняющие переменные, в линейном, степенном и экспоненциальном случае даёт $R^2=57\%$, $R^2=48\%$ и $R^2=56\%$ соответственно, что, разумеется, тоже довольно мало). Означает это лишь одно: истинная зависимость не приближается ни к линейной, ни к степенной. Это утверждение можно проиллюстрировать с помощью графиков для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:

```{r, warning=FALSE, comment=NA}
summary(nonlm_exp)
plot_model(nonlm_exp, type = 'slope')
```

На графиках хорошо видно, что если в случае с `Adult_mort` и `GDP` линия ещё как-то приближает хотя бы направление истинной зависимости (хотя игнорирует сложную нелинейную траекторию, а в случае `Adult_mort` истинная кривая заметно выбивается из доверительного интервала), то в случае с `Total_exp`, где налицо квадратичная функциональная зависимость (график напоминает параболу), и в этом случае линейно приблизить такую зависимость крайне проблематично (применение логарифма лишь чуть вытянуло край параболы, ситуацию спасает разве что довольно большой доверительный интервал).

Также необходимо проинтепретировать коэффициенты уравнения регрессии, указанные в выводе функции `summary()`. Поскольку мы имеем дело с экспоненциальной моделью, в которой логарифмировано значение целевой переменной, то можно сказать, что тот или иной коэффициент эластичности будет вычисляться по формуле: $$Э_j=\hat{\beta}_j \cdot \bar{x}_j.$$ Коэффициенты эластичности показывают, на сколько процентов в среднем изменится результирующая переменная -- продолжительность жизни в годах -- при $1\%$-ном увеличении той или иной независимой переменной:

* уменьшение значения смертности в возрасте 15-60 лет на 1000 человек на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen[["Adult_mort"]]) * 2.398e-04`\%$ (мизерное изменение, объясняющееся, впрочем, мизерным изменением объясняющей переменной); 
* увеличение значения процента государственных расходов, выделяемого на здравоохранение (по отношению к общей сумме расходов) на $1\%$ (здесь именно на $1\%$ необходимо увеличить величину, измеряемую в процентах – такова особенность переменной `Total_exp`) ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen[["Total_exp"]]) * 7.110e-03`\%$ -- также мизерное изменение, объясняющееся малым варьированием независимой переменной); 
* увеличение ВВП на душу населения на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen[["GDP"]]) * 3.617e-02`\%$ -- в этом случае изменение более существенное, хотя всё равно составляет доли процента (впрочем, если учесть, что такое изменение вызвано процентным изменением объясняющей переменной, можно размышлять о значительном влиянии изменения ВВП на продолжительность жизни, и это логично).

Также проанализируем остатки лучшей модели:
```{r, warning=FALSE, comment=NA}
nonl_res <- nonlm_exp$residuals
plot(seq(1, nrow(exponen), 1), nonl_res, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(nonl_res), col = 'red', lwd = 2)

hist(nonl_res, breaks = sqrt(length(nonl_res)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков', col = '#00cccc')

qqnorm(nonl_res, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(nonl_res, col = 'red', lwd = 2)
```

Как видно, остатки вполне гетероскедастичны, визуально приблизительно нормальны (могло быть и хуже), хотя выборочный квантиль едва приближает теоретический (особенно в хвостах): заметно также и осциллирование посередине. Гистограмма остатков имеет заметную островершинность.

Подводя итоги, можно вывести также и графики, показывающие как истинную зависимость, так и линии остатков. По ним видно некоторое отклонение этих линий, говорящее об их ненормальности и о необходимости применения других преобразований к исходным регрессорам (лучший результат разве что по переменной `GDP`).
```{r, warning=FALSE, comment=NA}
crPlots(nonlm_exp)
```


В частности, если говорить о качестве предсказаний по такой модели, видно, что раньше показывалось аналитически: неплохо в общем уловленное истинное распределение данных (примерно такая же картина и для переменной `GDP` и `Total_exp`), хотя некоторые особенности были чрезмерно обобщены.

Что неприятно удивило -- так это то, что в результате кластеризации не произошло отделение правого кластера точек (предполагалось, что этот кусок отделится). Значит, в многомерном признаковом пространстве этот кластер не выделяется так явно, как в двумерном:
```{r, warning=FALSE, comment=NA}
pred_nlm1 <- predict(nonlm_exp)

df_true <- data.frame(x = exponen$Adult_mort, y = exponen$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = exponen$Adult_mort, y = pred_nlm1)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'ВВП на душу населения (логарифмированный)', y = 'Продолжительность жизни, годы')
```

#### **Регрессия в кластере 2: хорошо развитые страны**
Аналогичную работу (не плодя сущности, впрочем, потому что алгоритм действий полностью совпадает с предыдущим кластером) проведём для 2-го кластера. Будем иметь в виду, что количество наблюдений в этом кластере критически мало и составляет лишь 43 экземпляра:
```{r, warning=FALSE, comment=NA}
dd2 <- as.data.frame(cbind(log(data_cl_2$Life_exp), log(data_cl_2$Adult_mort), log(data_cl_2$Alcohol), log(data_cl_2$Total_exp), log(data_cl_2$Dipht), data_cl_2$GDP, data_cl_2$Thinness_5_9, data_cl_2$Thinness_10_19, log(data_cl_2$Schooling)))
names(dd2) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_2_base <- lm(Life_exp ~ ., data_cl_2)
lm_2_1 <- lm(Life_exp ~ GDP + Schooling, data_cl_2) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1_2 <- lm(Life_exp ~ ., dd2)
nonlm_2_2 <- lm(Life_exp ~ GDP + Schooling, dd2) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen2 <- as.data.frame(data_cl_2)
exponen2$Life_exp <- log(exponen2$Life_exp)
nonlm_exp_t2 <- lm(Life_exp ~ ., exponen2)
nonlm_exp2 <- lm(Life_exp ~ GDP + Schooling, exponen2) # регрессия на статистически значимые независимые переменные
```

```{r, warning=FALSE, comment=NA}
BIC(lm_2_base)
BIC(lm_2_1)

BIC(nonlm_1_2)
BIC(nonlm_2_2)

BIC(nonlm_exp_t2)
BIC(nonlm_exp2)
```

Как и ранее, поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -133.3) -- это модель `nonlm_2_2`, то более подробное описание качества модели будет сделано лишь для этой модели. Это степенная модель, построенная на регрессоры `GDP` и `Schooling`. Эта модель объясняет $59\%$ вариации результирующей переменной, что, конечно, лучше, чем было в первом кластере. Для сравнения: регрессия, построенная на все объясняющие переменные, в линейном и нелинейном случае даёт $R^2=61\%$, что, разумеется, тоже не особо много). Означает это лишь одно: истинная зависимость, как и в прошлом случае, не аппроксимируется достаточно хорошо ни линейной, ни степенной зависимостями. Это утверждение можно проиллюстрировать, как и раньше, с помощью графиков для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:

```{r, warning=FALSE, comment=NA}
summary(nonlm_2_2)
plot_model(nonlm_2_2, type = 'slope')
```

На графиках хорошо видно, что линия очень слабо аппроксимирует истинную зависимость -- по крайней мере, направление истинной зависимости (хотя игнорирует сложную нелинейную траекторию с имеющимися у неё подъёмами и спадами: истинная зависимость выбивается из коридора возможных значений линейной регрессии).

Также необходимо проинтепретировать коэффициенты уравнения регрессии, указанные в выводе функции `summary()`. Поскольку мы имеем дело с логарифмированными значениями и зависимой переменной, и регрессоров, то можно сказать, что коэффициенты при независимых переменных $\hat{\beta}_1$ и $\hat{\beta}_2$ эквивалентны коэффициентам эластичности и показывают, на сколько процентов в среднем изменится результирующая переменная -- продолжительность жизни в годах -- при $1\%$-ном увеличении той или иной независимой переменной:

* увеличение ВВП на душу населения на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $0.05\%$ -- маленькое изменение, оно ад в 6.8 раз меньше, чем аналогичное изменение в случае регрессионной модели, проведённой в первом кластере. Значит, для стран, находящихся во втором кластере, при построении регрессии продолжительности жизни на выбранный набор переменных независимая переменная `GDP` имеет существенно меньшее влияние на *процентное среднее* изменение продолжительности жизни, чем для стран в первом кластере, и это хорошо объяснимо с экономико-социологической точки зрения: как известно, рост ВВП в хорошо развитых странах имеет не такой высокий темп прироста, как в развающихся странах, поэтому процентное изменение ВВП на душу населения оказывает меньший эффект на продолжительность жизни;
* увеличение количества лет, потраченных на образование, на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $0.14\%$. Также повторим, что здесь, наверное, уместно обратная интерпретация: в странах, где высокая продолжительности жизни, население больше времени тратит на образование, поэтому нельзя сказать наверняка, что при увеличении продолжительности образования вдруг возрастёт продолжительность жизни: вполне имеет место не причинно-следственная связь, а корреляция.

Проанализируем остатки лучшей модели:
```{r, warning=FALSE, comment=NA}
nonl_res2 <- nonlm_2_2$residuals
plot(seq(1, nrow(dd2), 1), nonl_res2, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(nonl_res2), col = 'red', lwd = 2)

hist(nonl_res2, breaks = sqrt(length(nonl_res2)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков', col = '#00cccc')

qqnorm(nonl_res2, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(nonl_res2, col = 'red', lwd = 2)
```

Как видно, остатки вполне гетероскедастичны, визуально отличаются от нормального закона распределения (хотя и в этом случае могло быть и хуже). Выборочный квантиль сильно (и тем более сильно, если сравнивать с остатками модели, построенной в первом кластере) отклоняется от теоретического, а гистограмма остатков имеет заметную асимметрию.

Подводя итоги, можно вывести также и графики, показывающие как истинную зависимость, так и линии остатков. По ним видно некоторое отклонение этих линий, говорящее об их ненормальности и о необходимости применения других преобразований к исходным регрессорам.
```{r, warning=FALSE, comment=NA}
crPlots(nonlm_2_2)
```
В частности, если говорить о качестве предсказаний по такой модели, видно, что раньше показывалось аналитически: довольно хороший процент объяснённой дисперсии (примерно такая же картина и для переменной `Schooling`), уловленное моделью истинное распределение (например, в хорошая аппроксимация особенности расположения данных в левом нижнем углу)
```{r}
pred_nlm2 <- predict(nonlm_2_2)

df_true <- data.frame(x = dd2$GDP, y = dd2$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = dd2$GDP, y = pred_nlm2)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'ВВП на душу населения (логарифмированный)', y = 'Продолжительность жизни, годы')
```


#### **Регрессия в кластере 3: неразвитые страны**

По уже отработанному алгоритму проведём регрессионный анализ для 3-го кластера. Без лишних слов и прелюдий построим несколько экспериментальных моделей:

```{r, warning=FALSE, comment=NA}
dd3 <- as.data.frame(cbind(log(data_cl_3$Life_exp), log(data_cl_3$Adult_mort), log(data_cl_3$Alcohol), log(data_cl_3$Total_exp), log(data_cl_3$Dipht), data_cl_3$GDP, data_cl_3$Thinness_5_9, data_cl_3$Thinness_10_19, log(data_cl_3$Schooling)))
names(dd3) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_3_base <- lm(Life_exp ~ ., data_cl_3)
lm_3_1 <- lm(Life_exp ~ Adult_mort + Schooling, data_cl_3) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1_3 <- lm(Life_exp ~ ., dd3)
nonlm_2_3 <- lm(Life_exp ~ Adult_mort + Schooling, dd3) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen3 <- as.data.frame(data_cl_3)
exponen3$Life_exp <- log(exponen3$Life_exp)
nonlm_exp_t3 <- lm(Life_exp ~ ., exponen3)
nonlm_exp3 <- lm(Life_exp ~ Adult_mort + Schooling, exponen3) # регрессия на статистически значимые независимые переменные
```

```{r, warning=FALSE, comment=NA}
BIC(lm_3_base)
BIC(lm_3_1)

BIC(nonlm_1_3)
BIC(nonlm_2_3)

BIC(nonlm_exp_t3)
BIC(nonlm_exp3)
```

Как и ранее, поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -140.22) -- это модель `nonlm_exp3`, то более подробное описание качества модели будет сделано лишь для этой модели. Это экспоненциальная модель, построенная на регрессоры `Adult_mort` и `Schooling`. Эта модель объясняет $43.7\%$ вариации результирующей переменной, что, конечно, провал по сравнению с предыдущими кластерами (хотя тоже не самый худший возможный показатель). Означает это лишь одно: истинная зависимость, как и в прошлом случае, не аппроксимируется в превосходной степени ни линейной, ни степенной зависимостями. Это утверждение можно проиллюстрировать, как и раньше, с помощью графиков для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:

```{r, warning=FALSE, comment=NA}
summary(nonlm_exp3)
plot_model(nonlm_exp3, type = 'slope')
```

На графиках хорошо видно, что линия очень слабо аппроксимирует истинную зависимость -- если в случае с `Schooling` хотя бы примерно верно указано направление зависимости (да и линия целиком попадает в доверительный интервал), то в случае с `Adult_mort` истинная зависимость выбивается из доверительного интервала.

Также необходимо проинтепретировать коэффициенты уравнения регрессии, указанные в выводе функции `summary()`. Поскольку мы имеем дело с экспоненциальной моделью, в которой логарифмировано значение целевой переменной, то можно сказать, что тот или иной коэффициент эластичности будет вычисляться по формуле: $$Э_j=\hat{\beta}_j \cdot \bar{x}_j.$$ Коэффициенты эластичности показывают, на сколько процентов в среднем изменится результирующая переменная -- продолжительность жизни в годах -- при $1\%$-ном увеличении той или иной независимой переменной:

* уменьшение значения смертности в возрасте 15-60 лет на 1000 человек на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen3[["Adult_mort"]]) * 5.281e-04`\%$. В этом кластере стран, как видно, изменение смертности сильнее (**в 5 раз** сильнее, если быть точнее) влияет на продолжительность жизни, чем в первом кластере, и это тоже объяснимо: грубо говоря, это показывает, что в неразвитых странах гораздо острее стоит вопрос именно выживания, и именно высокая смертность людей в возрасте 15-60 лет не позволяет (что логично) доживать людям до почтенного возраста; 
* увеличение количества лет, потраченных на образование, на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen3[["Schooling"]]) * 1.212e-02`\%$ (чуть менее сильно влияние, чем во втором кластере). Также повторим, что здесь, наверное, уместно обратная интерпретация: в странах, где высокая продолжительности жизни, население больше времени тратит на образование, поэтому нельзя сказать наверняка, что при увеличении продолжительности образования вдруг возрастёт продолжительность жизни: вполне имеет место не причинно-следственная связь, а корреляция. Обратим также внимание на экономико-социологический смысл полученного нами значения: аналогичный регрессор присутствует в кластере для развитых стран, но там он имеет большее влияние. Есть основания предполагать, что это связано с тем, что уровень образования в развитых странах коррелирует как с общим кругозором, так и с уровнем дохода, который оказывает существенное влияние на продолжительность жизни (почти во всех построенных моделях регрессии именно `GDP` был одним из самых статистически значимых переменных). В случае же неразвитых стран образование в меньшей степени обусловливает доход, который и так имеет довольно низкий уровень. Так что здесь имеет место, надо полагать, транзитивная связь "уровень образования - доход - продолжительность жизни", хотя это лишь гипотеза.

Проанализируем остатки лучшей модели:
```{r}
nonl_res3 <- nonlm_exp3$residuals
plot(seq(1, nrow(exponen3), 1), nonl_res3, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(nonl_res3), col = 'red', lwd = 2)

hist(nonl_res3, breaks = sqrt(length(nonl_res3)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков', col = '#00cccc')

qqnorm(nonl_res3, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(nonl_res3, col = 'red', lwd = 2)
```

Видно, что, хотя остатки вполне гетероскедастичны, визуально они очень отличаются от нормального закона распределения: выборочный квантиль сильно (и тем более сильно, если сравнивать с остатками модели, построенной в предыдущих кластерах) отклоняется от теоретического, а гистограмма остатков имеет существенные отклоненния от гауссианы (заметна левосторонняя асимметрия и островершинность).

Подводя итоги, можно вывести также и графики, показывающие как истинную зависимость, так и линии остатков. И они показывают уже серьёзные отклонения (особенно это касается переменной `Adult_mort`), говорящие об их ненормальности и о необходимости применения других преобразований к исходным регрессорам.
```{r, warning=FALSE, comment=NA}
crPlots(nonlm_2_3)
```

В частности, если говорить о качестве предсказаний по такой модели, видно, что, в целом, модель угадывает довольно гетероскедастичное по своему виду распределение наблюдений в пространстве результирующей и объясняющей переменных, но в целом качество оставляет желать лучшего:
```{r}
pred_nlm3 <- predict(nonlm_exp3)

df_true <- data.frame(x = exponen3$Schooling, y = dd3$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = exponen3$Schooling, y = pred_nlm3)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'Количество лет обучения (логарифмированное)', y = 'Продолжительность жизни, годы')
```

#### **Регрессия по всему набору данных**

Также по тому же принципу построим регрессию на общем наборе данных (ориентируясь на результаты первой компьютерной работы):
```{r, warning=FALSE, comment=NA}
dat <- data[, 2:10]
dd4 <- as.data.frame(cbind(log(dat$Life_exp), log(dat$Adult_mort), log(dat$Alcohol), log(dat$Total_exp), log(dat$Dipht), dat$GDP, dat$Thinness_5_9, dat$Thinness_10_19, log(dat$Schooling)))
names(dd4) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_4_base <- lm(Life_exp ~ ., dat)
lm_4_1 <- lm(Life_exp ~ Adult_mort + Schooling, data_cl_3) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1_4 <- lm(Life_exp ~ ., dd4)
nonlm_2_4 <- lm(Life_exp ~ Adult_mort + Alcohol + Dipht + GDP + Thinness_5_9 + Schooling, dd4) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen4 <- as.data.frame(dat)
exponen4$Life_exp <- log(exponen4$Life_exp)
nonlm_exp_t4 <- lm(Life_exp ~ ., exponen4)
nonlm_exp4 <- lm(Life_exp ~ Adult_mort +  GDP + Schooling, exponen4) # регрессия на статистически значимые независимые переменные
```


```{r, warning=FALSE, comment=NA}
BIC(lm_4_base)
BIC(lm_4_1)
BIC(nonlm_1_4)
BIC(nonlm_2_4)
BIC(nonlm_exp_t4)
BIC(nonlm_exp4)
```

Поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -497.98, что является лучшим значением не только среди моделей для общего датасета, но и всех остальных моделей, построенных для кластеров) – это модель `nonlm_exp4`, то более подробное описание качества модели будет сделано лишь для этой модели. Это степенная модель, построенная на регрессоры `Adult_mort`, `GDP`, `Schooling`. Эта модель объясняет $78\%$ вариации результирующей переменной, что является лучшим результатом по сравнению с моделями, построенными для кластеров. По графикам для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами, можно увидеть, что в случае совокупно взятого датасета истинные зависимости хорошо (даже с учётом имеющихся нелинейностей и довольно выбивающейся кривой для `Schooling`) аппроксимируются построенным уравнением регрессии:

```{r, warning=FALSE, comment=NA}
summary(nonlm_exp4)
plot_model(nonlm_exp4, type = 'slope')
```

Мы не будем анализировать остатки для этой модели, поскольку её технические характеристики подробно описывались на первой компьютерной работе. Мы лишь выведем коэффициенты эластичности и соответствия предсказаний истинным значениям.

Итак, анализ коэффициентов показывает нам следующее:

* уменьшение значения смертности в возрасте 15-60 лет на 1000 человек на $1\%$ ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen4[["Adult_mort"]]) * 5.213e-04`\%$: видно, что это значение находится в интервале между таковым для кластера развивающихся стран;
* увеличение ВВП на душу населения на $1\%$ ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen4[["GDP"]]) * 1.619e-02`\%$ -- и вновь видно, что это значение располагается в промежутке между значениями для развивающихся и развитых стран;
* увеличение количества лет, потраченных на образование, на $1\%$ ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen4[["Schooling"]]) * 1.651e-02`\%$. Это значение, напротив, превышает те, что были получены для отдельных кластеров: тем самым для всей совокупности стран получилось большее влияние образования на продолжительность жизни.

```{r, warning=FALSE, comment=NA}
pred_nlm4 <- predict(nonlm_exp4)

df_true <- data.frame(x = exponen4$Adult_mort, y = exponen4$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = exponen4$Adult_mort, y = pred_nlm4)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'ВВП на душу населения (логарифмированный)', y = 'Продолжительность жизни, годы')
```

Как видно -- и это справедливо для остальных регрессоров -- истинные значения хорошо предсказываются модельными. И вновь заметен кластер точек, который по каким-то причинам не был выделен в отдельный кластер -- возможно, как уже говорилось, это обусловлено особенностями исходного 8-мерного признакового пространства.

### **6.2. Сопоставление качества построенных моделей для кластеров и всей совокупности**
Остаётся подвести итог проделанной работе. Было построено множество регрессионных моделей, отбор лучших моделей производился в несколько этапов, были использованы как аналитические, так и графические средства проверки качества моделей. И, несмотря на то, что исходной гипотезой было то, что в кластерах регрессионная модель будет вести себя лучше, реальность оказалась иной: и по критерию Акаике (остался за кадром), и по Байесовскому критерию информативности, и по уровню объяснённой дисперсии лидирует -- причём с существенным отрывом -- модель, построенная на всей совокупности данных. Есть несколько гипотез, которые обосновывают, почему это так:

1. Истинные функциональные зависимости между результирующей переменной и регрессорами не были найдены верно. Действительно, графики явно показывали на достаточный пласт неучтённых зависимостей (особенно речь идёт о графиках для оцененных уравнений регрессии с каждым регрессором в отдельности), а нами были рассмотрены лишь несколько базовых преобразований, позволяющих найти нелинейные зависимости.
2. Исходное признаковое пространство таково, что гораздо более информативным является построение регрессии для всей совокупности, нежели для отдельных её частей. Об этом свидетельствует, например, то, что, несмотря на то, что в регрессионную модель по всей совокупности обобщённо вошли те же регрессоры, что присутствовали в уравнениях регрессии для отдельных кластеров, причём коэффициенты эластичности находятся в промежутках между значениями коэффициентов эластичности для отдельных кластеров, например, переменная `Schooling` в случае регрессии для всей совокупности объектов показала большее значение коэффициента эластичности: возможно, имели место некоторые неучтённые зависимости при делении на кластеры.
3. Деление на кластеры не было идеальным. Хотя мы предположили, что то распределение данных по кластерам, которые мы получили в результате применения алгоритма k-means вполне логично и оправданно, вполне может быть так, что пространство наблюдений формирует нечёткие и размытые между собой кластеры, которые не меняют радикально ни наклон, ни intercept обобщённой регрессионной модели. Также вполне возможен сценарий, что форма кластеров в n-мерном признаковом пространстве имеет какую-то нелинейную форму. Более того, если взглянуть на кластеры, то видно, что они несколько накладываются друг на друга, что тоже могло сыграть свою роль при качестве построенных в кластерах регрессионных моделей. Более подробно рассмотреть то, насколько информативным является деление на кластеры, можно в следующем пункте работы.

Тем не менее, даже анализируя коэффициенты эластичности в таких кластерах в неидеальных моделях, можно сделать интересные и подтверждающиеся здравым смыслом и экономической, социологической парадигмой выводы касательно того, какой аспект человеческого развития и в какой мере влияет на продолжительность жизни в тех или иных странах.


## **7. Линейный дискриминантный анализ**

В данном разделе речь пойдет про дискриминантный анализ, он используется для отнесения новых элементов генеральной совокупности к какому-то из классов при условии, что новый элемент будет относиться к какому-то из известных классов. Для решения задачи используются два основных метода ДА: линейный и вероятностный, по факту в нашей работе мы будем задействовать оба метода, к примеру, построение дискриминантной функции относистя к линейному подходу в то время как таблица соответствия предсказанных классов исходным относится к вероятностному подходу. 

### **7.1. Построение дискриминантных функций. Выводы о качестве модели**

Так как наши данные уже были логарифмированы, почищены на предмет нулей, там где это требовалось, и стандартизированы для применения евклидовой метрики, то можем приступать к ДА.

Для проведения дискриминантного анализа воспользуемся результатами кластерного анализа ИЗ пункта 4, для наглядности повторим k-means с тем же seed.


```{r, warning=FALSE, comment=NA}
set.seed(2)
kmeans3 <- kmeans(data2, centers = 3)
kmeans3
```
Присоединение вектора значений принадлежности к кластеру к основным данным:

```{r, warning=FALSE, comment=NA}
cl <- kmeans3$cluster
cluster_data <- cbind(data2,cl)
```


Итак,Линенйный Дискриминантный Анализ:
Разделение выборки на обучающую (2/3) и тестовую (1/3), именно в такой пропорции мы это делали на семинаре и так рекомендовано делить выборку:
```{r, warning=FALSE, comment=NA}
data.train <- as.data.frame(cluster_data[seq(1,nrow(cluster_data),1.5),])
data.unknown <- as.data.frame(cluster_data[-seq(1,nrow(cluster_data),1.5),])
```

Пострим непосредтвенно саму дискриминантную функцию, также удалим столбец cl, чтобы оне не мешал анализу:
```{r, warning=FALSE, comment=NA}
lda.fit <- lda(data.train[, -c(9)], data.train$cl)
lda.fit
```
Мы видим вероятности после опыта: вероятность быть в 1 группе - 0.4754098, во второй - 0.2377049, в 3 - 0.2868852. Тут же видим вклад наших дискриминантных функций: LD1 - 0.9387, LD2 - 0.0613 - это та доля, сколько наблдений каждая из функций помогает определить без построения иных дискриминантных функций.

### **Вывод о качестве модели (пункт 7.1)**
На первый взгляд так как вклад ld1 аж 0.9387, что крайне много, а линейных дискриминант у нас всего две, то это говорит о том, что модель крайне неплозо построена, так как она строится так, чтобы сначала вклад ld1 максимизировался (он вышел аж 0.9387), затем уже вклад ld2 (понтяно, что так как модели всего две, то на вторую дискриминанту придется оставшаяся доля наблюдений). Пока что модель 

-------------------

### **7.2. Отнесение новых объектов (3-4 наблюдения) к выделенным и описанным кластерам различными способами с использованием ДФ**

Так как по условию требуется взять 3-4 наблюдения, то прошлые выборки не годятся. Создадим новые выборки, так как у нас 183 исходных наблюдения, а нам надо отобрать всего лишь 4 для тестовой выборки, то возьмем для тренировочной выборки шаг 1.022346, так как мы берем в нее (183-4)/183 часть от общего числа, затем перевернем полученную дробь, будет 183/179, что и равно 1.022346:

```{r, warning=FALSE, comment=NA}
data.train2 <- as.data.frame(cluster_data[seq(1,nrow(cluster_data),1.022346),])
data.unknown2 <- as.data.frame(cluster_data[-seq(1,nrow(cluster_data),1.022346),])
```

Как мы видим, в data.train2 ровно 179 наблюдений, сколько мы и хотели взять, а в data.unknown2 их 4, так что отбор вышел замечательный.

```{r, warning=FALSE, comment=NA}
lda.fit2 <- lda(data.unknown2[, -c(9)], data.unknown2$cl)
lda.fit2
```
Мы видим выше, что у нас из 4 наблюдений 1 попадает в 1 класс, одно в - во второй и 2 попадают в третий класс.


### **7.2.1. Линейный метод**

Проведем саначал анализ попадания наших наблюдений с помощью линейного метода:


```{r, warning=FALSE, comment=NA}
data.unknown2
```

Прогноз для наблюдений из тестовой выборки:
"class" содержит информацию о предсказанной группе для каждого наблюдения
"posterior" -- матрица вероятности принадлежности к каждому классу для каждого наблюдения из тестовой выборки
"x" - значение дискриминантной функции

```{r, warning=FALSE, comment=NA}
lda.pred2 <- predict(lda.fit2, data.unknown2[,1:8])
names(lda.pred2)
```
```{r, warning=FALSE, comment=NA}
ldahist(data = lda.pred2$x[,1], g=cl)
```


Итак, у нас в 1 группу прогнозируются 2 попадания, во 2 ноль попаданий и в 3 группу два попадания. Данный метод работает некорректно, так как один из элементов вместо попадания во 2 группу попал в 1 группу, таким обращом в 1 группе у нас предсказывается 2 наблюдения.

### **7.2.2. Вероятностный метод**

Теперь перейдем к вероятностному подходу:

```{r, warning=FALSE, comment=NA}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred2$class, data.unknown2[,c("cl")])
```

Этот метод же показывает, что наша модель работает не идеально на 4 тестовых наблюдениях, так как ошибка определения, как и в предыдущем методе составил 1/4 или 0,33. Надежной модель считается, если погрешность меньше 0,1. Тем не мене не будем забывать, что по условию просили взять 4 наблюдения, что крайне мало для точного понимания того насколько классно работает обученная модель, надо брать куда больше значений, чем мы и займемся в одном из следующих пунктов!


### **7.3. Уточнение результатов классификации, выполненной с помощью метода к-средних, с помощью аппарата дискриминантного анализа (выявление некорректно классифицированных наблюдений)**

Вывдем данные по k-means еще раз:

```{r, warning=FALSE, comment=NA}
kmeans3
```

Теперь выведем данные по ДА:

```{r, warning=FALSE, comment=NA}
lda.fit<- lda(data.train[, -c(9)], data.train$cl)
lda.fit
```
Если мы взглянем на средние значения в k-means и в ДА, то поймем, что в целом модели очень похожи по тому, как они делят объекты на класетры, что не удивительно.

На вопрос о неверной классификации ответим с ледующем пункте.Потому как пока что была проделана работа только по выборке в 4 наблюдения, а работа по тестовой выборке (которая 1/3 от общей) проделана еще не была! Так что прошу искать частично ответ на данный пункт в следующем пунтке, пожалуйста.


### **7.4. Анализ классификационной матрицы (classification matrix). Вывод о качестве разбиения объектов на кластеры**


```{r, warning=FALSE, comment=NA}
lda.pred <- predict(lda.fit, data.unknown[,1:8])
names(lda.pred)
```

Таблица соответствия предсказанных классов исходным:

```{r, warning=FALSE, comment=NA}
lda.pred$class
table(lda.pred$class, data.unknown[,c("cl")])
summary(lda.pred$class)
```
Найдем еще ошибку:

```{r, warning=FALSE, comment=NA}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.unknown[,c("cl")])
```
Как мы видим, модель ДА прекрасно справилась с заданием, ведь ошибка составила всего 0.0495 по сравнению с результами с результатами Кластерного анализа. Вот только теперь мы можем точно сказать (в предыдущем пункте было рано сравнивать ДА с КА, так как еще не была построена матрица для выборки 1/3), что отличий почти нет, так как 0,0495 более чем в 2 раза менее 0,1 что считается максимальной допустимой погрешностью модели.

### **7.5. Построение графика принадлежности тестовой и тренировочной выборок к кластерам по результатам проведенного анализа**

Построим сначала гистограммы, чтобы показать, как разбиваются наблюдения на классы:

### **7.5.1. Тренировочная выборка**

Гистограмма значений первой дискриминантной функции:
```{r, warning=FALSE, comment=NA}
ldahist(data = lda.pred$x[,1], g=cl)
```

Видим, что у разбиений гистограммы отличаются по показателям (мода, смещение и т.д.), это говорит о том, что все хорошо, что класетры отличны друг от друга.

Построим гистограмму второй дискриминантной функции:

```{r, warning=FALSE, comment=NA}
ldahist(data = lda.pred$x[,2], g=cl)
```

Тут тоже все хорошо.

Сам график тренировочной выборки:
```{r, warning=FALSE, comment=NA}
plot(lda.fit)
```

Просматривается четкое деление на классы в данной выборке.

### **7.5.2. Тестовая выборка**

Проведем операции, необходимые для построения графиков: 

```{r}
lda.fit3<- lda(data.unknown[, -c(9)], data.unknown$cl)
lda.fit3
```
Значение вклада первой дискриминанты за 0.8, что является неплохим показателем.

```{r}
lda.pred3 <- predict(lda.fit3, data.unknown[,1:8])
names(lda.pred3)
```
По первой переменной:

```{r}
ldahist(data = lda.pred3$x[,1], g=cl)
```

Наблюдаем кардинально отличающиеся друг от друга разбиения, что говорит о хорошем различии между классами, а это в свою очередь говорит о качестве нашей модели lda.pred3

По второй:

```{r}
ldahist(data = lda.pred3$x[,2], g=cl)
```

По второй дискриминанте ситуация аналогична. Разбиения отличны друг от друга.

График для тестовой выборки:

```{r}
plot(lda.fit3)
```

Тут вообще прослеживается крайне четкое деление на классы, вопросов быть не может о качестве модели.

Напоследок добавим диаграмму рассеяния значений дискриминантных функций:
```{r}
plot(lda.pred$x[,1],lda.pred$x[,2]) # make a scatterplot
text(lda.pred$x[,1],lda.pred$x[,2],cl,cex=0.7,pos=4,col="blue") # add labels
```

Тут же мы видим scatterplot в осях lda.pred, наблюдаем, что в целом четкую особенность расположения точек какую-либо выделить сложно, они скорее на данном графике расположен беспорядочно.

### **7.5.3. Общий вывод по ДА**

Хоть в задании и не требовалось сделать вывод по всему комплексу заданий, связанных с ДА, я все-таки решил сделать такую главу, чтобы подвести черту под проделанной работой.

Итак, дискриминантная функция из раздела 6.1 вышла у нас вполне неплохая, что подтвердилось ка высоким значением вклада ld1, так и низкой ошибкой в матрице, так и распределениями с гистограммами по переменным. Что касается характеристики 4 объектов, то имеющаяся у нас модель по нашей выборке справилась также вполне сносно, учитывая, что наблюдений в unknown выборке бралось всего 4 (что крайне мало при общем наблюдении в 183, так как чем больше элементов в unknown, тем менее выборка подвержена ошибке классификации), так что тут тоже можно было бы поставить плюс. Что же касается сравнения результатов ДА с результатами кластерного анализа, то и тут я бы сказал, что мы справились уверенно, так как оба статистических подхода к выделени в выборке групп дали почти одинаковые занчения средних показателей по переменным в каждом из классов(в ДА) или кластеров (в КА), как таковых значимых отличий в результатах двух подходов не выявлено. Остальные же разделы работы в рамках пункта ДА (пункты 6.4,6.5) комментировать нет смысла, так как там была информация, которая просто подтверждала и описывала остальные пункты в рамках ДА.

## **8. Итоги**

На этом -- завершающем -- этапе выполнения компьютерной работы была проведена большая работа с признаковым пространством исходного массива данных: были применены как методы извлечения наиболее информативных переменных с помощью PCA, так и разделение объектов на отдельные кластеры по их признаковому описанию. Были уточнены результаты построения регрессионных моделей в отдельных кластерах и проведён линейный дискриминантый анализ, выясняющий, действительно ли деление на кластеры было качественным.