---
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: paged
    keep_tex: yes
    latex_engine: xelatex
  word_document: default
lang: ru-russian
header-includes:
- \newcommand{\bcenter}{\begin{center}}
- \newcommand{\ecenter}{\end{center}}
- \usepackage[russian]{babel}
fontenc: T2A
babel-lang: russian
---

<center>
# **Компьютерная работа №2 <br/> Многомерные статистические методы <br/> 2020-21 гг. — 2 курс**
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

## **1. Импорт данных**
На первом этапе необходимо импортировать все необходимые библиотеки и данные, с которыми будет впоследствии вестись работа. 
``` {r, include = FALSE}
library('rio')
library('DescTools')
library('ggpubr')
library('ggplot2')
library('nortest')
library('fBasics')
library('moments')
library('car')
library('dplyr')
library('readxl')
library('tibble')
library('EnvStats')
library('outliers')
library('Hmisc')
library('corrplot')
library('ppcor')
library('DiscriMiner')
library('pracma')
library('FactoMineR')
library('factoextra')
library('psych')
library('htmlTable')
library('REdaS')
library('grid')
library('plotly')
library('sjPlot')
```

Импортируемая таблица данных содержит 2938 наблюдений по 22 переменным. Эти наблюдения характеризуют показатели различных стран за промежуток с 2000 по 2015 год. В качестве рассматриваемого года был выбран 2014, поскольку по ряду объясняющих переменных отсутствуют данные за 2015 год.

```{r, comment = NA}
raw_data <- read.csv('life_expectancy_data.csv', header = TRUE, sep = ',')
data <- raw_data[which(raw_data$Year == 2014),]
rownames(data) <- 1:nrow(data) # Присвоим индексы наблюдениям в соответствии с их естественной очередностью, а не тем порядком, что был в исходной таблице.
data$Adult.Mortality <- as.numeric(data$Adult.Mortality) # приведём условно непрерывные данные, содержащие целые числа, к типу Numeric
data$infant.deaths <- as.numeric(data$infant.deaths)
data$Hepatitis.B  <- as.numeric(data$Hepatitis.B )
data$Measles  <- as.numeric(data$Measles)
data$under.five.deaths <- as.numeric(data$under.five.deaths)
data$Polio <- as.numeric(data$Polio)
data$Diphtheria <- as.numeric(data$Diphtheria)
data <- data[, !(names(data) == "Year")] # Теперь, когда рассматривается только 2014 год, эта переменная более не информативна.
str(data)
```

По итогам предварительного анализа имеющихся переменных, проведённого в первой компьютерной работе, было решено взять 9 переменных — 8 объясняющих и 1 зависимую, — которые нам потребуются для дальнейшего анализа. Также была оставлена одна дамми-переменная, которая позволит ориентироваться в данных:

```{r, warning=FALSE, comment=NA}
data <- data[, c("Country", "Life.expectancy", "Adult.Mortality",
                    "Alcohol", "Total.expenditure", "Diphtheria",
                    "GDP", "thinness.5.9.years",
                    "thinness..1.19.years", "Schooling")]

# Также упростим названия переменных
names(data) <- c("Country", "Life_exp", "Adult_mort",
                    "Alcohol", "Total_exp", "Dipht",
                    "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")
```

Наконец, был получен итоговый набор данных, показывающий данные по различным странам по 9 информативным показателям за 2014 год. Обработка данных, тем не менее, на этом ещё не окончена: необходимо провести обработку пропусков, которая будет проведена по той же схеме, что и при выполнении первой компьютерной работы (без сопутствующих пояснений):

```{r, warning=FALSE, comment=NA}
# Пропуски по `Alcohol`
data$Alcohol[151] <- 0.01 

# Пропуски по `Total_exp`
data$Total_exp[45] <- 6.1 # КНДР 
data$Total_exp[149] <- 4.3 # Сомали

# Пропуски по `GDP`
gdp_da <- read.csv("gdp_data.csv", sep = ",", header = FALSE)
gdp <- gdp_da[,1:ncol(gdp_da)]
data$GDP <- gdp$V6

# Пропуски по `Thinness`
data$Thinness_5_9[151] <- 7.3
data$Thinness_5_9[154] <- 7.6

data$Thinness_10_19[151] <- 7.8
data$Thinness_10_19[154] <- 7.9

# Пропуски по `Schooling`
data$Schooling[28] <- 5.4
data$Schooling[44] <- 12.5
data$Schooling[45] <- 7.8
data$Schooling[46] <- 6.7
data$Schooling[131] <- 12.5
data$Schooling[132] <- 10.0
data$Schooling[149] <- 4.7
data$Schooling[173] <- 11.8
data$Schooling[174] <- 5.8
data$Schooling[175] <- 12.9
```

Гистограммы распределения и ящики с усами для обновлённых переменных, которые теперь не содержат пропусков и имеют более чёткий и структурированный вид, выглядят следующим образом:

```{r, warning=FALSE, comment=NA}
# boxplot(scale(data[,3:ncol(data)]))

vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)

plot1 <- ggplot(data, aes(x=Life_exp)) + geom_histogram(color="black", fill='#00cccc')
plot2 <- ggplot(data, aes(x=Adult_mort)) + geom_histogram(color="black", fill='#00cccc')
plot3 <- ggplot(data, aes(x=Alcohol)) + geom_histogram(color="black", fill='#00cccc')
plot4 <- ggplot(data, aes(x=Total_exp)) + geom_histogram(color='black', fill='#00cccc')
plot5 <- ggplot(data, aes(x=Dipht)) + geom_histogram(color='black', fill='#00cccc')

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 3)))
print(plot1, vp = vplayout(1, 1))
print(plot2, vp = vplayout(1, 2))
print(plot3, vp = vplayout(1, 3))
print(plot4, vp = vplayout(2, 1))
print(plot5, vp = vplayout(2, 2))

plot13 <- ggplot(data, aes(x=GDP)) + geom_histogram(color='black', fill='#00cccc')
plot14 <- ggplot(data, aes(x=Thinness_5_9)) + geom_histogram(color='black', fill='#00cccc')
plot15 <- ggplot(data, aes(x=Thinness_10_19)) + geom_histogram(color='black', fill='#00cccc')
plot16 <- ggplot(data, aes(x=Schooling)) + geom_histogram(color='black', fill='#00cccc')


grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
print(plot13, vp = vplayout(1, 1))
print(plot14, vp = vplayout(1, 2))
print(plot15, vp = vplayout(2, 1))
print(plot16, vp = vplayout(2, 2))
```

Заметно, что гистограммы по большей части напоминают гистограммы данных, гипотетически подчиняющихся нормальному и логнормальному закону распределения (хотя и заметна асимметрия).

Поскольку многие методы кластеризации (в частности, метод k-means и метод ближнего соседа) довольно чувствительны к выбросам, проанализируем наличие аномальных данных по этим переменным:

```{r, warning=FALSE, comment=NA}
IQR3 <- function(y) {
  out_of_3IQR <- boxplot.stats(y, coef = 3)$out
  return(out_of_3IQR)
}
```

```{r, warning=FALSE, comment=NA}
for (i in names(data)[2:9]) {
  print(IQR3(as.matrix(data[i])))
  print(i)
}
```

Там, где это уместно, логарифмируем данные, стабилизировав таким образом дисперсию и уменьшив выбросы. В частности, после логарифмирования переменных `Thinness_5_9`,  `Thinness_10_19` и `GDP` выбросы по этим переменным пропадают:

```{r, warning=FALSE, comment=NA}
data$GDP <- log(data$GDP)
data$Thinness_5_9 <- log(data$Thinness_5_9)
data$Thinness_10_19 <- log(data$Thinness_10_19)
```

Как видно, остаётся лишь переменная `Dipht`, которая не стабилизируется логарифмированием (наоборот, логарифмирование делает разброс и количество выбросов ещё больше). Её было решено оставить в том виде, в каком она имеется: есть основания полагать, что это поможет более информативно выделить кластеры.

## **6. Построение регрессионных моделей в кластерах**

Следующим пунктом работы является проведение регрессионного анализа в кластерах, полученных методом k-means, и сопоставление качества регрессионных моделей с теми моделями, что были построены для совокупности в целом. Есть все основания полагать (об этом говорит тот факт, что данные по странам даже на уровне гипотезы компактности должны группироваться по разным кластерам в зависимости от уровня благополучия, и это же показал результат проведения кластеризации), что регрессионные модели, построенные в кластерах, будут более адекватны и качественны.

Хотелось бы также сделать небольшой дисклеймер: разумеется, наша команда зафиксировала случайность (что является ключевым для метода k-means) с помощью соответствующей функции `set.seed()`, однако на случай различных форс-мажорных обстоятельств хотелось бы предупредить, что числа, указанные в пояснениях, могут незначительно отличаться от тех, которые может выдать интерпретатор в ходе компиляции.

Для начала необходимо сформировать новые массивы данных из исходного, нестандартизированного датасета. Для этого получим из объекта `kmeans3`, содержащего всю информацию о кластерах по результатам проведённого кластерного анализа, столбец кластеров  и конкатенируем этот столбец с основным массивом. Далее надо разделить основной массив на три подмножества в соответствии с номером кластера и убрать ненужный теперь столбец кластеров: 
```{r, warning=FALSE, comment=NA}
clusts <- kmeans3$cluster
data_clustered <- as.data.frame(cbind(data[,2:10], clusts))

data_cl_1 <- data_clustered[which(data_clustered$clusts == 1), ][1:9]
data_cl_2 <- data_clustered[which(data_clustered$clusts == 2), ][1:9]
data_cl_3 <- data_clustered[which(data_clustered$clusts == 3), ][1:9]
```

Итого на выходе получены массивы `data_cl_1`, `data_cl_2` и `data_cl_3`, содержащие данные какого-то конкретного кластера (1, 2 или 3)

### **6.1. Построение уравнений регрессии для кластеров и для общей совокупности**

Далее в каждом кластере построим уравнение линейной регрессии. Сначала регрессию будем строить на весь набор объясняющих переменных, затем, судя по уровню значимости, отберём те переменные, которые наиболее значимы статистически и объясняют вариацию результирующей переменной.

#### **Регрессия в кластере 1: страны со средним уровнем развития (развивающиеся страны)**
При построении регресси необходимо учитывать то, что ранее, при первичной обработке данных, уже были прологарифмированы переменные `GDP`, `Thinness_5_9`, `Thinness_10_19`. Поэтому при дальнейшем построении уравнения регрессии были проведены некоторые эксперименты с модификацией всех прочих переменных: в частности, поскольку наибольший вклад внесла логарифмированная переменная `GDP`, было решено построить степенную регрессионную модель. Для этого были прологарифмированы все нелогарифмированные регрессоры и, опять-таки, оставлены лишь те из них, что показали статистическую значимость.
```{r, warning=FALSE, comment=NA}
dd <- as.data.frame(cbind(log(data_cl_1$Life_exp), log(data_cl_1$Adult_mort), log(data_cl_1$Alcohol), log(data_cl_1$Total_exp), log(data_cl_1$Dipht), data_cl_1$GDP, data_cl_1$Thinness_5_9, data_cl_1$Thinness_10_19, log(data_cl_1$Schooling)))
names(dd) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_1_base <- lm(Life_exp ~ ., data_cl_1)
lm_1_1 <- lm(Life_exp ~ Adult_mort + Total_exp + GDP, data_cl_1) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1 <- lm(Life_exp ~ ., dd)
nonlm_2 <- lm(Life_exp ~ Total_exp + GDP + Schooling, dd) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen <- as.data.frame(data_cl_1)
exponen$Life_exp <- log(exponen$Life_exp)
nonlm_exp_t <- lm(Life_exp ~ ., exponen)
nonlm_exp <- lm(Life_exp ~ Adult_mort + Total_exp + GDP, exponen) # регрессия на статистически значимые независимые переменные
```

```{r, warning=FALSE, comment=NA}
BIC(lm_1_base)
BIC(lm_1_1)

BIC(nonlm_1)
BIC(nonlm_2)

BIC(nonlm_exp_t)
BIC(nonlm_exp)
```

Поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -264.3) -- это модель `nonlm_exp` вида $$y = \exp(\beta_0+\sum^k_{i=1} \beta_ix_i + \varepsilon),$$ то более подробное описание качества модели будет сделано лишь для этой модели. Это экспоненциальная модель, построенная на регрессоры `Adult_mort`, `Total_exp` и `GDP`. Конечно, эта модель объясняет лишь $45\%$ вариации результирующей переменной (для сравнения: регрессия, построенная на все объясняющие переменные, в линейном, степенном и экспоненциальном случае даёт $R^2=57\%$, $R^2=48\%$ и $R^2=56\%$ соответственно, что, разумеется, тоже довольно мало). Означает это лишь одно: истинная зависимость не приближается ни к линейной, ни к степенной. Это утверждение можно проиллюстрировать с помощью графиков для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:

```{r, warning=FALSE, comment=NA}
summary(nonlm_exp)
plot_model(nonlm_exp, type = 'slope')
```

На графиках хорошо видно, что если в случае с `Adult_mort` и `GDP` линия ещё как-то приближает хотя бы направление истинной зависимости (хотя игнорирует сложную нелинейную траекторию, а в случае `Adult_mort` истинная кривая заметно выбивается из доверительного интервала), то в случае с `Total_exp`, где налицо квадратичная функциональная зависимость (график напоминает параболу), и в этом случае линейно приблизить такую зависимость крайне проблематично (применение логарифма лишь чуть вытянуло край параболы, ситуацию спасает разве что довольно большой доверительный интервал).

Также необходимо проинтепретировать коэффициенты уравнения регрессии, указанные в выводе функции `summary()`. Поскольку мы имеем дело с экспоненциальной моделью, в которой логарифмировано значение целевой переменной, то можно сказать, что тот или иной коэффициент эластичности будет вычисляться по формуле: $$Э_j=\hat{\beta}_j \cdot \bar{x}_j.$$ Коэффициенты эластичности показывают, на сколько процентов в среднем изменится результирующая переменная -- продолжительность жизни в годах -- при $1\%$-ном увеличении той или иной независимой переменной:

* уменьшение значения смертности в возрасте 15-60 лет на 1000 человек на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen[["Adult_mort"]]) * 2.398e-04`\%$ (мизерное изменение, объясняющееся, впрочем, мизерным изменением объясняющей переменной); 
* увеличение значения процента государственных расходов, выделяемого на здравоохранение (по отношению к общей сумме расходов) на $1\%$ (здесь именно на $1\%$ необходимо увеличить величину, измеряемую в процентах – такова особенность переменной `Total_exp`) ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen[["Total_exp"]]) * 7.110e-03`\%$ -- также мизерное изменение, объясняющееся малым варьированием независимой переменной); 
* увеличение ВВП на душу населения на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen[["GDP"]]) * 3.617e-02`\%$ -- в этом случае изменение более существенное, хотя всё равно составляет доли процента (впрочем, если учесть, что такое изменение вызвано процентным изменением объясняющей переменной, можно размышлять о значительном влиянии изменения ВВП на продолжительность жизни, и это логично).

Также проанализируем остатки лучшей модели:
```{r, warning=FALSE, comment=NA}
nonl_res <- nonlm_exp$residuals
plot(seq(1, nrow(exponen), 1), nonl_res, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(nonl_res), col = 'red', lwd = 2)

hist(nonl_res, breaks = sqrt(length(nonl_res)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков', col = '#00cccc')

qqnorm(nonl_res, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(nonl_res, col = 'red', lwd = 2)
```

Как видно, остатки вполне гетероскедастичны, визуально приблизительно нормальны (могло быть и хуже), хотя выборочный квантиль едва приближает теоретический (особенно в хвостах): заметно также и осциллирование посередине. Гистограмма остатков имеет заметную островершинность.

Подводя итоги, можно вывести также и графики, показывающие как истинную зависимость, так и линии остатков. По ним видно некоторое отклонение этих линий, говорящее об их ненормальности и о необходимости применения других преобразований к исходным регрессорам (лучший результат разве что по переменной `GDP`).
```{r, warning=FALSE, comment=NA}
crPlots(nonlm_exp)
```


В частности, если говорить о качестве предсказаний по такой модели, видно, что раньше показывалось аналитически: неплохо в общем уловленное истинное распределение данных (примерно такая же картина и для переменной `GDP` и `Total_exp`), хотя некоторые особенности были чрезмерно обобщены.

Что неприятно удивило -- так это то, что в результате кластеризации не произошло отделение правого кластера точек (предполагалось, что этот кусок отделится). Значит, в многомерном признаковом пространстве этот кластер не выделяется так явно, как в двумерном:
```{r, warning=FALSE, comment=NA}
pred_nlm1 <- predict(nonlm_exp)

df_true <- data.frame(x = exponen$Adult_mort, y = exponen$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = exponen$Adult_mort, y = pred_nlm1)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'ВВП на душу населения (логарифмированный)', y = 'Продолжительность жизни, годы')
```

#### **Регрессия в кластере 2: хорошо развитые страны**
Аналогичную работу (не плодя сущности, впрочем, потому что алгоритм действий полностью совпадает с предыдущим кластером) проведём для 2-го кластера. Будем иметь в виду, что количество наблюдений в этом кластере критически мало и составляет лишь 43 экземпляра:
```{r, warning=FALSE, comment=NA}
dd2 <- as.data.frame(cbind(log(data_cl_2$Life_exp), log(data_cl_2$Adult_mort), log(data_cl_2$Alcohol), log(data_cl_2$Total_exp), log(data_cl_2$Dipht), data_cl_2$GDP, data_cl_2$Thinness_5_9, data_cl_2$Thinness_10_19, log(data_cl_2$Schooling)))
names(dd2) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_2_base <- lm(Life_exp ~ ., data_cl_2)
lm_2_1 <- lm(Life_exp ~ GDP + Schooling, data_cl_2) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1_2 <- lm(Life_exp ~ ., dd2)
nonlm_2_2 <- lm(Life_exp ~ GDP + Schooling, dd2) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen2 <- as.data.frame(data_cl_2)
exponen2$Life_exp <- log(exponen2$Life_exp)
nonlm_exp_t2 <- lm(Life_exp ~ ., exponen2)
nonlm_exp2 <- lm(Life_exp ~ GDP + Schooling, exponen2) # регрессия на статистически значимые независимые переменные
```

```{r, warning=FALSE, comment=NA}
BIC(lm_2_base)
BIC(lm_2_1)

BIC(nonlm_1_2)
BIC(nonlm_2_2)

BIC(nonlm_exp_t2)
BIC(nonlm_exp2)
```

Как и ранее, поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -133.3) -- это модель `nonlm_2_2`, то более подробное описание качества модели будет сделано лишь для этой модели. Это степенная модель, построенная на регрессоры `GDP` и `Schooling`. Эта модель объясняет $59\%$ вариации результирующей переменной, что, конечно, лучше, чем было в первом кластере. Для сравнения: регрессия, построенная на все объясняющие переменные, в линейном и нелинейном случае даёт $R^2=61\%$, что, разумеется, тоже не особо много). Означает это лишь одно: истинная зависимость, как и в прошлом случае, не аппроксимируется достаточно хорошо ни линейной, ни степенной зависимостями. Это утверждение можно проиллюстрировать, как и раньше, с помощью графиков для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:

```{r, warning=FALSE, comment=NA}
summary(nonlm_2_2)
plot_model(nonlm_2_2, type = 'slope')
```

На графиках хорошо видно, что линия очень слабо аппроксимирует истинную зависимость -- по крайней мере, направление истинной зависимости (хотя игнорирует сложную нелинейную траекторию с имеющимися у неё подъёмами и спадами: истинная зависимость выбивается из коридора возможных значений линейной регрессии).

Также необходимо проинтепретировать коэффициенты уравнения регрессии, указанные в выводе функции `summary()`. Поскольку мы имеем дело с логарифмированными значениями и зависимой переменной, и регрессоров, то можно сказать, что коэффициенты при независимых переменных $\hat{\beta}_1$ и $\hat{\beta}_2$ эквивалентны коэффициентам эластичности и показывают, на сколько процентов в среднем изменится результирующая переменная -- продолжительность жизни в годах -- при $1\%$-ном увеличении той или иной независимой переменной:

* увеличение ВВП на душу населения на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $0.05\%$ -- маленькое изменение, оно ад в 6.8 раз меньше, чем аналогичное изменение в случае регрессионной модели, проведённой в первом кластере. Значит, для стран, находящихся во втором кластере, при построении регрессии продолжительности жизни на выбранный набор переменных независимая переменная `GDP` имеет существенно меньшее влияние на *процентное среднее* изменение продолжительности жизни, чем для стран в первом кластере, и это хорошо объяснимо с экономико-социологической точки зрения: как известно, рост ВВП в хорошо развитых странах имеет не такой высокий темп прироста, как в развающихся странах, поэтому процентное изменение ВВП на душу населения оказывает меньший эффект на продолжительность жизни;
* увеличение количества лет, потраченных на образование, на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $0.14\%$. Также повторим, что здесь, наверное, уместно обратная интерпретация: в странах, где высокая продолжительности жизни, население больше времени тратит на образование, поэтому нельзя сказать наверняка, что при увеличении продолжительности образования вдруг возрастёт продолжительность жизни: вполне имеет место не причинно-следственная связь, а корреляция.

Проанализируем остатки лучшей модели:
```{r, warning=FALSE, comment=NA}
nonl_res2 <- nonlm_2_2$residuals
plot(seq(1, nrow(dd2), 1), nonl_res2, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(nonl_res2), col = 'red', lwd = 2)

hist(nonl_res2, breaks = sqrt(length(nonl_res2)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков', col = '#00cccc')

qqnorm(nonl_res2, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(nonl_res2, col = 'red', lwd = 2)
```

Как видно, остатки вполне гетероскедастичны, визуально отличаются от нормального закона распределения (хотя и в этом случае могло быть и хуже). Выборочный квантиль сильно (и тем более сильно, если сравнивать с остатками модели, построенной в первом кластере) отклоняется от теоретического, а гистограмма остатков имеет заметную асимметрию.

Подводя итоги, можно вывести также и графики, показывающие как истинную зависимость, так и линии остатков. По ним видно некоторое отклонение этих линий, говорящее об их ненормальности и о необходимости применения других преобразований к исходным регрессорам.
```{r, warning=FALSE, comment=NA}
crPlots(nonlm_2_2)
```
В частности, если говорить о качестве предсказаний по такой модели, видно, что раньше показывалось аналитически: довольно хороший процент объяснённой дисперсии (примерно такая же картина и для переменной `Schooling`), уловленное моделью истинное распределение (например, в хорошая аппроксимация особенности расположения данных в левом нижнем углу)
```{r}
pred_nlm2 <- predict(nonlm_2_2)

df_true <- data.frame(x = dd2$GDP, y = dd2$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = dd2$GDP, y = pred_nlm2)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'ВВП на душу населения (логарифмированный)', y = 'Продолжительность жизни, годы')
```


#### **Регрессия в кластере 3: неразвитые страны**

По уже отработанному алгоритму проведём регрессионный анализ для 3-го кластера. Без лишних слов и прелюдий построим несколько экспериментальных моделей:

```{r, warning=FALSE, comment=NA}
dd3 <- as.data.frame(cbind(log(data_cl_3$Life_exp), log(data_cl_3$Adult_mort), log(data_cl_3$Alcohol), log(data_cl_3$Total_exp), log(data_cl_3$Dipht), data_cl_3$GDP, data_cl_3$Thinness_5_9, data_cl_3$Thinness_10_19, log(data_cl_3$Schooling)))
names(dd3) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_3_base <- lm(Life_exp ~ ., data_cl_3)
lm_3_1 <- lm(Life_exp ~ Adult_mort + Schooling, data_cl_3) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1_3 <- lm(Life_exp ~ ., dd3)
nonlm_2_3 <- lm(Life_exp ~ Adult_mort + Schooling, dd3) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen3 <- as.data.frame(data_cl_3)
exponen3$Life_exp <- log(exponen3$Life_exp)
nonlm_exp_t3 <- lm(Life_exp ~ ., exponen3)
nonlm_exp3 <- lm(Life_exp ~ Adult_mort + Schooling, exponen3) # регрессия на статистически значимые независимые переменные
```

```{r, warning=FALSE, comment=NA}
BIC(lm_3_base)
BIC(lm_3_1)

BIC(nonlm_1_3)
BIC(nonlm_2_3)

BIC(nonlm_exp_t3)
BIC(nonlm_exp3)
```

Как и ранее, поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -140.22) -- это модель `nonlm_exp3`, то более подробное описание качества модели будет сделано лишь для этой модели. Это экспоненциальная модель, построенная на регрессоры `Adult_mort` и `Schooling`. Эта модель объясняет $43.7\%$ вариации результирующей переменной, что, конечно, провал по сравнению с предыдущими кластерами (хотя тоже не самый худший возможный показатель). Означает это лишь одно: истинная зависимость, как и в прошлом случае, не аппроксимируется в превосходной степени ни линейной, ни степенной зависимостями. Это утверждение можно проиллюстрировать, как и раньше, с помощью графиков для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:

```{r, warning=FALSE, comment=NA}
summary(nonlm_exp3)
plot_model(nonlm_exp3, type = 'slope')
```

На графиках хорошо видно, что линия очень слабо аппроксимирует истинную зависимость -- если в случае с `Schooling` хотя бы примерно верно указано направление зависимости (да и линия целиком попадает в доверительный интервал), то в случае с `Adult_mort` истинная зависимость выбивается из доверительного интервала.

Также необходимо проинтепретировать коэффициенты уравнения регрессии, указанные в выводе функции `summary()`. Поскольку мы имеем дело с экспоненциальной моделью, в которой логарифмировано значение целевой переменной, то можно сказать, что тот или иной коэффициент эластичности будет вычисляться по формуле: $$Э_j=\hat{\beta}_j \cdot \bar{x}_j.$$ Коэффициенты эластичности показывают, на сколько процентов в среднем изменится результирующая переменная -- продолжительность жизни в годах -- при $1\%$-ном увеличении той или иной независимой переменной:

* уменьшение значения смертности в возрасте 15-60 лет на 1000 человек на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen3[["Adult_mort"]]) * 5.281e-04`\%$. В этом кластере стран, как видно, изменение смертности сильнее (**в 5 раз** сильнее, если быть точнее) влияет на продолжительность жизни, чем в первом кластере, и это тоже объяснимо: грубо говоря, это показывает, что в неразвитых странах гораздо острее стоит вопрос именно выживания, и именно высокая смертность людей в возрасте 15-60 лет не позволяет (что логично) доживать людям до почтенного возраста; 
* увеличение количества лет, потраченных на образование, на $1\%$ ведёт к увеличению продолжительности жизни **в среднем** на $`r mean(exponen3[["Schooling"]]) * 1.212e-02`\%$ (чуть менее сильно влияние, чем во втором кластере). Также повторим, что здесь, наверное, уместно обратная интерпретация: в странах, где высокая продолжительности жизни, население больше времени тратит на образование, поэтому нельзя сказать наверняка, что при увеличении продолжительности образования вдруг возрастёт продолжительность жизни: вполне имеет место не причинно-следственная связь, а корреляция. Обратим также внимание на экономико-социологический смысл полученного нами значения: аналогичный регрессор присутствует в кластере для развитых стран, но там он имеет большее влияние. Есть основания предполагать, что это связано с тем, что уровень образования в развитых странах коррелирует как с общим кругозором, так и с уровнем дохода, который оказывает существенное влияние на продолжительность жизни (почти во всех построенных моделях регрессии именно `GDP` был одним из самых статистически значимых переменных). В случае же неразвитых стран образование в меньшей степени обусловливает доход, который и так имеет довольно низкий уровень. Так что здесь имеет место, надо полагать, транзитивная связь "уровень образования - доход - продолжительность жизни", хотя это лишь гипотеза.

Проанализируем остатки лучшей модели:
```{r}
nonl_res3 <- nonlm_exp3$residuals
plot(seq(1, nrow(exponen3), 1), nonl_res3, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(nonl_res3), col = 'red', lwd = 2)

hist(nonl_res3, breaks = sqrt(length(nonl_res3)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков', col = '#00cccc')

qqnorm(nonl_res3, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(nonl_res3, col = 'red', lwd = 2)
```

Видно, что, хотя остатки вполне гетероскедастичны, визуально они очень отличаются от нормального закона распределения: выборочный квантиль сильно (и тем более сильно, если сравнивать с остатками модели, построенной в предыдущих кластерах) отклоняется от теоретического, а гистограмма остатков имеет существенные отклоненния от гауссианы (заметна левосторонняя асимметрия и островершинность).

Подводя итоги, можно вывести также и графики, показывающие как истинную зависимость, так и линии остатков. И они показывают уже серьёзные отклонения (особенно это касается переменной `Adult_mort`), говорящие об их ненормальности и о необходимости применения других преобразований к исходным регрессорам.
```{r, warning=FALSE, comment=NA}
crPlots(nonlm_2_3)
```

В частности, если говорить о качестве предсказаний по такой модели, видно, что, в целом, модель угадывает довольно гетероскедастичное по своему виду распределение наблюдений в пространстве результирующей и объясняющей переменных, но в целом качество оставляет желать лучшего:
```{r}
pred_nlm3 <- predict(nonlm_exp3)

df_true <- data.frame(x = exponen3$Schooling, y = dd3$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = exponen3$Schooling, y = pred_nlm3)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'Количество лет обучения (логарифмированное)', y = 'Продолжительность жизни, годы')
```

#### **Регрессия по всему набору данных**

Также по тому же принципу построим регрессию на общем наборе данных (ориентируясь на результаты первой компьютерной работы):
```{r, warning=FALSE, comment=NA}
dat <- data[, 2:10]
dd4 <- as.data.frame(cbind(log(dat$Life_exp), log(dat$Adult_mort), log(dat$Alcohol), log(dat$Total_exp), log(dat$Dipht), dat$GDP, dat$Thinness_5_9, dat$Thinness_10_19, log(dat$Schooling)))
names(dd4) <- c("Life_exp", "Adult_mort", "Alcohol", "Total_exp", "Dipht", "GDP", "Thinness_5_9", "Thinness_10_19", "Schooling")

# Линейная модель
lm_4_base <- lm(Life_exp ~ ., dat)
lm_4_1 <- lm(Life_exp ~ Adult_mort + Schooling, data_cl_3) # регрессия на статистически значимые независимые переменные

# Степенная модель
nonlm_1_4 <- lm(Life_exp ~ ., dd4)
nonlm_2_4 <- lm(Life_exp ~ Adult_mort + Alcohol + Dipht + GDP + Thinness_5_9 + Schooling, dd4) # регрессия на статистически значимые независимые переменные

# Экспоненциальная модель
exponen4 <- as.data.frame(dat)
exponen4$Life_exp <- log(exponen4$Life_exp)
nonlm_exp_t4 <- lm(Life_exp ~ ., exponen4)
nonlm_exp4 <- lm(Life_exp ~ Adult_mort +  GDP + Schooling, exponen4) # регрессия на статистически значимые независимые переменные
```


```{r, warning=FALSE, comment=NA}
BIC(lm_4_base)
BIC(lm_4_1)
BIC(nonlm_1_4)
BIC(nonlm_2_4)
BIC(nonlm_exp_t4)
BIC(nonlm_exp4)
```

Поскольку, согласно Байесовскому информационному критерию, наилучшая регрессионная модель (со значением Байесовского критерия -497.98, что является лучшим значением не только среди моделей для общего датасета, но и всех остальных моделей, построенных для кластеров) – это модель `nonlm_exp4`, то более подробное описание качества модели будет сделано лишь для этой модели. Это степенная модель, построенная на регрессоры `Adult_mort`, `GDP`, `Schooling`. Эта модель объясняет $78\%$ вариации результирующей переменной, что является лучшим результатом по сравнению с моделями, построенными для кластеров. По графикам для оцененных уравнений регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами, можно увидеть, что в случае совокупно взятого датасета истинные зависимости хорошо (даже с учётом имеющихся нелинейностей и довольно выбивающейся кривой для `Schooling`) аппроксимируются построенным уравнением регрессии:

```{r, warning=FALSE, comment=NA}
summary(nonlm_exp4)
plot_model(nonlm_exp4, type = 'slope')
```

Мы не будем анализировать остатки для этой модели, поскольку её технические характеристики подробно описывались на первой компьютерной работе. Мы лишь выведем коэффициенты эластичности и соответствия предсказаний истинным значениям.

Итак, анализ коэффициентов показывает нам следующее:

* уменьшение значения смертности в возрасте 15-60 лет на 1000 человек на $1\%$ ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen4[["Adult_mort"]]) * 5.213e-04`\%$: видно, что это значение находится в интервале между таковым для кластера развивающихся стран;
* увеличение ВВП на душу населения на $1\%$ ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen4[["GDP"]]) * 1.619e-02`\%$ -- и вновь видно, что это значение располагается в промежутке между значениями для развивающихся и развитых стран;
* увеличение количества лет, потраченных на образование, на $1\%$ ведёт к увеличению продолжительности жизни в среднем на $`r mean(exponen4[["Schooling"]]) * 1.651e-02`\%$. Это значение, напротив, превышает те, что были получены для отдельных кластеров: тем самым для всей совокупности стран получилось большее влияние образования на продолжительность жизни.

```{r, warning=FALSE, comment=NA}
pred_nlm4 <- predict(nonlm_exp4)

df_true <- data.frame(x = exponen4$Adult_mort, y = exponen4$Life_exp)
df_true$Продолжительность_жизни <- 'фактическая'

df_pred <- data.frame(x = exponen4$Adult_mort, y = pred_nlm4)
df_pred$Продолжительность_жизни <- 'модельная'

df_1 <- rbind.data.frame(df_true, df_pred)

ggplot(df_1, aes(x = x, y = y)) +
  geom_point(aes(color = Продолжительность_жизни)) +
  labs(x = 'ВВП на душу населения (логарифмированный)', y = 'Продолжительность жизни, годы')
```

Как видно -- и это справедливо для остальных регрессоров -- истинные значения хорошо предсказываются модельными. И вновь заметен кластер точек, который по каким-то причинам не был выделен в отдельный кластер -- возможно, как уже говорилось, это обусловлено особенностями исходного 8-мерного признакового пространства.

### **6.2. Сопоставление качества построенных моделей для кластеров и всей совокупности**
Остаётся подвести итог проделанной работе. Было построено множество регрессионных моделей, отбор лучших моделей производился в несколько этапов, были использованы как аналитические, так и графические средства проверки качества моделей. И, несмотря на то, что исходной гипотезой было то, что в кластерах регрессионная модель будет вести себя лучше, реальность оказалась иной: и по критерию Акаике (остался за кадром), и по Байесовскому критерию информативности, и по уровню объяснённой дисперсии лидирует -- причём с существенным отрывом -- модель, построенная на всей совокупности данных. Есть несколько гипотез, которые обосновывают, почему это так:

1. Истинные функциональные зависимости между результирующей переменной и регрессорами не были найдены верно. Действительно, графики явно показывали на достаточный пласт неучтённых зависимостей (особенно речь идёт о графиках для оцененных уравнений регрессии с каждым регрессором в отдельности), а нами были рассмотрены лишь несколько базовых преобразований, позволяющих найти нелинейные зависимости.
2. Исходное признаковое пространство таково, что гораздо более информативным является построение регрессии для всей совокупности, нежели для отдельных её частей. Об этом свидетельствует, например, то, что, несмотря на то, что в регрессионную модель по всей совокупности обобщённо вошли те же регрессоры, что присутствовали в уравнениях регрессии для отдельных кластеров, причём коэффициенты эластичности находятся в промежутках между значениями коэффициентов эластичности для отдельных кластеров, например, переменная `Schooling` в случае регрессии для всей совокупности объектов показала большее значение коэффициента эластичности: возможно, имели место некоторые неучтённые зависимости при делении на кластеры.
3. Деление на кластеры не было идеальным. Хотя мы предположили, что то распределение данных по кластерам, которые мы получили в результате применения алгоритма k-means вполне логично и оправданно, вполне может быть так, что пространство наблюдений формирует нечёткие и размытые между собой кластеры, которые не меняют радикально ни наклон, ни intercept обобщённой регрессионной модели. Также вполне возможен сценарий, что форма кластеров в n-мерном признаковом пространстве имеет какую-то нелинейную форму. Более того, если взглянуть на кластеры, то видно, что они несколько накладываются друг на друга, что тоже могло сыграть свою роль при качестве построенных в кластерах регрессионных моделей. Более подробно рассмотреть то, насколько информативным является деление на кластеры, можно в следующем пункте работы.

Тем не менее, даже анализируя коэффициенты эластичности в таких кластерах в неидеальных моделях, можно сделать интересные и подтверждающиеся здравым смыслом и экономической, социологической парадигмой выводы касательно того, какой аспект человеческого развития и в какой мере влияет на продолжительность жизни в тех или иных странах.


## **7. Линейный дискриминантный анализ**

В данном разделе речь пойдет про дискриминантный анализ, он используется для отнесения новых элементов генеральной совокупности к какому-то из классов при условии, что новый элемент будет относиться к какому-то из известных классов. Для решения задачи используются два основных метода ДА: линейный и вероятностный, по факту в нашей работе мы будем задействовать оба метода, к примеру, построение дискриминантной функции относистя к линейному подходу в то время как таблица соответствия предсказанных классов исходным относится к вероятностному подходу. 

### **7.1. Построение дискриминантных функций. Выводы о качестве модели**

Так как наши данные уже были логарифмированы, почищены на предмет нулей, там где это требовалось, и стандартизированы для применения евклидовой метрики, то можем приступать к ДА.

Для проведения дискриминантного анализа воспользуемся результатами кластерного анализа ИЗ пункта 4, для наглядности повторим k-means с тем же seed.


```{r, warning=FALSE, comment=NA}
set.seed(2)
kmeans3 <- kmeans(data2, centers = 3)
kmeans3
```
Присоединение вектора значений принадлежности к кластеру к основным данным:

```{r, warning=FALSE, comment=NA}
cl <- kmeans3$cluster
cluster_data <- cbind(data2,cl)
```


Итак,Линенйный Дискриминантный Анализ:
Разделение выборки на обучающую (2/3) и тестовую (1/3), именно в такой пропорции мы это делали на семинаре и так рекомендовано делить выборку:
```{r, warning=FALSE, comment=NA}
data.train <- as.data.frame(cluster_data[seq(1,nrow(cluster_data),1.5),])
data.unknown <- as.data.frame(cluster_data[-seq(1,nrow(cluster_data),1.5),])
```

Пострим непосредтвенно саму дискриминантную функцию, также удалим столбец cl, чтобы оне не мешал анализу:
```{r, warning=FALSE, comment=NA}
lda.fit <- lda(data.train[, -c(9)], data.train$cl)
lda.fit
```
Мы видим вероятности после опыта: вероятность быть в 1 группе - 0.4754098, во второй - 0.2377049, в 3 - 0.2868852. Тут же видим вклад наших дискриминантных функций: LD1 - 0.9387, LD2 - 0.0613 - это та доля, сколько наблдений каждая из функций помогает определить без построения иных дискриминантных функций.

### **Вывод о качестве модели (пункт 7.1)**
На первый взгляд так как вклад ld1 аж 0.9387, что крайне много, а линейных дискриминант у нас всего две, то это говорит о том, что модель крайне неплозо построена, так как она строится так, чтобы сначала вклад ld1 максимизировался (он вышел аж 0.9387), затем уже вклад ld2 (понтяно, что так как модели всего две, то на вторую дискриминанту придется оставшаяся доля наблюдений). Пока что модель 

-------------------

### **7.2. Отнесение новых объектов (3-4 наблюдения) к выделенным и описанным кластерам различными способами с использованием ДФ**

Так как по условию требуется взять 3-4 наблюдения, то прошлые выборки не годятся. Создадим новые выборки, так как у нас 183 исходных наблюдения, а нам надо отобрать всего лишь 4 для тестовой выборки, то возьмем для тренировочной выборки шаг 1.022346, так как мы берем в нее (183-4)/183 часть от общего числа, затем перевернем полученную дробь, будет 183/179, что и равно 1.022346:

```{r, warning=FALSE, comment=NA}
data.train2 <- as.data.frame(cluster_data[seq(1,nrow(cluster_data),1.022346),])
data.unknown2 <- as.data.frame(cluster_data[-seq(1,nrow(cluster_data),1.022346),])
```

Как мы видим, в data.train2 ровно 179 наблюдений, сколько мы и хотели взять, а в data.unknown2 их 4, так что отбор вышел замечательный.

```{r, warning=FALSE, comment=NA}
lda.fit2 <- lda(data.unknown2[, -c(9)], data.unknown2$cl)
lda.fit2
```
Мы видим выше, что у нас из 4 наблюдений 1 попадает в 1 класс, одно в - во второй и 2 попадают в третий класс.


### **7.2.1. Линейный метод**

Проведем саначал анализ попадания наших наблюдений с помощью линейного метода:


```{r, warning=FALSE, comment=NA}
data.unknown2
```

Прогноз для наблюдений из тестовой выборки:
"class" содержит информацию о предсказанной группе для каждого наблюдения
"posterior" -- матрица вероятности принадлежности к каждому классу для каждого наблюдения из тестовой выборки
"x" - значение дискриминантной функции

```{r, warning=FALSE, comment=NA}
lda.pred2 <- predict(lda.fit2, data.unknown2[,1:8])
names(lda.pred2)
```
```{r, warning=FALSE, comment=NA}
ldahist(data = lda.pred2$x[,1], g=cl)
```


Итак, у нас в 1 группу прогнозируются 2 попадания, во 2 ноль попаданий и в 3 группу два попадания. Данный метод работает некорректно, так как один из элементов вместо попадания во 2 группу попал в 1 группу, таким обращом в 1 группе у нас предсказывается 2 наблюдения.

### **7.2.2. Вероятностный метод**

Теперь перейдем к вероятностному подходу:

```{r, warning=FALSE, comment=NA}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred2$class, data.unknown2[,c("cl")])
```

Этот метод же показывает, что наша модель работает не идеально на 4 тестовых наблюдениях, так как ошибка определения, как и в предыдущем методе составил 1/4 или 0,33. Надежной модель считается, если погрешность меньше 0,1. Тем не мене не будем забывать, что по условию просили взять 4 наблюдения, что крайне мало для точного понимания того насколько классно работает обученная модель, надо брать куда больше значений, чем мы и займемся в одном из следующих пунктов!


### **7.3. Уточнение результатов классификации, выполненной с помощью метода к-средних, с помощью аппарата дискриминантного анализа (выявление некорректно классифицированных наблюдений)**

Вывдем данные по k-means еще раз:

```{r, warning=FALSE, comment=NA}
kmeans3
```

Теперь выведем данные по ДА:

```{r, warning=FALSE, comment=NA}
lda.fit<- lda(data.train[, -c(9)], data.train$cl)
lda.fit
```
Если мы взглянем на средние значения в k-means и в ДА, то поймем, что в целом модели очень похожи по тому, как они делят объекты на класетры, что не удивительно.

На вопрос о неверной классификации ответим с ледующем пункте.Потому как пока что была проделана работа только по выборке в 4 наблюдения, а работа по тестовой выборке (которая 1/3 от общей) проделана еще не была! Так что прошу искать частично ответ на данный пункт в следующем пунтке, пожалуйста.


### **7.4. Анализ классификационной матрицы (classification matrix). Вывод о качестве разбиения объектов на кластеры**


```{r, warning=FALSE, comment=NA}
lda.pred <- predict(lda.fit, data.unknown[,1:8])
names(lda.pred)
```

Таблица соответствия предсказанных классов исходным:

```{r, warning=FALSE, comment=NA}
lda.pred$class
table(lda.pred$class, data.unknown[,c("cl")])
summary(lda.pred$class)
```
Найдем еще ошибку:

```{r, warning=FALSE, comment=NA}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.unknown[,c("cl")])
```
Как мы видим, модель ДА прекрасно справилась с заданием, ведь ошибка составила всего 0.0495 по сравнению с результами с результатами Кластерного анализа. Вот только теперь мы можем точно сказать (в предыдущем пункте было рано сравнивать ДА с КА, так как еще не была построена матрица для выборки 1/3), что отличий почти нет, так как 0,0495 более чем в 2 раза менее 0,1 что считается максимальной допустимой погрешностью модели.

### **7.5. Построение графика принадлежности тестовой и тренировочной выборок к кластерам по результатам проведенного анализа**

Построим сначала гистограммы, чтобы показать, как разбиваются наблюдения на классы:

### **7.5.1. Тренировочная выборка**

Гистограмма значений первой дискриминантной функции:
```{r, warning=FALSE, comment=NA}
ldahist(data = lda.pred$x[,1], g=cl)
```

Видим, что у разбиений гистограммы отличаются по показателям (мода, смещение и т.д.), это говорит о том, что все хорошо, что класетры отличны друг от друга.

Построим гистограмму второй дискриминантной функции:

```{r, warning=FALSE, comment=NA}
ldahist(data = lda.pred$x[,2], g=cl)
```

Тут тоже все хорошо.

Сам график тренировочной выборки:
```{r, warning=FALSE, comment=NA}
plot(lda.fit)
```

Просматривается четкое деление на классы в данной выборке.

### **7.5.2. Тестовая выборка**

Проведем операции, необходимые для построения графиков: 

```{r}
lda.fit3<- lda(data.unknown[, -c(9)], data.unknown$cl)
lda.fit3
```
Значение вклада первой дискриминанты за 0.8, что является неплохим показателем.

```{r}
lda.pred3 <- predict(lda.fit3, data.unknown[,1:8])
names(lda.pred3)
```
По первой переменной:

```{r}
ldahist(data = lda.pred3$x[,1], g=cl)
```

Наблюдаем кардинально отличающиеся друг от друга разбиения, что говорит о хорошем различии между классами, а это в свою очередь говорит о качестве нашей модели lda.pred3

По второй:

```{r}
ldahist(data = lda.pred3$x[,2], g=cl)
```

По второй дискриминанте ситуация аналогична. Разбиения отличны друг от друга.

График для тестовой выборки:

```{r}
plot(lda.fit3)
```

Тут вообще прослеживается крайне четкое деление на классы, вопросов быть не может о качестве модели.

Напоследок добавим диаграмму рассеяния значений дискриминантных функций:
```{r}
plot(lda.pred$x[,1],lda.pred$x[,2]) # make a scatterplot
text(lda.pred$x[,1],lda.pred$x[,2],cl,cex=0.7,pos=4,col="blue") # add labels
```

Тут же мы видим scatterplot в осях lda.pred, наблюдаем, что в целом четкую особенность расположения точек какую-либо выделить сложно, они скорее на данном графике расположен беспорядочно.

### **7.5.3. Общий вывод по ДА**

Хоть в задании и не требовалось сделать вывод по всему комплексу заданий, связанных с ДА, я все-таки решил сделать такую главу, чтобы подвести черту под проделанной работой.

Итак, дискриминантная функция из раздела 6.1 вышла у нас вполне неплохая, что подтвердилось ка высоким значением вклада ld1, так и низкой ошибкой в матрице, так и распределениями с гистограммами по переменным. Что касается характеристики 4 объектов, то имеющаяся у нас модель по нашей выборке справилась также вполне сносно, учитывая, что наблюдений в unknown выборке бралось всего 4 (что крайне мало при общем наблюдении в 183, так как чем больше элементов в unknown, тем менее выборка подвержена ошибке классификации), так что тут тоже можно было бы поставить плюс. Что же касается сравнения результатов ДА с результатами кластерного анализа, то и тут я бы сказал, что мы справились уверенно, так как оба статистических подхода к выделени в выборке групп дали почти одинаковые занчения средних показателей по переменным в каждом из классов(в ДА) или кластеров (в КА), как таковых значимых отличий в результатах двух подходов не выявлено. Остальные же разделы работы в рамках пункта ДА (пункты 6.4,6.5) комментировать нет смысла, так как там была информация, которая просто подтверждала и описывала остальные пункты в рамках ДА.

## **8. Итоги**

На этом -- завершающем -- этапе выполнения компьютерной работы была проведена большая работа с признаковым пространством исходного массива данных: были применены как методы извлечения наиболее информативных переменных с помощью PCA, так и разделение объектов на отдельные кластеры по их признаковому описанию. Были уточнены результаты построения регрессионных моделей в отдельных кластерах и проведён линейный дискриминантый анализ, выясняющий, действительно ли деление на кластеры было качественным.
